{
  "description": "Real-world ETL job with multiple reads/writes and env vars",
  "priority": "HIGH",
  "env_vars": [
    {"name": "DATABASE_HOST", "pattern": "os.getenv"},
    {"name": "OUTPUT_PATH", "pattern": "os.getenv", "has_default": true}
  ],
  "tables_read": [
    {"name": "warehouse.dim_users", "pattern": "spark.read.table"},
    {"name": "s3://data-lake/delta/fact_events", "pattern": "spark.read.format.load", "source_type": "delta"},
    {"name": "s3://reference-data/geo/countries.parquet", "pattern": "spark.read.parquet", "source_type": "parquet"},
    {"name": "marketing.campaigns", "pattern": "spark.sql.FROM"}
  ],
  "tables_written": [
    {"name": "warehouse.daily_user_metrics", "pattern": "write.saveAsTable"},
    {"name": "s3://ml-features/user_segments/", "pattern": "write.format.save", "source_type": "delta"},
    {"name": "reporting.segment_summary", "pattern": "write.insertInto"}
  ],
  "notes": "The f-string parquet write is harder to detect statically - acceptable miss"
}
