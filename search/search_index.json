{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Jnkn","text":"<p>Catch cross-domain breaking changes before they reach production.</p>"},{"location":"#why-jnkn","title":"Why <code>jnkn</code>?","text":"<ul> <li> <p> The 3am Page</p> <p>You renamed a variable in Terraform. CI passed. Code review approved. Deploy went smooth. Then your app crashes at 3am because Python still expects the old name. Every team has this story.</p> </li> <li> <p> It Gets Worse</p> <p>Your data engineer updates a dbt model, renaming <code>user_id</code> to <code>customer_id</code>. dbt tests pass. But downstream, three Airflow DAGs, a Spark job, and your ML pipeline all expect <code>user_id</code>. You find out when dashboards go blank Monday morning.</p> </li> <li> <p> Junkan Catches It</p> <p>Junkan builds a dependency graph across your entire stack. Before you merge, you see: \"This change impacts 4 Python files, 2 Terraform outputs, and 1 Kubernetes secret.\" No more surprises.</p> </li> </ul>"},{"location":"#how-it-works","title":"How It Works","text":"<pre><code>graph LR\n    subgraph Parse\n        PY[Python] --&gt; G[Graph]\n        TF[Terraform] --&gt; G\n        K8[Kubernetes] --&gt; G\n    end\n\n    subgraph Stitch\n        G --&gt; M{Token&lt;br/&gt;Matching}\n        M --&gt; L[Cross-Domain&lt;br/&gt;Links]\n    end\n\n    subgraph Analyze\n        L --&gt; B[Blast Radius]\n        B --&gt; PR[PR Comment]\n    end</code></pre> <ol> <li>Parse your codebase into a dependency graph</li> <li>Stitch cross-domain links using token matching</li> <li>Analyze blast radius for any change</li> </ol>"},{"location":"#quick-start","title":"Quick Start","text":"<ul> <li> <p>1. Install</p> <pre><code>pip install jnkn\n</code></pre> </li> <li> <p>2. Scan</p> <pre><code>jnkn init\njnkn scan\n</code></pre> </li> <li> <p>3. Analyze</p> <pre><code>jnkn blast env:DATABASE_URL\n</code></pre> </li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li> <p> 5-Minute Setup</p> <p>Install and scan your first project in minutes. No configuration required.</p> <p> Quickstart</p> </li> <li> <p> Understand Impact</p> <p>See what breaks when you change infrastructure or code. Traverse dependencies across domains.</p> <p> Blast Radius</p> </li> <li> <p> CI/CD Ready</p> <p>Block risky PRs automatically with GitHub Actions. Get impact analysis on every pull request.</p> <p> CI Integration</p> </li> <li> <p> Highly Configurable</p> <p>Tune confidence thresholds and suppress false positives. Make it work for your codebase.</p> <p> Configuration</p> </li> </ul>"},{"location":"#supported-stacks","title":"Supported Stacks","text":"Language/Tool Patterns Detected Python <code>os.getenv</code>, Pydantic Settings, Click/Typer, django-environ Terraform Resources, variables, outputs, data sources Kubernetes ConfigMaps, Secrets, environment variables dbt <code>ref()</code>, <code>source()</code>, <code>var()</code> JavaScript <code>process.env</code>, dotenv"},{"location":"#get-started","title":"Get Started","text":"<ul> <li> <p> Install</p> <p> Installation Guide</p> </li> <li> <p> Quickstart</p> <p> 5-Minute Tutorial</p> </li> <li> <p> Learn</p> <p> Full Tutorials</p> </li> </ul>"},{"location":"about/","title":"About Junkan (<code>jnkn</code>)","text":""},{"location":"about/#the-name","title":"The Name","text":"<p>Junkan (\u5faa\u74b0, \u3058\u3085\u3093\u304b\u3093, jun-kan) is Japanese for \"circulation\" or \"cycle.\"</p> <p>In Japanese, you can have:</p> <ul> <li>\u597d\u5faa\u74b0 (k\u014d-junkan) \u2014 a virtuous cycle</li> <li>\u60aa\u5faa\u74b0 (aku-junkan) \u2014 a vicious cycle</li> </ul> <p>That's exactly what cross-domain dependencies create. </p> <p>When everything is connected but nothing is tracked, you get a vicious cycle: change something, break something else, scramble to fix it, break a third thing. Repeat at 3am.</p> <p>Junkan breaks that cycle by going around your entire infrastructure \u2014 Python, Terraform, Kubernetes, dbt \u2014 and showing you the connections before they become incidents.</p> <p><code>jnkn</code> is just the shorthand. Easier to type, fits nicely in a CLI.</p>"},{"location":"about/#the-philosophy","title":"The Philosophy","text":"<p>Most tools check their own domain:</p> <ul> <li>Terraform validates Terraform</li> <li>pytest tests Python</li> <li>dbt tests dbt</li> </ul> <p>Nobody checks the seams. The gaps between domains are where production breaks hide.</p> <p>Junkan's job is to trace the circle \u2014 follow a variable from your Python config, through your Terraform outputs, into your Kubernetes secrets, and back. If renaming one thing ripples across your stack, you should know before you merge.</p>"},{"location":"about/#the-goal","title":"The Goal","text":"<p>Turn \u60aa\u5faa\u74b0 into \u597d\u5faa\u74b0.</p> <p>Instead of: change \u2192 deploy \u2192 break \u2192 page \u2192 fix \u2192 repeat</p> <p>You get: change \u2192 analyze \u2192 fix \u2192 deploy \u2192 sleep</p> <p>That's the cycle we're optimizing for.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to Jnkn.</p>"},{"location":"changelog/#versioning","title":"Versioning","text":"<p>Jnkn follows Semantic Versioning:</p> <ul> <li>MAJOR \u2014 Incompatible API changes</li> <li>MINOR \u2014 New functionality, backwards compatible</li> <li>PATCH \u2014 Bug fixes, backwards compatible</li> </ul>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Python parser expansion (50+ patterns)</li> <li>Click/Typer <code>envvar=</code> detection</li> <li>django-environ support</li> <li>python-dotenv support</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Improved confidence calculation</li> <li>Better false positive handling</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Multiline env var detection</li> <li>Pydantic <code>env_prefix</code> handling</li> </ul>"},{"location":"changelog/#010-2023-12-01","title":"[0.1.0] - 2023-12-01","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial release</li> </ul>"},{"location":"changelog/#links","title":"Links","text":"<ul> <li>GitHub Releases</li> <li>Migration Guides</li> </ul>"},{"location":"cicd_date/design/","title":"CI/CD Gate Design","text":"<p>Version: 1.0.0 Last Updated: December 2024</p> <p>This document describes the design and implementation of Jnkn's CI/CD gate - a pre-merge enforcement layer that prevents breaking changes from reaching production.</p>"},{"location":"cicd_date/design/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Value Proposition</li> <li>The Problem We Solve</li> <li>Solution Overview</li> <li>Architecture</li> <li>Data Flow</li> <li>Policy Engine</li> <li>Exit Codes and Gate Logic</li> <li>Integration Points</li> <li>Technical Implementation</li> <li>Usage Examples</li> <li>Deployment Guide</li> </ol>"},{"location":"cicd_date/design/#value-proposition","title":"Value Proposition","text":""},{"location":"cicd_date/design/#the-one-liner","title":"The One-Liner","text":"<p>Jnkn's CI/CD gate tells you \"this PR will break the executive dashboard\" before you merge, not after the CEO asks why the numbers are wrong.</p>"},{"location":"cicd_date/design/#before-jnkn","title":"Before Jnkn","text":"<pre><code>Developer \u2192 PR \u2192 Code Review \u2192 Merge \u2192 Deploy \u2192 Run \u2192 Silent Failure \u2192 Alert\n                      \u2502                                       \u2502\n                      \u2502                                       \u25bc\n                      \u2502                              Hours/days later:\n                      \u2502                              \"Why are the numbers wrong?\"\n                      \u25bc\n              Reviewer can only check:\n              - Code quality\n              - Test coverage\n              - Logic correctness\n\n              Reviewer CANNOT know:\n              - What consumes this data in production\n              - Which dashboards depend on it\n              - Which ML models use it as input\n</code></pre>"},{"location":"cicd_date/design/#after-jnkn","title":"After Jnkn","text":"<pre><code>Developer \u2192 PR \u2192 Jnkn Check \u2192 Decision\n                      \u2502\n                      \u251c\u2500\u2192 PASS: Safe to merge\n                      \u2502\n                      \u251c\u2500\u2192 WARN: \"Affects ML pipeline, notify @ml-team\"\n                      \u2502\n                      \u2514\u2500\u2192 BLOCK: \"Affects executive dashboard, requires approval\"\n                              \u2502\n                              \u25bc\n                      Before any damage occurs:\n                      - Stakeholders notified\n                      - Approvals requested\n                      - Impact documented\n</code></pre>"},{"location":"cicd_date/design/#quantified-value","title":"Quantified Value","text":"Metric Without Jnkn With Jnkn Time to detect impact Hours to days Seconds (at PR time) Data incidents from code changes Common Preventable Mean time to resolution 4+ hours N/A (prevented) Stakeholder surprise Frequent Eliminated Cross-team coordination Reactive Proactive"},{"location":"cicd_date/design/#the-problem-we-solve","title":"The Problem We Solve","text":""},{"location":"cicd_date/design/#silent-data-pipeline-failures","title":"Silent Data Pipeline Failures","text":"<p>Data pipelines don't crash - they produce wrong data. A schema change, a filter modification, a column rename - none of these throw errors. They silently propagate incorrect data downstream.</p> <pre><code>graph LR\n    subgraph \"The Silent Failure Chain\"\n        CHANGE[Code Change] --&gt; JOB[Job Runs Successfully]\n        JOB --&gt; DATA[Wrong Data Written]\n        DATA --&gt; DOWNSTREAM[Downstream Jobs Run]\n        DOWNSTREAM --&gt; DASHBOARD[Dashboard Shows Wrong Numbers]\n        DASHBOARD --&gt; CEO[CEO Asks Questions]\n    end\n\n    style CHANGE fill:#4dabf7,stroke:#1971c2,color:#fff\n    style CEO fill:#ff6b6b,stroke:#c92a2a,color:#fff</code></pre>"},{"location":"cicd_date/design/#the-knowledge-gap","title":"The Knowledge Gap","text":"<p>The developer making a change often doesn't know:</p> <ol> <li>Who consumes their output - \"I didn't know the ML team uses this table\"</li> <li>What's considered critical - \"I didn't know this feeds the board deck\"</li> <li>Who to notify - \"I didn't know @finance-data owns this\"</li> </ol> <p>This knowledge exists in two places: - OpenLineage: Actual runtime dependencies - Tribal knowledge: Who owns what, what's critical</p> <p>Jnkn bridges both gaps.</p>"},{"location":"cicd_date/design/#solution-overview","title":"Solution Overview","text":""},{"location":"cicd_date/design/#core-concept","title":"Core Concept","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         jnkn check                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502   INPUTS:                                                           \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502   \u2502 Changed     \u2502  \u2502 OpenLineage \u2502  \u2502 Policy      \u2502                \u2502\n\u2502   \u2502 Files       \u2502  \u2502 Data        \u2502  \u2502 Rules       \u2502                \u2502\n\u2502   \u2502 (from PR)   \u2502  \u2502 (runtime)   \u2502  \u2502 (business)  \u2502                \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502          \u2502                \u2502                \u2502                        \u2502\n\u2502          \u25bc                \u25bc                \u25bc                        \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u2502\n\u2502   \u2502              Analysis Engine                     \u2502              \u2502\n\u2502   \u2502                                                  \u2502              \u2502\n\u2502   \u2502  1. Parse changed files                          \u2502              \u2502\n\u2502   \u2502  2. Identify affected assets                     \u2502              \u2502\n\u2502   \u2502  3. Expand blast radius (OpenLineage)            \u2502              \u2502\n\u2502   \u2502  4. Evaluate policy rules                        \u2502              \u2502\n\u2502   \u2502  5. Determine gate decision                      \u2502              \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2502\n\u2502                           \u2502                                         \u2502\n\u2502                           \u25bc                                         \u2502\n\u2502   OUTPUT:                                                           \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502   \u2502 Exit Code   \u2502  \u2502 PR Comment  \u2502  \u2502 JSON Report \u2502                \u2502\n\u2502   \u2502 0/1/2       \u2502  \u2502 (Markdown)  \u2502  \u2502 (Artifact)  \u2502                \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502                                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cicd_date/design/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Fail-safe defaults: Unknown assets are flagged, not ignored</li> <li>Configurable severity: Business defines what's critical</li> <li>Transparent decisions: Every block/warn includes explanation</li> <li>Graceful degradation: Works without OpenLineage (reduced accuracy)</li> <li>CI-native: Exit codes, JSON output, environment variable support</li> </ol>"},{"location":"cicd_date/design/#architecture","title":"Architecture","text":""},{"location":"cicd_date/design/#component-diagram","title":"Component Diagram","text":"<pre><code>graph TB\n    subgraph \"Input Sources\"\n        GIT[Git Diff]\n        GITHUB[GitHub API]\n        FILE[Diff File]\n    end\n\n    subgraph \"Jnkn Check Command\"\n        PARSER[Diff Parser]\n        ENGINE[Check Engine]\n        POLICY_ENGINE[Policy Engine]\n        OL_CLIENT[OpenLineage Client]\n    end\n\n    subgraph \"External Systems\"\n        MARQUEZ[(Marquez/DataHub)]\n        POLICY_FILE[policy.yaml]\n    end\n\n    subgraph \"Outputs\"\n        EXIT[Exit Code]\n        REPORT[JSON Report]\n        MARKDOWN[PR Comment]\n    end\n\n    GIT --&gt; PARSER\n    GITHUB --&gt; PARSER\n    FILE --&gt; PARSER\n\n    PARSER --&gt; ENGINE\n    OL_CLIENT --&gt; ENGINE\n    POLICY_ENGINE --&gt; ENGINE\n\n    MARQUEZ --&gt; OL_CLIENT\n    POLICY_FILE --&gt; POLICY_ENGINE\n\n    ENGINE --&gt; EXIT\n    ENGINE --&gt; REPORT\n    ENGINE --&gt; MARKDOWN\n\n    style ENGINE fill:#4dabf7,stroke:#1971c2,color:#fff</code></pre>"},{"location":"cicd_date/design/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>classDiagram\n    class CheckEngine {\n        -Policy policy\n        -str openlineage_url\n        -Dict runtime_graph\n        +run(changed_files) CheckReport\n        -_load_openlineage_data()\n        -_detect_column_changes()\n        -_identify_affected_assets()\n        -_expand_blast_radius()\n        -_evaluate_policy()\n        -_determine_result()\n    }\n\n    class Policy {\n        +List~PolicyRule~ rules\n        +Severity default_severity\n        +bool block_on_critical\n        +bool warn_on_high\n    }\n\n    class PolicyRule {\n        +str name\n        +str pattern\n        +Severity severity\n        +List~str~ owners\n        +bool require_approval\n    }\n\n    class CheckReport {\n        +CheckResult result\n        +List~ChangedFile~ changed_files\n        +List~ColumnChange~ column_changes\n        +List~AffectedAsset~ affected_assets\n        +List~PolicyViolation~ violations\n        +to_dict()\n        +to_markdown()\n    }\n\n    class ChangedFile {\n        +str path\n        +str change_type\n        +str old_path\n    }\n\n    class AffectedAsset {\n        +str id\n        +str name\n        +str asset_type\n        +Severity severity\n        +float confidence\n        +List~str~ path\n        +List~str~ owners\n    }\n\n    CheckEngine --&gt; Policy\n    CheckEngine --&gt; CheckReport\n    Policy --&gt; PolicyRule\n    CheckReport --&gt; ChangedFile\n    CheckReport --&gt; AffectedAsset</code></pre>"},{"location":"cicd_date/design/#data-flow","title":"Data Flow","text":""},{"location":"cicd_date/design/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant CI as CI Pipeline\n    participant Check as jnkn check\n    participant Git as Git\n    participant OL as OpenLineage API\n    participant Policy as Policy File\n\n    CI-&gt;&gt;Check: Run check command\n\n    rect rgb(240, 248, 255)\n        Note over Check,Git: Step 1: Get Changed Files\n        Check-&gt;&gt;Git: git diff main HEAD\n        Git--&gt;&gt;Check: List of changed files\n    end\n\n    rect rgb(255, 248, 240)\n        Note over Check,OL: Step 2: Load Runtime Lineage\n        Check-&gt;&gt;OL: GET /api/v1/namespaces/{ns}/jobs\n        OL--&gt;&gt;Check: Jobs with inputs/outputs\n        Check-&gt;&gt;Check: Build dependency graph\n    end\n\n    rect rgb(240, 255, 240)\n        Note over Check,Policy: Step 3: Load Policy\n        Check-&gt;&gt;Policy: Read policy.yaml\n        Policy--&gt;&gt;Check: Rules and severity levels\n    end\n\n    rect rgb(255, 240, 255)\n        Note over Check: Step 4: Analysis\n        Check-&gt;&gt;Check: Parse changed files for assets\n        Check-&gt;&gt;Check: Expand blast radius via graph\n        Check-&gt;&gt;Check: Evaluate policy rules\n        Check-&gt;&gt;Check: Determine result\n    end\n\n    Check--&gt;&gt;CI: Exit code + Report\n\n    alt Exit Code 1 (BLOCKED)\n        CI-&gt;&gt;CI: Fail the build\n    else Exit Code 2 (WARN)\n        CI-&gt;&gt;CI: Pass with warnings\n    else Exit Code 0 (PASS)\n        CI-&gt;&gt;CI: Pass\n    end</code></pre>"},{"location":"cicd_date/design/#blast-radius-expansion","title":"Blast Radius Expansion","text":"<pre><code>graph TB\n    subgraph \"Direct Impact (from code)\"\n        FILE[daily_user_etl.py]\n        JOB1[job:daily_user_etl]\n        TABLE1[data:dim_users]\n    end\n\n    subgraph \"1st Degree (from OpenLineage)\"\n        JOB2[job:user_metrics_agg]\n        JOB3[job:churn_features]\n        JOB4[job:marketing_loader]\n    end\n\n    subgraph \"2nd Degree (from OpenLineage)\"\n        TABLE2[data:agg_user_metrics]\n        TABLE3[data:ml-features/churn]\n        TABLE4[data:campaign_targets]\n    end\n\n    subgraph \"3rd Degree (from OpenLineage)\"\n        JOB5[job:exec_dashboard_loader]\n    end\n\n    subgraph \"Final Impact\"\n        CRITICAL[data:exec_dashboard]\n    end\n\n    FILE --&gt; JOB1\n    JOB1 --&gt; TABLE1\n    TABLE1 --&gt; JOB2\n    TABLE1 --&gt; JOB3\n    TABLE1 --&gt; JOB4\n    JOB2 --&gt; TABLE2\n    JOB3 --&gt; TABLE3\n    JOB4 --&gt; TABLE4\n    TABLE2 --&gt; JOB5\n    JOB5 --&gt; CRITICAL\n\n    style FILE fill:#4dabf7,stroke:#1971c2,color:#fff\n    style CRITICAL fill:#ff6b6b,stroke:#c92a2a,color:#fff</code></pre>"},{"location":"cicd_date/design/#policy-engine","title":"Policy Engine","text":""},{"location":"cicd_date/design/#policy-configuration","title":"Policy Configuration","text":"<pre><code># policy.yaml\n\n# Global settings\nblock_on_critical: true   # Exit 1 when critical systems affected\nwarn_on_high: true        # Exit 2 when high-severity systems affected\n\n# Critical assets - BLOCK the PR\ncritical:\n  - name: Executive Dashboards\n    pattern: \".*(exec|executive|board).*dashboard.*\"\n    severity: critical\n    owners:\n      - \"@data-platform-team\"\n      - \"@analytics-leadership\"\n    require_approval: true\n    notify_always: true\n\n  - name: Financial Systems\n    pattern: \".*(revenue|finance|billing).*\"\n    severity: critical\n    owners:\n      - \"@finance-data\"\n    require_approval: true\n\n# High-severity assets - WARN\nrules:\n  - name: ML Feature Pipelines\n    pattern: \".*(ml-feature|feature-store|model).*\"\n    severity: high\n    owners:\n      - \"@ml-engineering\"\n    require_approval: true\n\n  - name: Data Warehouse Core\n    pattern: \".*(dim_|fact_|warehouse\\\\.).*\"\n    severity: medium\n    owners:\n      - \"@data-engineering\"\n    require_approval: false\n</code></pre>"},{"location":"cicd_date/design/#pattern-matching","title":"Pattern Matching","text":"<pre><code>graph TB\n    subgraph \"Asset ID\"\n        ASSET[\"data:redshift/analytics.exec_dashboard\"]\n    end\n\n    subgraph \"Policy Rules\"\n        RULE1[\"Executive Dashboards&lt;br/&gt;pattern: .*(exec|executive).*dashboard.*\"]\n        RULE2[\"Financial Systems&lt;br/&gt;pattern: .*(revenue|finance).*\"]\n        RULE3[\"ML Features&lt;br/&gt;pattern: .*(ml-feature).*\"]\n    end\n\n    subgraph \"Evaluation\"\n        MATCH[\"\u2713 MATCH: Executive Dashboards\"]\n        RESULT[\"severity: CRITICAL&lt;br/&gt;owners: @data-platform-team\"]\n    end\n\n    ASSET --&gt; RULE1\n    ASSET --&gt; RULE2\n    ASSET --&gt; RULE3\n\n    RULE1 --&gt; MATCH\n    MATCH --&gt; RESULT\n\n    style MATCH fill:#40c057,stroke:#2f9e44,color:#fff\n    style RESULT fill:#ff6b6b,stroke:#c92a2a,color:#fff</code></pre>"},{"location":"cicd_date/design/#severity-levels","title":"Severity Levels","text":"Severity Exit Code Behavior Use Case CRITICAL 1 Block PR Executive dashboards, compliance, revenue HIGH 2 Warn, request approval ML pipelines, core warehouse MEDIUM 0 Warn, notify owners Reporting tables, analytics LOW 0 Log only Staging, temp tables"},{"location":"cicd_date/design/#exit-codes-and-gate-logic","title":"Exit Codes and Gate Logic","text":""},{"location":"cicd_date/design/#decision-tree","title":"Decision Tree","text":"<pre><code>flowchart TB\n    START[Start Check] --&gt; ANALYZE[Analyze Changed Files]\n    ANALYZE --&gt; EXPAND[Expand Blast Radius]\n    EXPAND --&gt; EVAL[Evaluate Policy Rules]\n\n    EVAL --&gt; CRIT{Any CRITICAL&lt;br/&gt;violations?}\n\n    CRIT --&gt;|Yes| BLOCK_CHECK{block_on_critical&lt;br/&gt;= true?}\n    BLOCK_CHECK --&gt;|Yes| BLOCKED[Exit 1: BLOCKED]\n    BLOCK_CHECK --&gt;|No| HIGH_CHECK\n\n    CRIT --&gt;|No| HIGH_CHECK{Any HIGH&lt;br/&gt;violations?}\n\n    HIGH_CHECK --&gt;|Yes| WARN_CHECK{warn_on_high&lt;br/&gt;= true?}\n    WARN_CHECK --&gt;|Yes| WARN[Exit 2: WARN]\n    WARN_CHECK --&gt;|No| ANY_CHECK\n\n    HIGH_CHECK --&gt;|No| ANY_CHECK{Any other&lt;br/&gt;violations?}\n\n    ANY_CHECK --&gt;|Yes| WARN\n    ANY_CHECK --&gt;|No| PASS[Exit 0: PASS]\n\n    style BLOCKED fill:#ff6b6b,stroke:#c92a2a,color:#fff\n    style WARN fill:#fab005,stroke:#f59f00,color:#000\n    style PASS fill:#40c057,stroke:#2f9e44,color:#fff</code></pre>"},{"location":"cicd_date/design/#exit-code-semantics","title":"Exit Code Semantics","text":"<pre><code>Exit 0 (PASS)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n- No policy violations\n- OR only LOW severity matches\n- Safe to merge automatically\n\nExit 1 (BLOCKED)  \n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n- CRITICAL severity violation\n- AND block_on_critical = true\n- PR cannot merge without override\n- Requires explicit approval from owners\n\nExit 2 (WARN)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n- HIGH severity violation\n- OR MEDIUM with notify\n- PR can merge but stakeholders notified\n- Approval recommended but not required\n</code></pre>"},{"location":"cicd_date/design/#integration-points","title":"Integration Points","text":""},{"location":"cicd_date/design/#github-actions","title":"GitHub Actions","text":"<pre><code>name: Jnkn Impact Analysis\n\non:\n  pull_request:\n    paths:\n      - 'src/**/*.py'\n      - 'dbt/**/*.sql'\n\njobs:\n  impact-check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Run Jnkn Check\n        id: jnkn\n        env:\n          OPENLINEAGE_URL: ${{ secrets.MARQUEZ_URL }}\n        run: |\n          jnkn check \\\n            --git-diff origin/${{ github.base_ref }} HEAD \\\n            --policy policy.yaml \\\n            --output report.json \\\n            --format json\n        continue-on-error: true\n\n      - name: Comment on PR\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const report = require('./report.json');\n            // Post formatted comment\n\n      - name: Enforce Gate\n        if: steps.jnkn.outcome == 'failure'\n        run: exit 1\n</code></pre>"},{"location":"cicd_date/design/#gitlab-ci","title":"GitLab CI","text":"<pre><code>jnkn-check:\n  stage: validate\n  script:\n    - jnkn check --git-diff $CI_MERGE_REQUEST_TARGET_BRANCH_NAME HEAD\n      --openlineage-url $MARQUEZ_URL\n      --policy policy.yaml\n      --output report.json\n  artifacts:\n    reports:\n      dotenv: jnkn.env\n    paths:\n      - report.json\n  allow_failure:\n    exit_codes:\n      - 2  # Allow WARN to pass\n</code></pre>"},{"location":"cicd_date/design/#jenkins","title":"Jenkins","text":"<pre><code>pipeline {\n    stages {\n        stage('Impact Analysis') {\n            steps {\n                sh '''\n                    jnkn check \\\n                        --git-diff origin/main HEAD \\\n                        --policy policy.yaml \\\n                        --output report.json\n                '''\n            }\n            post {\n                always {\n                    archiveArtifacts artifacts: 'report.json'\n                }\n                failure {\n                    // Notify stakeholders\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"cicd_date/design/#technical-implementation","title":"Technical Implementation","text":""},{"location":"cicd_date/design/#command-interface","title":"Command Interface","text":"<pre><code>@click.command()\n@click.option(\"--diff\", \"diff_file\", type=click.Path(exists=True),\n              help=\"File containing list of changed files\")\n@click.option(\"--git-diff\", \"git_diff\", nargs=2, metavar=\"BASE HEAD\",\n              help=\"Git refs to diff (e.g., main HEAD)\")\n@click.option(\"--github-pr\", type=int, \n              help=\"GitHub PR number\")\n@click.option(\"--repo\", \n              help=\"GitHub repo (owner/repo)\")\n@click.option(\"--openlineage-url\", envvar=\"OPENLINEAGE_URL\",\n              help=\"OpenLineage/Marquez API URL\")\n@click.option(\"--policy\", \"policy_file\", type=click.Path(exists=True),\n              help=\"Policy YAML file\")\n@click.option(\"--fail-if-critical\", is_flag=True,\n              help=\"Exit 1 if critical systems affected\")\n@click.option(\"--output\", \"-o\", type=click.Path(),\n              help=\"Write JSON report to file\")\n@click.option(\"--format\", type=click.Choice([\"text\", \"json\", \"markdown\"]),\n              default=\"text\")\ndef check(...):\n    \"\"\"Run pre-merge impact analysis.\"\"\"\n</code></pre>"},{"location":"cicd_date/design/#core-algorithm","title":"Core Algorithm","text":"<pre><code>class CheckEngine:\n    def run(self, changed_files: List[ChangedFile]) -&gt; CheckReport:\n        # Step 1: Load runtime lineage (if configured)\n        if self.openlineage_url:\n            self._load_openlineage_data()\n\n        # Step 2: Detect column-level changes\n        column_changes = self._detect_column_changes(changed_files)\n\n        # Step 3: Identify directly affected assets\n        direct_assets = self._identify_affected_assets(changed_files)\n\n        # Step 4: Expand blast radius using OpenLineage graph\n        all_affected = self._expand_blast_radius(direct_assets)\n\n        # Step 5: Evaluate policy rules\n        violations = self._evaluate_policy(all_affected)\n\n        # Step 6: Determine final result\n        result = self._determine_result(violations)\n\n        return CheckReport(\n            result=result,\n            changed_files=changed_files,\n            column_changes=column_changes,\n            affected_assets=all_affected,\n            violations=violations,\n        )\n</code></pre>"},{"location":"cicd_date/design/#output-formats","title":"Output Formats","text":"<p>Text (Terminal) <pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551  \u274c BLOCKED - Critical Impact Detected \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nSummary:\n  Changed files:     1\n  Downstream impact: 10\n  Critical systems:  2\n\nPolicy Violations:\n  \ud83d\udea8 Executive Dashboards: Changes affect 2 assets\n     Required approvers: @data-platform-team\n</code></pre></p> <p>JSON (Artifacts) <pre><code>{\n  \"result\": \"BLOCKED\",\n  \"exit_code\": 1,\n  \"summary\": {\n    \"changed_files\": 1,\n    \"total_downstream\": 10,\n    \"critical_count\": 2\n  },\n  \"violations\": [\n    {\n      \"rule\": \"Executive Dashboards\",\n      \"severity\": \"critical\",\n      \"required_approvers\": [\"@data-platform-team\"]\n    }\n  ]\n}\n</code></pre></p> <p>Markdown (PR Comments) <pre><code>## \ud83d\udd0d Jnkn Impact Analysis\n\n### \u274c BLOCKED - Critical Impact Detected\n\n| Metric | Count |\n|--------|-------|\n| Changed Files | 1 |\n| Downstream Impact | 10 |\n| Critical Systems | 2 |\n\n### \ud83d\udea8 Critical Systems Affected\n\n- **exec_dashboard** (owners: @data-platform-team)\n</code></pre></p>"},{"location":"cicd_date/design/#usage-examples","title":"Usage Examples","text":""},{"location":"cicd_date/design/#basic-usage","title":"Basic Usage","text":"<pre><code># Diff against main branch\njnkn check --git-diff main HEAD\n\n# From a file of changed paths\njnkn check --diff changed_files.txt\n\n# From GitHub PR\njnkn check --github-pr 123 --repo myorg/myrepo\n</code></pre>"},{"location":"cicd_date/design/#with-openlineage","title":"With OpenLineage","text":"<pre><code># Enrich with runtime lineage\njnkn check --git-diff main HEAD \\\n    --openlineage-url http://marquez:5000 \\\n    --openlineage-namespace spark-production\n</code></pre>"},{"location":"cicd_date/design/#with-policy","title":"With Policy","text":"<pre><code># Apply business rules\njnkn check --git-diff main HEAD \\\n    --policy policy.yaml \\\n    --fail-if-critical\n</code></pre>"},{"location":"cicd_date/design/#full-ci-integration","title":"Full CI Integration","text":"<pre><code># Complete example for CI\njnkn check \\\n    --git-diff origin/main HEAD \\\n    --openlineage-url $MARQUEZ_URL \\\n    --policy policy.yaml \\\n    --output impact-report.json \\\n    --format json \\\n    --fail-if-critical\n\n# Use exit code for gate decision\nif [ $? -eq 1 ]; then\n    echo \"BLOCKED: Critical impact detected\"\n    exit 1\nelif [ $? -eq 2 ]; then\n    echo \"WARNING: Review required\"\nfi\n</code></pre>"},{"location":"cicd_date/design/#deployment-guide","title":"Deployment Guide","text":""},{"location":"cicd_date/design/#prerequisites","title":"Prerequisites","text":"<ol> <li>Jnkn installed: <code>pip install jnkn</code></li> <li>Git available: For diff parsing</li> <li>OpenLineage/Marquez (optional): For runtime enrichment</li> <li>Policy file: Define your critical assets</li> </ol>"},{"location":"cicd_date/design/#quick-start","title":"Quick Start","text":"<pre><code># 1. Create policy file\ncat &gt; policy.yaml &lt;&lt; 'EOF'\nblock_on_critical: true\ncritical:\n  - name: Executive Dashboards\n    pattern: \".*exec.*dashboard.*\"\n    severity: critical\n    owners: [\"@data-platform\"]\n    require_approval: true\nEOF\n\n# 2. Test locally\njnkn check --git-diff main HEAD --policy policy.yaml\n\n# 3. Add to CI pipeline\n# (see GitHub Actions example above)\n</code></pre>"},{"location":"cicd_date/design/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>OPENLINEAGE_URL</code> Marquez/DataHub API URL None <code>OPENLINEAGE_NAMESPACE</code> Namespace to query All <code>GITHUB_TOKEN</code> For GitHub PR API None <code>JUNKAN_POLICY</code> Default policy file None"},{"location":"cicd_date/design/#rollout-strategy","title":"Rollout Strategy","text":"<pre><code>Phase 1: Shadow Mode (Week 1-2)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n- Run check but don't block\n- Collect data on what would be blocked\n- Tune policy rules based on findings\n\nPhase 2: Warn Mode (Week 3-4)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n- Enable warn_on_high\n- Post PR comments\n- Don't block merges yet\n\nPhase 3: Enforce Mode (Week 5+)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n- Enable block_on_critical\n- Fail CI for critical impacts\n- Full enforcement active\n</code></pre>"},{"location":"cicd_date/design/#summary","title":"Summary","text":""},{"location":"cicd_date/design/#what-we-built","title":"What We Built","text":"Component Purpose <code>jnkn check</code> CLI command for CI/CD integration Policy Engine Business rules for severity classification OpenLineage Client Runtime lineage enrichment Multiple Outputs Text, JSON, Markdown for different consumers"},{"location":"cicd_date/design/#key-differentiators","title":"Key Differentiators","text":"<ol> <li>Pre-merge, not post-facto - Catch issues before they cause damage</li> <li>OpenLineage integration - Real production dependencies, not guesses</li> <li>Policy-driven - Business defines what's critical</li> <li>CI-native - Exit codes, artifacts, PR comments</li> </ol>"},{"location":"cicd_date/design/#whats-next","title":"What's Next","text":"<ol> <li>Diff-aware analysis - \"Column X removed\" vs \"file changed\"</li> <li>Approval workflows - Native GitHub/GitLab approval integration</li> <li>Slack notifications - Direct alerts to team channels</li> <li>Dashboard - Web UI for exploring impact history</li> </ol>"},{"location":"community/","title":"Community","text":"<p>Join the Jnkn community.</p>"},{"location":"community/#get-help","title":"Get Help","text":"<ul> <li>GitHub Discussions \u2014 Ask questions, share ideas</li> <li>Slack \u2014 Real-time chat with maintainers</li> <li>Stack Overflow \u2014 Tag questions with <code>jnkn</code></li> </ul>"},{"location":"community/#contribute","title":"Contribute","text":"<p>We welcome contributions of all kinds:</p> <ul> <li>\ud83d\udc1b Bug reports \u2014 Found an issue? Open an issue</li> <li>\ud83d\udca1 Feature requests \u2014 Have an idea? Start a discussion</li> <li>\ud83d\udcd6 Documentation \u2014 Fix typos, add examples</li> <li>\ud83d\udd27 Code \u2014 Bug fixes, new features</li> </ul> <p>See Contributing Guide to get started.</p>"},{"location":"community/#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>Issue Tracker</li> <li>Discussions</li> <li>Slack Workspace</li> </ul>"},{"location":"community/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inclusive experience for everyone. Please read our Code of Conduct.</p>"},{"location":"community/code-of-conduct/","title":"Code of Conduct","text":"<p>Community standards.</p>"},{"location":"community/contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to Jnkn!</p>"},{"location":"community/contributing/#quick-start","title":"Quick Start","text":"<pre><code># Clone\ngit clone https://github.com/bordumb/jnkn.git\ncd jnkn\n\n# Install with dev dependencies\npip install -e \".[full,dev]\"\n\n# Run tests\npytest\n\n# Run linter\nruff check .\n</code></pre>"},{"location":"community/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"community/contributing/#1-find-or-create-an-issue","title":"1. Find or Create an Issue","text":"<ul> <li>Check existing issues</li> <li>For new features, open a discussion first</li> <li>Comment on an issue to claim it</li> </ul>"},{"location":"community/contributing/#2-create-a-branch","title":"2. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/issue-number-description\n</code></pre>"},{"location":"community/contributing/#3-make-changes","title":"3. Make Changes","text":"<ul> <li>Write code</li> <li>Add tests</li> <li>Update documentation</li> </ul>"},{"location":"community/contributing/#4-test","title":"4. Test","text":"<pre><code># Run all tests\npytest\n\n# Run specific tests\npytest tests/unit/test_confidence.py\n\n# Run with coverage\npytest --cov=jnkn\n</code></pre>"},{"location":"community/contributing/#5-lint","title":"5. Lint","text":"<pre><code># Check\nruff check .\n\n# Fix automatically\nruff check --fix .\n\n# Format\nruff format .\n</code></pre>"},{"location":"community/contributing/#6-submit-pr","title":"6. Submit PR","text":"<ul> <li>Push your branch</li> <li>Open a Pull Request</li> <li>Fill out the PR template</li> <li>Wait for review</li> </ul>"},{"location":"community/contributing/#what-to-contribute","title":"What to Contribute","text":""},{"location":"community/contributing/#good-first-issues","title":"Good First Issues","text":"<p>Look for issues labeled <code>good first issue</code>:</p> <ul> <li>Documentation improvements</li> <li>Test coverage</li> <li>Small bug fixes</li> </ul>"},{"location":"community/contributing/#larger-contributions","title":"Larger Contributions","text":"<p>For bigger changes, discuss first:</p> <ul> <li>New language parsers</li> <li>New stitching rules</li> <li>Performance improvements</li> <li>New output formats</li> </ul>"},{"location":"community/contributing/#code-style","title":"Code Style","text":"<ul> <li>Python 3.11+ \u2014 Use modern syntax</li> <li>Type hints \u2014 All public functions</li> <li>Docstrings \u2014 Google style</li> <li>Tests \u2014 Aim for 90%+ coverage</li> </ul>"},{"location":"community/contributing/#example","title":"Example","text":"<pre><code>def calculate_confidence(\n    source_tokens: list[str],\n    target_tokens: list[str],\n) -&gt; float:\n    \"\"\"\n    Calculate match confidence between token sets.\n\n    Args:\n        source_tokens: Tokens from source artifact.\n        target_tokens: Tokens from target artifact.\n\n    Returns:\n        Confidence score between 0.0 and 1.0.\n\n    Example:\n        &gt;&gt;&gt; calculate_confidence([\"database\", \"url\"], [\"database\", \"url\"])\n        0.9\n    \"\"\"\n    ...\n</code></pre>"},{"location":"community/contributing/#documentation","title":"Documentation","text":"<p>Documentation lives in <code>docs/</code>. We use MkDocs Material.</p>"},{"location":"community/contributing/#preview-locally","title":"Preview Locally","text":"<pre><code>pip install mkdocs-material\nmkdocs serve\n# Open http://localhost:8000\n</code></pre>"},{"location":"community/contributing/#guidelines","title":"Guidelines","text":"<ul> <li>Be concise</li> <li>Include examples</li> <li>Update relevant pages when changing features</li> </ul>"},{"location":"community/contributing/#questions","title":"Questions?","text":"<ul> <li>Open a GitHub Discussion</li> <li>Ask in Slack</li> <li>Email maintainers@jnkn.io</li> </ul>"},{"location":"community/development-setup/","title":"Development Setup","text":"<p>Set up a local development environment.</p>"},{"location":"community/development-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Git</li> <li>(Optional) Docker</li> </ul>"},{"location":"community/development-setup/#clone-repository","title":"Clone Repository","text":"<pre><code>git clone https://github.com/bordumb/jnkn.git\ncd jnkn\n</code></pre>"},{"location":"community/development-setup/#install-dependencies","title":"Install Dependencies","text":""},{"location":"community/development-setup/#using-pip","title":"Using pip","text":"<pre><code># Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # or .venv\\Scripts\\activate on Windows\n\n# Install with all dependencies\npip install -e \".[full,dev]\"\n</code></pre>"},{"location":"community/development-setup/#using-uv-recommended","title":"Using uv (Recommended)","text":"<pre><code>uv venv\nsource .venv/bin/activate\nuv pip install -e \".[full,dev]\"\n</code></pre>"},{"location":"community/development-setup/#verify-installation","title":"Verify Installation","text":"<pre><code># CLI works\njnkn --version\n\n# Tests pass\npytest tests/unit/ -v\n\n# Linter passes\nruff check .\n</code></pre>"},{"location":"community/development-setup/#project-structure","title":"Project Structure","text":"<pre><code>jnkn/\n\u251c\u2500\u2500 src/jnkn/           # Main package\n\u2502   \u251c\u2500\u2500 cli/              # CLI commands\n\u2502   \u251c\u2500\u2500 core/             # Core types, graph, storage\n\u2502   \u251c\u2500\u2500 parsing/          # Language parsers\n\u2502   \u2502   \u251c\u2500\u2500 python/\n\u2502   \u2502   \u251c\u2500\u2500 terraform/\n\u2502   \u2502   \u2514\u2500\u2500 kubernetes/\n\u2502   \u251c\u2500\u2500 stitching/        # Cross-domain linking\n\u2502   \u2514\u2500\u2500 analysis/         # Blast radius, explain, diff\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 unit/             # Unit tests\n\u2502   \u251c\u2500\u2500 integration/      # Integration tests\n\u2502   \u2514\u2500\u2500 e2e/              # End-to-end tests\n\u251c\u2500\u2500 docs/                 # Documentation\n\u2514\u2500\u2500 pyproject.toml        # Project configuration\n</code></pre>"},{"location":"community/development-setup/#running-tests","title":"Running Tests","text":"<pre><code># All tests\npytest\n\n# Unit tests only\npytest tests/unit/\n\n# Specific test file\npytest tests/unit/test_confidence.py\n\n# With coverage\npytest --cov=jnkn --cov-report=html\nopen htmlcov/index.html\n</code></pre>"},{"location":"community/development-setup/#running-linter","title":"Running Linter","text":"<pre><code># Check\nruff check .\n\n# Fix automatically\nruff check --fix .\n\n# Format\nruff format .\n</code></pre>"},{"location":"community/development-setup/#building-documentation","title":"Building Documentation","text":"<pre><code># Install docs dependencies\npip install mkdocs-material mkdocstrings-python\n\n# Serve locally\nmkdocs serve\n\n# Build\nmkdocs build\n</code></pre>"},{"location":"community/development-setup/#ide-setup","title":"IDE Setup","text":""},{"location":"community/development-setup/#vs-code","title":"VS Code","text":"<p>Recommended extensions: - Python - Ruff - Even Better TOML</p> <p>Settings (<code>.vscode/settings.json</code>):</p> <pre><code>{\n  \"python.defaultInterpreterPath\": \".venv/bin/python\",\n  \"python.testing.pytestEnabled\": true,\n  \"editor.formatOnSave\": true,\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"charliermarsh.ruff\"\n  }\n}\n</code></pre>"},{"location":"community/development-setup/#pycharm","title":"PyCharm","text":"<ul> <li>Set Project Interpreter to <code>.venv</code></li> <li>Enable pytest as test runner</li> <li>Configure Ruff as external tool</li> </ul>"},{"location":"community/development-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"community/development-setup/#tree-sitter-not-found","title":"Tree-sitter not found","text":"<pre><code>pip install tree-sitter tree-sitter-languages\n</code></pre>"},{"location":"community/development-setup/#tests-fail-with-import-error","title":"Tests fail with import error","text":"<pre><code>pip install -e \".[full,dev]\"\n</code></pre>"},{"location":"community/development-setup/#permission-denied-on-scripts","title":"Permission denied on scripts","text":"<pre><code>chmod +x scripts/*.sh\n</code></pre>"},{"location":"community/release-process/","title":"Release Process","text":"<p>This document outlines the steps required to publish a new version of <code>jnkn</code> to PyPI and update the documentation site.</p>"},{"location":"community/release-process/#versioning-strategy","title":"Versioning Strategy","text":"<p><code>jnkn</code> follows Semantic Versioning 2.0.0:</p> <ul> <li>MAJOR version when you make incompatible API changes.</li> <li>MINOR version when you add functionality in a backward-compatible manner.</li> <li>PATCH version when you make backward-compatible bug fixes.</li> </ul> <p>Pre-releases: For beta or release candidates, append a suffix like <code>-beta.1</code> or <code>-rc.1</code>. * Example: <code>0.5.0-rc.1</code></p>"},{"location":"community/release-process/#prerequisites","title":"Prerequisites","text":"<p>To perform a release, you must have:</p> <ol> <li>Write access to the GitHub repository.</li> <li>Admin access on PyPI (or be part of the trusted publisher workflow).</li> </ol>"},{"location":"community/release-process/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"community/release-process/#1-prepare-the-release","title":"1. Prepare the Release","text":"<p>Before tagging, ensure the codebase is ready.</p> <ol> <li> <p>Update the Changelog:     Edit <code>docs/changelog/CHANGELOG.md</code> to move \"Unreleased\" changes into a new version header.</p> </li> <li> <p>Bump the Version:     Update the <code>version</code> field in <code>pyproject.toml</code>.     <pre><code>[project]\nname = \"jnkn\"\nversion = \"0.6.0\"  # &lt;--- Update this\n</code></pre></p> </li> <li> <p>Commit the Changes: <pre><code>git add pyproject.toml docs/changelog/CHANGELOG.md\ngit commit -m \"chore: bump version to 0.6.0\"\n</code></pre></p> </li> </ol>"},{"location":"community/release-process/#2-tag-and-push","title":"2. Tag and Push","text":"<p>Create a git tag for the version. The tag must match the version in <code>pyproject.toml</code>.</p> <p>```bash</p>"},{"location":"community/release-process/#create-an-annotated-tag","title":"Create an annotated tag","text":"<p>git tag -a v0.6.0 -m \"Release v0.6.0\"</p>"},{"location":"community/release-process/#push-the-commit-and-the-tag","title":"Push the commit and the tag","text":"<p>git push origin main --follow-tags ````</p>"},{"location":"community/release-process/#3-create-github-release","title":"3. Create GitHub Release","text":"<p>This is the trigger for our deployment pipeline.</p> <ol> <li>Go to the Releases page on GitHub.</li> <li>Click Draft a new release.</li> <li>Choose a tag: Select the tag you just pushed (<code>v0.6.0</code>).</li> <li>Release title: Use the version number (<code>v0.6.0</code>).</li> <li>Description: Click \"Generate release notes\" to auto-populate from PRs, or paste the relevant section from your <code>CHANGELOG.md</code>.</li> <li>Click Publish release.</li> </ol>"},{"location":"community/release-process/#automated-pipeline","title":"Automated Pipeline","text":"<p>Once you click \"Publish release\", GitHub Actions takes over.</p>"},{"location":"community/release-process/#1-pypi-publication","title":"1. PyPI Publication","text":"<ul> <li>Workflow: <code>.github/workflows/pypi-publish.yml</code></li> <li>Trigger: Release published.</li> <li>Action:<ul> <li>Builds the package using <code>uv build</code>.</li> <li>Authenticates with PyPI using OIDC (Trusted Publishing).</li> <li>Uploads the <code>.whl</code> and <code>.tar.gz</code> files to PyPI.</li> </ul> </li> </ul>"},{"location":"community/release-process/#2-documentation-deployment","title":"2. Documentation Deployment","text":"<ul> <li>Workflow: <code>.github/workflows/docs-deploy.yml</code></li> <li>Trigger: Push to <code>main</code> (which happened in Step 2) or Release.</li> <li>Action:<ul> <li>Installs documentation dependencies.</li> <li>Builds the MkDocs site.</li> <li>Deploys the static HTML to the <code>gh-pages</code> branch.</li> <li>Updates the <code>latest</code> version alias.</li> </ul> </li> </ul>"},{"location":"community/release-process/#verification","title":"Verification","text":"<p>After the pipelines complete (usually \\~2-3 minutes), verify the release:</p> <ol> <li>PyPI: Check pypi.org/project/jnkn/ to see the new version.</li> <li>Installation: Run <code>pip install --upgrade jnkn</code> locally to ensure it installs correctly.</li> <li>Docs: Visit docs.jnkn.io and ensure the version dropdown (if enabled) or the changelog reflects the update.</li> </ol>"},{"location":"community/roadmap/","title":"Roadmap","text":"<p>Future plans.</p>"},{"location":"community/testing-guide/","title":"Testing Guide","text":"<p>Running and writing tests.</p>"},{"location":"explanation/","title":"Explanation","text":"<p>Understand how and why Jnkn works.</p>"},{"location":"explanation/#architecture","title":"Architecture","text":"<ul> <li>Overview \u2014 High-level system design</li> <li>Parsing Pipeline \u2014 How code becomes a graph</li> <li>Stitching Engine \u2014 How cross-domain links are made</li> <li>Confidence Model \u2014 How match quality is scored</li> </ul>"},{"location":"explanation/#concepts","title":"Concepts","text":"<ul> <li>Dependency Graph \u2014 The core data structure</li> <li>Blast Radius \u2014 Understanding impact analysis</li> <li>Cross-Domain Dependencies \u2014 The problem Jnkn solves</li> </ul>"},{"location":"explanation/#design-decisions","title":"Design Decisions","text":"<ul> <li>Why Tree-Sitter \u2014 Parser technology choice</li> <li>Why Token Matching \u2014 Matching strategy rationale</li> </ul>"},{"location":"explanation/#security","title":"Security","text":"<ul> <li>Threat Model \u2014 Security considerations</li> </ul>"},{"location":"explanation/architecture/confidence-model/","title":"Confidence Model","text":"<p>How Jnkn scores match quality.</p>"},{"location":"explanation/architecture/confidence-model/#overview","title":"Overview","text":"<p>Confidence answers: \"How likely is this match correct?\"</p> <pre><code>Final Score = Base Signal \u00d7 \u03a0(Penalties)\n</code></pre> <p>A score of 0.85 means \"85% confident this is a real dependency.\"</p>"},{"location":"explanation/architecture/confidence-model/#signals","title":"Signals","text":"<p>Signals are positive indicators that increase base confidence.</p> Signal Score Description <code>exact_match</code> 1.0 Names are identical <code>normalized_match</code> 0.9 Names equal after normalization <code>token_overlap_high</code> 0.85 &gt;80% of tokens overlap <code>token_overlap_medium</code> 0.7 50-80% overlap <code>suffix_match</code> 0.6 Names end with same tokens <code>prefix_match</code> 0.6 Names start with same tokens <code>contains</code> 0.5 One name contains the other <code>single_token</code> 0.4 Only one token matches <p>Only the highest signal is used (they don't stack).</p>"},{"location":"explanation/architecture/confidence-model/#penalties","title":"Penalties","text":"<p>Penalties reduce confidence when matches are weak.</p> Penalty Multiplier Condition <code>short_token</code> 0.5 Any matched token &lt; 4 chars <code>common_token</code> 0.7 Only generic words matched <code>ambiguous</code> 0.8 Multiple targets could match <code>low_value</code> 0.9 Matched low-value tokens <p>Penalties stack multiplicatively.</p>"},{"location":"explanation/architecture/confidence-model/#calculation-example","title":"Calculation Example","text":"<p>Match: <code>env:DB_HOST</code> \u2192 <code>infra:db_host</code></p> <pre><code>Tokens:\n  Source: [db, host]\n  Target: [db, host]\n\nSignal:\n  normalized_match \u2192 0.9 (names equal after normalization)\n\nPenalties:\n  short_token (db=2 chars) \u2192 \u00d70.5\n\nFinal:\n  0.9 \u00d7 0.5 = 0.45\n</code></pre> <p>With threshold 0.5, this match is rejected (0.45 &lt; 0.5).</p>"},{"location":"explanation/architecture/confidence-model/#another-example","title":"Another Example","text":"<p>Match: <code>env:PAYMENT_DATABASE_URL</code> \u2192 <code>infra:payment_db_instance</code></p> <pre><code>Tokens:\n  Source: [payment, database, url]\n  Target: [payment, db, instance]\n\nOverlap: [payment] (1 of 3)\n\nSignal:\n  single_token \u2192 0.4\n\nPenalties:\n  none\n\nFinal:\n  0.4\n</code></pre> <p>With threshold 0.5, this is rejected.</p>"},{"location":"explanation/architecture/confidence-model/#high-confidence-match","title":"High-Confidence Match","text":"<p>Match: <code>env:STRIPE_API_KEY</code> \u2192 <code>infra:var.stripe_api_key</code></p> <pre><code>Tokens:\n  Source: [stripe, api, key]\n  Target: [stripe, api, key]\n\nSignal:\n  normalized_match \u2192 0.9\n\nPenalties:\n  short_token (api=3, key=3) \u2192 \u00d70.5 \u00d7 0.5 = \u00d70.25? \n\n  No! Penalty applies once per category, not per token.\n  short_token \u2192 \u00d70.5\n\nFinal:\n  0.9 \u00d7 0.5 = 0.45\n</code></pre> <p>Hmm, this is still rejected. Let's tune:</p>"},{"location":"explanation/architecture/confidence-model/#tuning-confidence","title":"Tuning Confidence","text":"<p>If good matches are being rejected, you have options:</p>"},{"location":"explanation/architecture/confidence-model/#lower-the-threshold","title":"Lower the threshold","text":"<pre><code>stitching:\n  min_confidence: 0.4\n</code></pre>"},{"location":"explanation/architecture/confidence-model/#adjust-signal-weights","title":"Adjust signal weights","text":"<pre><code>stitching:\n  confidence:\n    signals:\n      normalized_match: 0.95  # Increase from 0.9\n</code></pre>"},{"location":"explanation/architecture/confidence-model/#reduce-penalty-severity","title":"Reduce penalty severity","text":"<pre><code>stitching:\n  confidence:\n    penalties:\n      short_token: 0.7  # Less harsh than 0.5\n</code></pre>"},{"location":"explanation/architecture/confidence-model/#dont-penalize-specific-tokens","title":"Don't penalize specific tokens","text":"<pre><code>stitching:\n  min_token_length: 2  # Allow 2-char tokens\n</code></pre>"},{"location":"explanation/architecture/confidence-model/#threshold-guidelines","title":"Threshold Guidelines","text":"Threshold Use Case 0.3 Discovery mode \u2014 find all possible links 0.5 Balanced (default) 0.7 Conservative \u2014 high precision 0.9 Very strict \u2014 only obvious matches"},{"location":"explanation/architecture/confidence-model/#the-tradeoff","title":"The Tradeoff","text":"<pre><code>Lower threshold \u2192 More matches \u2192 More false positives\nHigher threshold \u2192 Fewer matches \u2192 More false negatives\n</code></pre> <p>Jnkn's philosophy: False positives are worse than false negatives. A developer investigating a bogus alert wastes time. Missing a dependency is caught during testing or review.</p>"},{"location":"explanation/architecture/confidence-model/#inspecting-confidence","title":"Inspecting Confidence","text":"<pre><code>jnkn explain env:DB_HOST infra:db_host\n</code></pre> <p>Shows full breakdown of signals and penalties.</p>"},{"location":"explanation/architecture/graph-algorithms/","title":"Graph Algorithms","text":"<p>Traversal, pathfinding, and analysis algorithms in jnkn.</p>"},{"location":"explanation/architecture/graph-algorithms/#architectural-overview","title":"Architectural Overview","text":"<p>jnkn's graph engine uses a robust, interface-driven architecture backed by high-performance rustworkx. This design decouples analysis logic (Blast Radius, Trace, Diff) from storage and processing, enabling backend agnosticism while leveraging Rust's speed for heavy traversals.</p>"},{"location":"explanation/architecture/graph-algorithms/#the-igraph-interface","title":"The IGraph Interface","text":"<p>All graph operations are defined by the <code>IGraph</code> protocol, ensuring consistent behavior across different backends (e.g., in-memory <code>rustworkx</code>, persistent stores, or test mocks).</p> <pre><code>class IGraph(Protocol):\n    \"\"\"Abstract contract for dependency graph implementations.\"\"\"\n\n    def get_descendants(self, node_id: str, max_depth: int = -1) -&gt; Set[str]: ...\n    def get_ancestors(self, node_id: str, max_depth: int = -1) -&gt; Set[str]: ...\n    def get_impacted_nodes(self, source_ids: List[str], max_depth: int = -1) -&gt; Set[str]: ...\n    def trace(self, source_id: str, target_id: str) -&gt; List[List[str]]: ...\n</code></pre>"},{"location":"explanation/architecture/graph-algorithms/#core-data-structure-dependencygraph","title":"Core Data Structure: DependencyGraph","text":"<p>The <code>DependencyGraph</code> class implements <code>IGraph</code> using <code>rustworkx.PyDiGraph</code> for speed, with secondary indexes for O(1) lookups.</p> <pre><code>class DependencyGraph(IGraph):\n    def __init__(self):\n        self._graph = rx.PyDiGraph()\n        # Map external string IDs to internal integer indices\n        self._id_to_idx: Dict[str, int] = {}\n        self._idx_to_node: Dict[int, Node] = {}\n        # Inverted index for fuzzy stitching\n        self.token_index = TokenIndex()\n</code></pre>"},{"location":"explanation/architecture/graph-algorithms/#semantic-impact-analysis","title":"Semantic Impact Analysis","text":"<p>Standard graph traversal (downstream/upstream) is insufficient for infrastructure-as-code analysis because dependency direction often opposes data flow direction.</p> <p>The Problem:</p> <ul> <li>Code: <code>App.py</code> --(READS)--&gt; <code>ENV_VAR</code></li> <li>Graph Edge: <code>App</code> -&gt; <code>Env</code></li> <li>Impact: If <code>Env</code> changes, <code>App</code> breaks.</li> <li>Standard Descendants: <code>Env</code> has no outgoing edges to <code>App</code>.</li> </ul> <p>The Solution: <code>get_impacted_nodes</code> jnkn implements a custom semantic BFS that traverses edges based on their logical data flow, not just graph direction.</p> <pre><code>def get_impacted_nodes(self, source_ids: List[str], max_depth: int = -1) -&gt; Set[str]:\n    \"\"\"\n    Calculate impact by traversing:\n    1. Downstream for data flow (PROVIDES, WRITES, FLOWS_TO)\n    2. Upstream for dependencies (READS, DEPENDS_ON) - Reverse traversal\n    \"\"\"\n    FORWARD_TYPES = {\"provides\", \"writes\", \"flows_to\", \"provisions\"}\n    REVERSE_TYPES = {\"reads\", \"depends_on\", \"calls\"}\n\n    # ... BFS implementation that flips direction based on edge type ...\n</code></pre> <p>This ensures that changing an infrastructure resource correctly \"blasts\" upwards to the applications that read its outputs.</p>"},{"location":"explanation/architecture/graph-algorithms/#algorithms-implementations","title":"Algorithms &amp; Implementations","text":""},{"location":"explanation/architecture/graph-algorithms/#1-blast-radius-semantic-bfs","title":"1. Blast Radius (Semantic BFS)","text":"<p>Finds all artifacts semantically affected by a change.</p> <ul> <li>Algorithm: Semantic Breadth-First Search (BFS)</li> <li>Complexity: $O(V + E)$</li> <li>Logic:<ul> <li>Start at changed nodes.</li> <li>Follow <code>PROVIDES</code> edges forward.</li> <li>Follow <code>READS</code> edges backward.</li> <li>Repeat until <code>max_depth</code> or exhaust.</li> </ul> </li> </ul>"},{"location":"explanation/architecture/graph-algorithms/#2-lineage-tracing-pathfinding","title":"2. Lineage Tracing (Pathfinding)","text":"<p>Finds the path explaining why Node A affects Node B.</p> <ul> <li>Algorithm: <code>rustworkx.all_simple_paths</code></li> <li>Complexity: $O(V + E)$ (highly optimized in Rust)</li> <li>Implementation:<ol> <li>Attempt forward semantic trace (following data flow).</li> <li>Fallback to reverse dependency check if direct flow is broken but a dependency exists.</li> </ol> </li> </ul> <pre><code>def trace(self, source_id: str, target_id: str) -&gt; List[List[str]]:\n    src_idx = self._id_to_idx[source_id]\n    tgt_idx = self._id_to_idx[target_id]\n    # Optimized Rust call\n    return rx.all_simple_paths(self._graph, src_idx, tgt_idx)\n</code></pre>"},{"location":"explanation/architecture/graph-algorithms/#3-stitching-fuzzy-matching","title":"3. Stitching (Fuzzy Matching)","text":"<p>Connects nodes across different domains (e.g., Terraform -&gt; Python) using token overlap.</p> <ul> <li>Structure: <code>TokenIndex</code> (Inverted Index)</li> <li>Complexity: $O(1)$ lookup per token</li> <li>Logic:<ul> <li>Tokenize node names (e.g., <code>PAYMENT_DB_HOST</code> -&gt; <code>payment</code>, <code>db</code>, <code>host</code>).</li> <li>Index nodes by tokens.</li> <li>Query index to find candidates sharing significant tokens.</li> </ul> </li> </ul>"},{"location":"explanation/architecture/graph-algorithms/#performance-improvements","title":"Performance Improvements","text":"<p>Moving to <code>rustworkx</code> provided significant gains over the previous NetworkX implementation:</p> Operation NetworkX (Old) rustworkx (New) Improvement Graph Construction Python Loop Rust Vector Ops \\~5x faster Pathfinding Pure Python BFS Parallel Rust \\~10-50x faster Memory Usage Heavy Python Objects Compact Rust Structs \\~3x reduction Descendants Recursive Python Optimized Native \\~100x faster"},{"location":"explanation/architecture/graph-algorithms/#future-enhancements","title":"Future Enhancements","text":""},{"location":"explanation/architecture/graph-algorithms/#1-weighted-impact-scoring","title":"1. Weighted Impact Scoring","text":"<p>Enhance <code>get_impacted_nodes</code> to decay impact score over distance: $$\\text{Score}(node) = \\frac{\\text{Confidence}(path)}{1 + \\text{Distance}}$$</p>"},{"location":"explanation/architecture/graph-algorithms/#2-cycle-detection-breaking","title":"2. Cycle Detection &amp; breaking","text":"<p>Use <code>rustworkx.simple_cycles</code> to detect circular dependencies (e.g., A reads B, B reads A) that might cause infinite analysis loops or deployment deadlocks.</p>"},{"location":"explanation/architecture/graph-algorithms/#3-parallel-subgraph-analysis","title":"3. Parallel Subgraph Analysis","text":"<p>For massive monorepos, partition the graph into disjoint subgraphs (using <code>rustworkx.connected_components</code>) and run semantic analysis on each component in parallel threads.</p>"},{"location":"explanation/architecture/overview/","title":"Architecture Overview","text":"<p>Jnkn's architecture in one page.</p>"},{"location":"explanation/architecture/overview/#system-diagram","title":"System Diagram","text":"<pre><code>graph TB\n    subgraph Input\n        F[Source Files]\n        C[Config]\n    end\n\n    subgraph \"Parsing Layer\"\n        PE[Parser Engine]\n        PP[Python Parser]\n        PT[Terraform Parser]\n        PK[K8s Parser]\n    end\n\n    subgraph \"Graph Layer\"\n        G[(Dependency&lt;br/&gt;Graph)]\n        S[Storage&lt;br/&gt;SQLite]\n    end\n\n    subgraph \"Stitching Layer\"\n        SE[Stitching Engine]\n        M[Matchers]\n        R[Rules]\n        CC[Confidence&lt;br/&gt;Calculator]\n    end\n\n    subgraph \"Analysis Layer\"\n        BR[Blast Radius]\n        EX[Explainer]\n        DF[Differ]\n    end\n\n    subgraph Output\n        CLI[CLI]\n        JSON[JSON]\n        SARIF[SARIF]\n    end\n\n    F --&gt; PE\n    C --&gt; PE\n    PE --&gt; PP &amp; PT &amp; PK\n    PP &amp; PT &amp; PK --&gt; G\n    G &lt;--&gt; S\n    G --&gt; SE\n    SE --&gt; M &amp; R\n    M --&gt; CC\n    R --&gt; CC\n    CC --&gt; G\n    G --&gt; BR &amp; EX &amp; DF\n    BR &amp; EX &amp; DF --&gt; CLI &amp; JSON &amp; SARIF</code></pre>"},{"location":"explanation/architecture/overview/#components","title":"Components","text":""},{"location":"explanation/architecture/overview/#parsing-layer","title":"Parsing Layer","text":"<p>Responsibility: Convert source files into nodes and edges.</p> <ul> <li>Parser Engine \u2014 Orchestrates file discovery and parser dispatch</li> <li>Language Parsers \u2014 Extract patterns from specific languages</li> <li>Extractors \u2014 Detect specific code patterns (e.g., <code>os.getenv</code>)</li> </ul>"},{"location":"explanation/architecture/overview/#graph-layer","title":"Graph Layer","text":"<p>Responsibility: Store and query the dependency graph.</p> <ul> <li>Dependency Graph \u2014 In-memory NetworkX graph for traversal</li> <li>Storage \u2014 SQLite for persistence and incremental updates</li> </ul>"},{"location":"explanation/architecture/overview/#stitching-layer","title":"Stitching Layer","text":"<p>Responsibility: Create cross-domain edges.</p> <ul> <li>Stitching Engine \u2014 Evaluates all node pairs against rules</li> <li>Matchers \u2014 Compute similarity between artifact names</li> <li>Rules \u2014 Define which node types can be linked</li> <li>Confidence Calculator \u2014 Score match quality</li> </ul>"},{"location":"explanation/architecture/overview/#analysis-layer","title":"Analysis Layer","text":"<p>Responsibility: Answer questions about the graph.</p> <ul> <li>Blast Radius \u2014 BFS traversal to find downstream impact</li> <li>Explainer \u2014 Show why matches were made/rejected</li> <li>Differ \u2014 Compare graphs across git refs</li> </ul>"},{"location":"explanation/architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"explanation/architecture/overview/#scan","title":"Scan","text":"<pre><code>Files \u2192 Parsers \u2192 Nodes/Edges \u2192 Graph \u2192 Stitching \u2192 Cross-Domain Edges \u2192 Storage\n</code></pre>"},{"location":"explanation/architecture/overview/#query","title":"Query","text":"<pre><code>User Query \u2192 Graph Load \u2192 Traversal \u2192 Results \u2192 Output Format\n</code></pre>"},{"location":"explanation/architecture/overview/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Incremental by default \u2014 Only re-scan changed files</li> <li>Explainable \u2014 Every match can be explained</li> <li>Configurable \u2014 Thresholds and rules are tunable</li> <li>Extensible \u2014 Add new parsers/rules without core changes</li> <li>Fast \u2014 SQLite + in-memory graph for speed</li> </ol>"},{"location":"explanation/architecture/overview/#technology-choices","title":"Technology Choices","text":"Component Technology Rationale Parsing Tree-sitter Language-agnostic, accurate AST Graph NetworkX Rich algorithms, Python-native Storage SQLite Zero-config, portable, fast CLI Click Standard for Python CLIs Config YAML Human-readable, version-controllable"},{"location":"explanation/architecture/parsing-pipeline/","title":"Parsing Pipeline","text":"<p>How source files become nodes and edges.</p>"},{"location":"explanation/architecture/parsing-pipeline/#pipeline-stages","title":"Pipeline Stages","text":"<pre><code>graph LR\n    F[File] --&gt; D{Discover}\n    D --&gt; R[Read]\n    R --&gt; P[Parse AST]\n    P --&gt; E[Extract]\n    E --&gt; N[Nodes]\n    E --&gt; Ed[Edges]</code></pre>"},{"location":"explanation/architecture/parsing-pipeline/#stage-1-discovery","title":"Stage 1: Discovery","text":"<p>The parser engine discovers files to scan:</p> <pre><code># Pseudocode\nfor file in walk_directory(root):\n    if not ignored(file) and supported_extension(file):\n        yield file\n</code></pre> <p>Filters applied: - <code>.jnknignore</code> patterns - Built-in ignores (<code>.git</code>, <code>node_modules</code>, etc.) - File extension whitelist</p>"},{"location":"explanation/architecture/parsing-pipeline/#stage-2-read-hash","title":"Stage 2: Read &amp; Hash","text":"<p>Files are read and hashed for incremental scanning:</p> <pre><code>content = file.read_bytes()\nhash = xxhash.xxh64(content).hexdigest()\n\nif hash == stored_hash:\n    skip()  # File unchanged\n</code></pre>"},{"location":"explanation/architecture/parsing-pipeline/#stage-3-parse-ast","title":"Stage 3: Parse AST","text":"<p>Tree-sitter parses the file into an Abstract Syntax Tree:</p> <pre><code>parser = get_parser(\"python\")\ntree = parser.parse(content)\n</code></pre> <p>The AST represents code structure:</p> <pre><code>(module\n  (expression_statement\n    (assignment\n      left: (identifier)  # DATABASE_URL\n      right: (call\n        function: (attribute\n          object: (identifier)   # os\n          attribute: (identifier))  # getenv\n        arguments: (argument_list\n          (string))))))  # \"DATABASE_URL\"\n</code></pre>"},{"location":"explanation/architecture/parsing-pipeline/#stage-4-extract","title":"Stage 4: Extract","text":"<p>Extractors run tree-sitter queries to find patterns:</p> <pre><code>query = language.query(\"\"\"\n(call\n  function: (attribute\n    object: (identifier) @obj\n    attribute: (identifier) @method)\n  arguments: (argument_list (string) @env_var)\n  (#eq? @obj \"os\")\n  (#eq? @method \"getenv\"))\n\"\"\")\n\nfor match in query.captures(tree.root_node):\n    yield EnvVar(name=match.text)\n</code></pre> <p>Extractors run in priority order: 1. StdlibExtractor (100) \u2014 <code>os.getenv</code>, <code>os.environ</code> 2. PydanticExtractor (90) \u2014 BaseSettings 3. ClickTyperExtractor (80) \u2014 <code>envvar=</code> 4. HeuristicExtractor (10) \u2014 Fallback patterns</p>"},{"location":"explanation/architecture/parsing-pipeline/#stage-5-yield-nodes-edges","title":"Stage 5: Yield Nodes &amp; Edges","text":"<p>Extractors yield graph elements:</p> <pre><code># Node: The env var itself\nyield Node(\n    id=\"env:DATABASE_URL\",\n    type=NodeType.ENV_VAR,\n    metadata={\"line\": 10}\n)\n\n# Edge: File reads this env var\nyield Edge(\n    source=\"file://src/config.py\",\n    target=\"env:DATABASE_URL\",\n    type=RelationshipType.READS\n)\n</code></pre>"},{"location":"explanation/architecture/parsing-pipeline/#complete-example","title":"Complete Example","text":"<p>Input file <code>src/config.py</code>:</p> <pre><code>import os\n\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nREDIS_HOST = os.environ.get(\"REDIS_HOST\", \"localhost\")\n</code></pre> <p>Output:</p> <pre><code>Nodes:\n  - file://src/config.py (code_file)\n  - env:DATABASE_URL (env_var)\n  - env:REDIS_HOST (env_var)\n\nEdges:\n  - file://src/config.py \u2192 env:DATABASE_URL (reads)\n  - file://src/config.py \u2192 env:REDIS_HOST (reads)\n</code></pre>"},{"location":"explanation/architecture/parsing-pipeline/#fallback-regex-parsing","title":"Fallback: Regex Parsing","text":"<p>When tree-sitter isn't available, regex patterns are used:</p> <pre><code>ENV_VAR_PATTERNS = [\n    (r'os\\.getenv\\s*\\(\\s*[\"\\']([^\"\\']+)[\"\\']', \"os.getenv\"),\n    (r'os\\.environ\\.get\\s*\\(\\s*[\"\\']([^\"\\']+)[\"\\']', \"os.environ.get\"),\n    ...\n]\n</code></pre> <p>Regex is less accurate but provides broad compatibility.</p>"},{"location":"explanation/architecture/parsing-pipeline/#adding-a-new-extractor","title":"Adding a New Extractor","text":"<ol> <li>Implement <code>BaseExtractor</code></li> <li>Define tree-sitter query or regex pattern</li> <li>Register with parser</li> <li>Extractors are automatically run in priority order</li> </ol> <p>See Custom Parsers Tutorial.</p>"},{"location":"explanation/architecture/stitching-engine/","title":"Stitching Engine","text":"<p>How cross-domain links are discovered.</p>"},{"location":"explanation/architecture/stitching-engine/#the-problem","title":"The Problem","text":"<p>After parsing, we have nodes from different domains:</p> <pre><code>Python: env:DATABASE_URL, env:REDIS_HOST\nTerraform: infra:aws_db_instance.main, infra:aws_elasticache.redis\n</code></pre> <p>These are semantically related but not explicitly connected. Stitching finds these implicit dependencies.</p>"},{"location":"explanation/architecture/stitching-engine/#how-it-works","title":"How It Works","text":"<pre><code>graph TD\n    N[All Nodes] --&gt; R{Rules Filter}\n    R --&gt; |Candidate Pairs| M[Matcher]\n    M --&gt; C[Confidence Calculator]\n    C --&gt; |Score \u2265 Threshold| E[Create Edge]\n    C --&gt; |Score &lt; Threshold| X[Reject]</code></pre>"},{"location":"explanation/architecture/stitching-engine/#stage-1-rule-filtering","title":"Stage 1: Rule Filtering","text":"<p>Rules define which node pairs to consider:</p> <pre><code>class EnvVarToInfraRule(StitchingRule):\n    source_types = {NodeType.ENV_VAR}\n    target_types = {NodeType.INFRA_RESOURCE}\n\n    def should_evaluate(self, source, target):\n        # Quick filter before expensive matching\n        return len(source.tokens) &gt; 0 and len(target.tokens) &gt; 0\n</code></pre> <p>This reduces the search space from O(n\u00b2) to relevant pairs.</p>"},{"location":"explanation/architecture/stitching-engine/#stage-2-token-extraction","title":"Stage 2: Token Extraction","text":"<p>Names are tokenized for comparison:</p> <pre><code>\"DATABASE_URL\"     \u2192 [\"database\", \"url\"]\n\"aws_db_instance\"  \u2192 [\"aws\", \"db\", \"instance\"]\n\"REDIS_HOST\"       \u2192 [\"redis\", \"host\"]\n</code></pre> <p>Tokenization rules: - Split on <code>_</code>, <code>-</code>, case changes - Lowercase everything - Remove blocked tokens (<code>id</code>, <code>key</code>, etc.) - Filter tokens shorter than <code>min_token_length</code></p>"},{"location":"explanation/architecture/stitching-engine/#stage-3-matching","title":"Stage 3: Matching","text":"<p>The matcher computes similarity:</p> <pre><code>source_tokens = {\"database\", \"url\"}\ntarget_tokens = {\"database\", \"instance\"}\n\noverlap = source_tokens &amp; target_tokens  # {\"database\"}\nscore = len(overlap) / max(len(source_tokens), len(target_tokens))\n# score = 1/2 = 0.5\n</code></pre> <p>Match types:</p> Type Condition Base Score Exact Normalized names equal 1.0 Normalized Lowercased names equal 0.9 High Overlap &gt;80% token overlap 0.85 Medium Overlap 50-80% overlap 0.7 Suffix Names end the same 0.6 Contains One name contains the other 0.5"},{"location":"explanation/architecture/stitching-engine/#stage-4-confidence-calculation","title":"Stage 4: Confidence Calculation","text":"<p>Penalties adjust the base score:</p> <pre><code>base_score = 0.85  # High token overlap\n\npenalties = [\n    0.5 if any(len(t) &lt; 4 for t in matched_tokens) else 1.0,  # Short token\n    0.7 if matched_only_common_tokens else 1.0,  # Common tokens\n]\n\nfinal_score = base_score * product(penalties)\n# 0.85 * 0.5 = 0.425\n</code></pre>"},{"location":"explanation/architecture/stitching-engine/#stage-5-edge-creation","title":"Stage 5: Edge Creation","text":"<p>If confidence \u2265 threshold, create an edge:</p> <pre><code>if confidence.score &gt;= min_confidence:\n    graph.add_edge(\n        source=source.id,\n        target=target.id,\n        type=RelationshipType.REFERENCES,\n        metadata={\n            \"rule\": rule.name,\n            \"confidence\": confidence.score,\n            \"matched_tokens\": matched_tokens,\n        }\n    )\n</code></pre>"},{"location":"explanation/architecture/stitching-engine/#example-walkthrough","title":"Example Walkthrough","text":"<p>Input: - <code>env:DATABASE_URL</code> (from Python) - <code>infra:output.database_url</code> (from Terraform)</p> <p>Step 1: Rule check - EnvVarToInfraRule matches (env_var \u2192 infra_resource)</p> <p>Step 2: Tokenize - Source: <code>[\"database\", \"url\"]</code> - Target: <code>[\"database\", \"url\"]</code></p> <p>Step 3: Match - Overlap: <code>{\"database\", \"url\"}</code> = 100% - Type: Normalized match (names equal after normalization)</p> <p>Step 4: Confidence - Base: 0.9 (normalized match) - Penalties: none - Final: 0.9 (HIGH)</p> <p>Step 5: Create edge - <code>env:DATABASE_URL \u2192 infra:output.database_url</code> (confidence: 0.9)</p>"},{"location":"explanation/architecture/stitching-engine/#performance","title":"Performance","text":"<p>Stitching is O(n \u00d7 m) where n and m are node counts per type. Optimizations:</p> <ol> <li>Rule filtering \u2014 Only evaluate relevant pairs</li> <li>Token indexing \u2014 Hash tokens for fast lookup</li> <li>Early termination \u2014 Skip if no token overlap possible</li> <li>Batch processing \u2014 Process in chunks for large graphs</li> </ol>"},{"location":"explanation/architecture/stitching-engine/#adding-custom-rules","title":"Adding Custom Rules","text":"<p>See Custom Stitching Rules Tutorial.</p>"},{"location":"explanation/architecture/storage-model/","title":"Storage Model","text":"<p>SQLite schema and indexing strategy for jnkn's persistence layer.</p>"},{"location":"explanation/architecture/storage-model/#current-design","title":"Current Design","text":"<p>jnkn uses SQLite as its persistence layer, chosen for zero-configuration portability and single-file deployment. The storage system operates in two modes: an in-memory <code>MemoryStorage</code> adapter for testing and CI pipelines, and a persistent <code>SQLiteStorage</code> adapter for production use.</p>"},{"location":"explanation/architecture/storage-model/#schema-version-2","title":"Schema (Version 2)","text":"<p>The database consists of three core tables plus a schema versioning table for migrations:</p> <pre><code>-- Schema versioning for migrations\nCREATE TABLE schema_version (\n    version INTEGER PRIMARY KEY,\n    applied_at TEXT NOT NULL,\n    description TEXT\n);\n\n-- Nodes represent all entities (files, env vars, infra resources, etc.)\nCREATE TABLE nodes (\n    id TEXT PRIMARY KEY,           -- e.g., \"env:DB_HOST\", \"infra:payment_db\"\n    name TEXT NOT NULL,            -- Human-readable name\n    type TEXT NOT NULL,            -- NodeType enum value\n    path TEXT,                     -- File path (if applicable)\n    language TEXT,                 -- Source language (python, terraform, etc.)\n    file_hash TEXT,                -- For incremental scanning\n    tokens TEXT,                   -- JSON array of tokenized name parts\n    metadata TEXT,                 -- JSON blob for extensible data\n    created_at TEXT NOT NULL\n);\n\n-- Edges represent relationships between nodes\nCREATE TABLE edges (\n    source_id TEXT NOT NULL,\n    target_id TEXT NOT NULL,\n    type TEXT NOT NULL,            -- RelationshipType enum value\n    confidence REAL DEFAULT 1.0,   -- Match confidence score (0.0-1.0)\n    match_strategy TEXT,           -- How the match was made\n    metadata TEXT,                 -- JSON blob (matched_tokens, explanation)\n    created_at TEXT NOT NULL,\n    PRIMARY KEY (source_id, target_id, type)\n);\n\n-- File scan tracking for incremental updates\nCREATE TABLE scan_metadata (\n    file_path TEXT PRIMARY KEY,\n    file_hash TEXT NOT NULL,       -- Content hash for change detection\n    last_scanned TEXT NOT NULL,\n    node_count INTEGER DEFAULT 0,\n    edge_count INTEGER DEFAULT 0\n);\n</code></pre>"},{"location":"explanation/architecture/storage-model/#indexes","title":"Indexes","text":"<p>Strategic indexes support common query patterns:</p> <pre><code>-- Node lookups\nCREATE INDEX idx_nodes_type ON nodes(type);    -- Filter by NodeType\nCREATE INDEX idx_nodes_path ON nodes(path);    -- File-based queries\n\n-- Edge traversals\nCREATE INDEX idx_edges_source ON edges(source_id);    -- Forward traversal\nCREATE INDEX idx_edges_target ON edges(target_id);    -- Reverse traversal\nCREATE INDEX idx_edges_confidence ON edges(confidence); -- Confidence filtering\n</code></pre>"},{"location":"explanation/architecture/storage-model/#connection-configuration","title":"Connection Configuration","text":"<p>The SQLite connection uses performance-optimized pragmas:</p> <pre><code>conn.execute(\"PRAGMA journal_mode=WAL\")      # Write-ahead logging for concurrency\nconn.execute(\"PRAGMA foreign_keys=ON\")       # Referential integrity\nconn.execute(\"PRAGMA synchronous=NORMAL\")    # Balanced durability/performance\n</code></pre>"},{"location":"explanation/architecture/storage-model/#recursive-cte-traversals","title":"Recursive CTE Traversals","text":"<p>Graph traversals use recursive Common Table Expressions for efficient ancestor/descendant queries without loading the full graph into memory:</p> <pre><code>-- Descendants (blast radius calculation)\nWITH RECURSIVE descendants AS (\n    SELECT target_id as id, 1 as depth\n    FROM edges WHERE source_id = ?\n    UNION\n    SELECT e.target_id, d.depth + 1\n    FROM edges e JOIN descendants d ON e.source_id = d.id\n    WHERE d.depth &lt; ?  -- Optional depth limit\n)\nSELECT DISTINCT id FROM descendants;\n\n-- Ancestors (upstream impact)\nWITH RECURSIVE ancestors AS (\n    SELECT source_id as id, 1 as depth\n    FROM edges WHERE target_id = ?\n    UNION\n    SELECT e.source_id, a.depth + 1\n    FROM edges e JOIN ancestors a ON e.target_id = a.id\n    WHERE a.depth &lt; ?\n)\nSELECT DISTINCT id FROM ancestors;\n</code></pre>"},{"location":"explanation/architecture/storage-model/#batch-operations","title":"Batch Operations","text":"<p>The storage layer supports batch inserts for 10-100x faster bulk loading:</p> <pre><code>def save_nodes_batch(self, nodes: List[Node]) -&gt; int:\n    with self._connection() as conn:\n        conn.executemany(\"\"\"\n            INSERT OR REPLACE INTO nodes \n            (id, name, type, path, language, file_hash, tokens, metadata, created_at)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\", [(n.id, n.name, ...) for n in nodes])\n    return len(nodes)\n</code></pre>"},{"location":"explanation/architecture/storage-model/#storage-adapter-interface","title":"Storage Adapter Interface","text":"<p>Both storage backends implement the <code>StorageAdapter</code> abstract base class, enabling seamless switching between in-memory and persistent storage:</p> <pre><code>class StorageAdapter(ABC):\n    @abstractmethod\n    def save_node(self, node: Node) -&gt; None: ...\n    @abstractmethod\n    def load_graph(self) -&gt; DependencyGraph: ...\n    @abstractmethod\n    def query_descendants(self, node_id: str, max_depth: int) -&gt; List[str]: ...\n    # ... additional methods\n</code></pre>"},{"location":"explanation/architecture/storage-model/#scale-characteristics","title":"Scale Characteristics","text":"<p>SQLite performs well for typical project sizes:</p> Graph Size Load Time Traversal Notes &lt;10K nodes &lt;100ms &lt;10ms Excellent 10K-50K nodes 100-500ms 10-50ms Good 50K-100K nodes 500ms-2s 50-200ms Acceptable &gt;100K nodes &gt;2s &gt;200ms Consider alternatives <p>The primary bottleneck at scale is not SQLite itself but the in-memory NetworkX graph that gets rebuilt from storage.</p>"},{"location":"explanation/architecture/storage-model/#future-ideas","title":"Future Ideas","text":""},{"location":"explanation/architecture/storage-model/#short-term-improvements","title":"Short-term Improvements","text":"<p>Token Index Persistence: Currently the inverted token index exists only in memory. Persisting it would eliminate the need to rebuild during graph loading:</p> <pre><code>CREATE TABLE token_index (\n    token TEXT NOT NULL,\n    node_id TEXT NOT NULL,\n    PRIMARY KEY (token, node_id)\n);\nCREATE INDEX idx_token ON token_index(token);\n</code></pre> <p>Confidence-based Pruning: Add a materialized view or summary table for high-confidence edges only, reducing noise in traversals:</p> <pre><code>CREATE VIEW high_confidence_edges AS\nSELECT * FROM edges WHERE confidence &gt;= 0.7;\n</code></pre> <p>Incremental Edge Updates: Track which edges were created by which stitching rules, enabling targeted re-stitching when rules change.</p>"},{"location":"explanation/architecture/storage-model/#medium-term-rustworkx-integration","title":"Medium-term: rustworkx Integration","text":"<p>Replace NetworkX with rustworkx for 10-100x graph operation speedups while keeping SQLite for persistence. The hybrid architecture would look like:</p> <pre><code>SQLite (persistence) &lt;-&gt; rustworkx (in-memory operations) &lt;-&gt; Python API\n</code></pre> <p>Key benefits: parallel graph algorithms, better memory layout, native DAG operations.</p>"},{"location":"explanation/architecture/storage-model/#long-term-graph-database-evaluation","title":"Long-term: Graph Database Evaluation","text":"<p>For very large codebases (&gt;100K nodes, &gt;1M edges), evaluate dedicated graph databases:</p> Option Pros Cons Memgraph In-memory, Cypher-compatible, fast Requires separate process DuckDB Embedded, analytical queries Limited graph operations K\u00f9zu Embedded graph DB, Cypher Newer, smaller ecosystem Neo4j Mature, powerful Heavy, server-based <p>The migration path would maintain the <code>StorageAdapter</code> interface, allowing gradual rollout with fallback to SQLite.</p>"},{"location":"explanation/concepts/blast-radius/","title":"Blast Radius","text":"<p>Understanding impact analysis.</p>"},{"location":"explanation/concepts/blast-radius/#definition","title":"Definition","text":"<p>Blast radius is the set of artifacts affected by a change.</p> <p>If you modify <code>env:DATABASE_URL</code>, what else might break?</p> <pre><code>graph TD\n    E[env:DATABASE_URL]:::source --&gt; F1[connection.py]\n    E --&gt; F2[users.py]\n    E --&gt; T[aws_rds.main]\n    F1 --&gt; F3[api.py]\n\n    classDef source fill:#ff6b6b,stroke:#c92a2a</code></pre> <p>Blast radius of <code>env:DATABASE_URL</code>: 4 artifacts</p>"},{"location":"explanation/concepts/blast-radius/#how-its-calculated","title":"How It's Calculated","text":"<p>Jnkn uses Breadth-First Search (BFS) from the source node:</p> <pre><code>1. Start at source node\n2. Find all nodes with edges FROM source\n3. Add to \"impacted\" set\n4. Repeat from each impacted node\n5. Stop at max_depth (if set)\n</code></pre>"},{"location":"explanation/concepts/blast-radius/#direct-vs-transitive","title":"Direct vs. Transitive","text":"<p>Direct impact (depth 1): - <code>connection.py</code> \u2014 directly reads <code>DATABASE_URL</code> - <code>users.py</code> \u2014 directly reads <code>DATABASE_URL</code> - <code>aws_rds.main</code> \u2014 directly provides <code>DATABASE_URL</code></p> <p>Transitive impact (depth 2+): - <code>api.py</code> \u2014 imports <code>connection.py</code>, which reads <code>DATABASE_URL</code></p>"},{"location":"explanation/concepts/blast-radius/#interpreting-results","title":"Interpreting Results","text":"<pre><code>{\n  \"source_artifacts\": [\"env:DATABASE_URL\"],\n  \"total_impacted_count\": 4,\n  \"max_depth_reached\": 2\n}\n</code></pre>"},{"location":"explanation/concepts/blast-radius/#by-impact-size","title":"By Impact Size","text":"Impact Risk Level Action 1-3 Low Normal review 4-10 Medium Careful review, notify teams 10+ High Extra caution, staged rollout"},{"location":"explanation/concepts/blast-radius/#by-artifact-type","title":"By Artifact Type","text":"<pre><code>{\n  \"breakdown\": {\n    \"code\": [\"connection.py\", \"users.py\", \"api.py\"],\n    \"infra\": [\"aws_rds.main\"],\n    \"env\": [],\n    \"data\": []\n  }\n}\n</code></pre> <p>Infrastructure impact often needs more scrutiny than code impact.</p>"},{"location":"explanation/concepts/blast-radius/#limiting-depth","title":"Limiting Depth","text":"<p>For large graphs, limit traversal:</p> <pre><code># Only direct dependencies\njnkn blast env:X --max-depth 1\n\n# Up to 3 levels\njnkn blast env:X --max-depth 3\n</code></pre>"},{"location":"explanation/concepts/blast-radius/#multiple-sources","title":"Multiple Sources","text":"<p>Analyze combined impact:</p> <pre><code>jnkn blast env:DATABASE_URL env:REDIS_URL\n</code></pre> <p>Returns the union of all impacted artifacts.</p>"},{"location":"explanation/concepts/blast-radius/#use-cases","title":"Use Cases","text":""},{"location":"explanation/concepts/blast-radius/#pre-merge-review","title":"Pre-Merge Review","text":"<p>\"What does this PR affect?\"</p> <pre><code>jnkn blast file://terraform/rds.tf\n</code></pre>"},{"location":"explanation/concepts/blast-radius/#incident-response","title":"Incident Response","text":"<p>\"What could have caused this failure?\"</p> <pre><code>jnkn blast infra:aws_rds.main --reverse\n</code></pre>"},{"location":"explanation/concepts/blast-radius/#change-planning","title":"Change Planning","text":"<p>\"If we deprecate this env var, what needs updating?\"</p> <pre><code>jnkn blast env:LEGACY_API_KEY\n</code></pre>"},{"location":"explanation/concepts/blast-radius/#limitations","title":"Limitations","text":"<p>Blast radius shows potential impact, not guaranteed breakage:</p> <ul> <li>A code file might read an env var but handle missing values gracefully</li> <li>Infrastructure might be referenced but not actively used</li> <li>Some dependencies may be optional</li> </ul> <p>Use blast radius as a signal for investigation, not absolute truth.</p>"},{"location":"explanation/concepts/confidence-levels/","title":"Confidence Levels","text":"<p>Understanding HIGH, MEDIUM, and LOW confidence scores in jnkn.</p>"},{"location":"explanation/concepts/confidence-levels/#current-design","title":"Current Design","text":"<p>Every cross-domain edge in jnkn's dependency graph carries a confidence score between 0.0 and 1.0. This score reflects how certain jnkn is that the relationship is real, not a false positive. The confidence system is designed around a core principle: every match must be explainable.</p>"},{"location":"explanation/concepts/confidence-levels/#confidence-tiers","title":"Confidence Tiers","text":"Level Score Range Meaning Example HIGH 0.80 - 1.00 Strong evidence of relationship Exact/normalized name match MEDIUM 0.50 - 0.79 Likely related, some uncertainty 3+ significant token overlap LOW 0.00 - 0.49 Weak evidence, may be false positive Single token match with penalties <p>The default minimum threshold is 0.5, meaning only MEDIUM and HIGH confidence matches create edges.</p>"},{"location":"explanation/concepts/confidence-levels/#signal-based-scoring","title":"Signal-Based Scoring","text":"<p>Confidence is built from signals\u2014evidence that two artifacts are related:</p> <pre><code>class ConfidenceSignal(StrEnum):\n    EXACT_MATCH = \"exact_match\"           # \"db_host\" == \"db_host\"\n    NORMALIZED_MATCH = \"normalized_match\" # \"DB_HOST\" == \"db_host\" after normalization\n    TOKEN_OVERLAP_HIGH = \"token_overlap_high\"    # 3+ significant tokens shared\n    TOKEN_OVERLAP_MEDIUM = \"token_overlap_medium\" # 2 significant tokens shared\n    SUFFIX_MATCH = \"suffix_match\"         # target ends with source\n    PREFIX_MATCH = \"prefix_match\"         # target starts with source\n    CONTAINS = \"contains\"                 # target contains source (weak)\n    SINGLE_TOKEN = \"single_token\"         # Only 1 token match (weakest)\n</code></pre> <p>Each signal has a configurable weight:</p> <pre><code>signal_weights = {\n    ConfidenceSignal.EXACT_MATCH: 1.0,\n    ConfidenceSignal.NORMALIZED_MATCH: 0.9,\n    ConfidenceSignal.TOKEN_OVERLAP_HIGH: 0.8,\n    ConfidenceSignal.TOKEN_OVERLAP_MEDIUM: 0.6,\n    ConfidenceSignal.SUFFIX_MATCH: 0.7,\n    ConfidenceSignal.PREFIX_MATCH: 0.7,\n    ConfidenceSignal.CONTAINS: 0.4,\n    ConfidenceSignal.SINGLE_TOKEN: 0.2,\n}\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#penalty-system","title":"Penalty System","text":"<p>Penalties reduce confidence when matches have concerning characteristics:</p> <pre><code>class PenaltyType(StrEnum):\n    SHORT_TOKEN = \"short_token\"      # Tokens &lt; 4 chars are less reliable\n    COMMON_TOKEN = \"common_token\"    # Generic tokens like \"id\", \"host\", \"key\"\n    AMBIGUITY = \"ambiguity\"          # Multiple potential matches exist\n    LOW_VALUE_TOKEN = \"low_value_token\"  # Cloud prefixes like \"aws\", \"gcp\"\n</code></pre> <p>Penalty multipliers are applied multiplicatively:</p> <pre><code>penalty_multipliers = {\n    PenaltyType.SHORT_TOKEN: 0.5,     # Cuts score in half\n    PenaltyType.COMMON_TOKEN: 0.7,    # 30% reduction\n    PenaltyType.AMBIGUITY: 0.8,       # 20% reduction per alternative\n    PenaltyType.LOW_VALUE_TOKEN: 0.6, # 40% reduction\n}\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#common-and-low-value-tokens","title":"Common and Low-Value Tokens","text":"<p>Certain tokens are flagged as providing weak signal:</p> <p>Common tokens (very generic, match many things): <pre><code>common_tokens = {\n    \"id\", \"db\", \"host\", \"url\", \"key\", \"name\", \"type\", \"data\",\n    \"info\", \"temp\", \"test\", \"api\", \"app\", \"env\", \"var\", \"val\",\n    \"config\", \"setting\", \"path\", \"port\", \"user\", \"password\",\n    \"secret\", \"token\", \"auth\", \"log\", \"file\", \"dir\", \"src\",\n    \"dst\", \"in\", \"out\", \"err\", \"msg\", \"str\", \"int\", \"num\",\n}\n</code></pre></p> <p>Low-value tokens (provide some signal but reduced): <pre><code>low_value_tokens = {\n    \"aws\", \"gcp\", \"azure\", \"main\", \"default\", \"primary\",\n    \"production\", \"prod\", \"staging\", \"dev\", \"development\",\n    \"internal\", \"external\", \"public\", \"private\", \"local\",\n    \"remote\", \"master\", \"slave\", \"read\", \"write\",\n}\n</code></pre></p>"},{"location":"explanation/concepts/confidence-levels/#score-calculation","title":"Score Calculation","text":"<p>The <code>ConfidenceCalculator</code> combines signals and penalties:</p> <pre><code>class ConfidenceCalculator:\n    def calculate(\n        self,\n        source_name: str,\n        target_name: str,\n        source_tokens: List[str],\n        target_tokens: List[str],\n        matched_tokens: Optional[List[str]] = None,\n        alternative_match_count: int = 0,\n    ) -&gt; ConfidenceResult:\n        # 1. Evaluate all signals\n        signal_results = self._evaluate_signals(\n            source_name, target_name,\n            source_tokens, target_tokens,\n            matched_tokens\n        )\n\n        # 2. Evaluate penalties\n        penalty_results = self._evaluate_penalties(\n            matched_tokens, alternative_match_count\n        )\n\n        # 3. Calculate base score (max signal, not sum)\n        base_score = self._calculate_base_score(signal_results)\n\n        # 4. Apply penalties multiplicatively\n        final_score = self._apply_penalties(base_score, penalty_results)\n\n        return ConfidenceResult(score=final_score, ...)\n</code></pre> <p>Important: The base score uses the maximum signal weight, not the sum. This prevents multiple weak signals from inflating scores:</p> <pre><code>def _calculate_base_score(self, signal_results: List[SignalResult]) -&gt; float:\n    matched_weights = [s.weight for s in signal_results if s.matched]\n    if not matched_weights:\n        return 0.0\n\n    # Use max weight, with small bonus for additional signals\n    max_weight = max(matched_weights)\n    bonus = min(0.1, (len(matched_weights) - 1) * 0.02)\n\n    return min(1.0, max_weight + bonus)\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#explainability","title":"Explainability","text":"<p>Every confidence result includes a human-readable explanation:</p> <pre><code>result = calculator.calculate(\n    source_name=\"PAYMENT_DB_HOST\",\n    target_name=\"payment_db_host\",\n    source_tokens=[\"payment\", \"db\", \"host\"],\n    target_tokens=[\"payment\", \"db\", \"host\"]\n)\n\nprint(calculator.explain(result))\n</code></pre> <p>Output: <pre><code>Match: PAYMENT_DB_HOST \u2192 payment_db_host\nConfidence: 0.90\n\nSignals:\n  \u2713 normalized_match (0.90)\n    \u2192 'paymentdbhost' == 'paymentdbhost'\n\nPenalties: None\n\nScore Breakdown:\n  Base: 0.90\n  Final: 0.90\n</code></pre></p>"},{"location":"explanation/concepts/confidence-levels/#ambiguity-penalty-example","title":"Ambiguity Penalty Example","text":"<p>When multiple targets could match a source, confidence is reduced:</p> <pre><code># Source: DB_HOST\n# Potential targets: payment_db_host, orders_db_host, users_db_host\n\n# Each match gets penalized for ambiguity\n# penalty = 0.8 ** (1 + (alternative_count - 2) * 0.2)\n\n# With 3 alternatives:\n# penalty = 0.8 ** (1 + 0.2) = 0.8 ** 1.2 \u2248 0.76\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#real-world-examples","title":"Real-World Examples","text":""},{"location":"explanation/concepts/confidence-levels/#high-confidence-090","title":"HIGH Confidence (0.90)","text":"<pre><code>Source: env:STRIPE_API_KEY\nTarget: infra:stripe_api_key\n\nSignals:\n  \u2713 normalized_match (0.90)\n    \u2192 'stripeapikey' == 'stripeapikey'\n  \u2713 token_overlap_high (0.80)\n    \u2192 3 significant tokens: ['stripe', 'api', 'key']\n\nPenalties: None\n\nFinal Score: 0.90 (HIGH)\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#medium-confidence-063","title":"MEDIUM Confidence (0.63)","text":"<pre><code>Source: env:DB_CONNECTION_URL\nTarget: infra:database_url\n\nSignals:\n  \u2713 token_overlap_medium (0.60)\n    \u2192 2 significant tokens: ['db', 'url']\n\nPenalties:\n  - common_token (\u00d70.70)\n    \u2192 All matched tokens are common: ['db', 'url']\n  - short_token (\u00d70.50)\n    \u2192 Short tokens (&lt; 4 chars): ['db']\n\nScore Breakdown:\n  Base: 0.60\n  After penalties: 0.60 \u00d7 0.70 \u00d7 0.50 = 0.21\n\nWait, that's LOW. Let's recalculate with better tokens...\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#low-confidence-035","title":"LOW Confidence (0.35)","text":"<pre><code>Source: env:API_KEY\nTarget: infra:payment_service_key\n\nSignals:\n  \u2713 single_token (0.20)\n    \u2192 Single token match: ['key']\n\nPenalties:\n  - common_token (\u00d70.70)\n    \u2192 All matched tokens are common: ['key']\n  - ambiguity (\u00d70.64)\n    \u2192 Source has 5 potential matches\n\nScore Breakdown:\n  Base: 0.20\n  After penalties: 0.20 \u00d7 0.70 \u00d7 0.64 = 0.09\n\nFinal Score: 0.09 (LOW - filtered out)\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#configuration","title":"Configuration","text":"<p>Confidence thresholds are configurable in <code>.jnkn/config.yaml</code>:</p> <pre><code>confidence:\n  min_threshold: 0.5  # Only create edges above this score\n\n  # Override signal weights\n  signal_weights:\n    exact_match: 1.0\n    normalized_match: 0.9\n    token_overlap_high: 0.8\n    token_overlap_medium: 0.6\n\n  # Override penalty multipliers\n  penalty_multipliers:\n    short_token: 0.5\n    common_token: 0.7\n    ambiguity: 0.8\n\n  # Customize common tokens for your project\n  common_tokens:\n    - id\n    - key\n    - your_custom_prefix\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#future-ideas","title":"Future Ideas","text":""},{"location":"explanation/concepts/confidence-levels/#short-term-confidence-explanations-in-output","title":"Short-term: Confidence Explanations in Output","text":"<p>Include confidence breakdowns in CLI and JSON output:</p> <pre><code>{\n  \"edge\": {\n    \"source\": \"infra:payment_db_host\",\n    \"target\": \"env:PAYMENT_DB_HOST\",\n    \"confidence\": 0.90,\n    \"confidence_breakdown\": {\n      \"base_score\": 0.90,\n      \"signals\": [\n        {\"type\": \"normalized_match\", \"weight\": 0.90}\n      ],\n      \"penalties\": []\n    }\n  }\n}\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#short-term-project-specific-common-tokens","title":"Short-term: Project-Specific Common Tokens","text":"<p>Auto-detect common tokens from the codebase:</p> <pre><code>def detect_common_tokens(graph: DependencyGraph, threshold: float = 0.2) -&gt; Set[str]:\n    \"\"\"Tokens appearing in &gt;20% of nodes are likely common.\"\"\"\n    token_counts = Counter()\n    total_nodes = graph.node_count\n\n    for node in graph.iter_nodes():\n        for token in node.tokens:\n            token_counts[token] += 1\n\n    return {t for t, c in token_counts.items() if c / total_nodes &gt; threshold}\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#medium-term-confidence-calibration","title":"Medium-term: Confidence Calibration","text":"<p>Track false positive rates to adjust weights:</p> <pre><code>class ConfidenceCalibrator:\n    def record_feedback(self, edge: Edge, is_correct: bool):\n        \"\"\"Record user feedback on match quality.\"\"\"\n        self.feedback_log.append({\n            \"edge\": edge,\n            \"predicted_confidence\": edge.confidence,\n            \"actual_correct\": is_correct\n        })\n\n    def calibrate(self) -&gt; Dict[str, float]:\n        \"\"\"Adjust weights based on actual accuracy.\"\"\"\n        # If normalized_match has 95% accuracy but weight is 0.9,\n        # maybe increase to 0.95\n        pass\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#medium-term-multi-factor-scoring","title":"Medium-term: Multi-Factor Scoring","text":"<p>Add additional signals beyond name matching:</p> <ul> <li>Co-location: Files in same directory get bonus</li> <li>Co-change: Files that change together in commits</li> <li>Documentation: README or comments mentioning relationship</li> <li>Import patterns: Transitive dependency chains</li> </ul>"},{"location":"explanation/concepts/confidence-levels/#long-term-ml-based-confidence","title":"Long-term: ML-Based Confidence","text":"<p>Train a classifier on confirmed matches:</p> <pre><code>class MLConfidenceScorer:\n    def __init__(self, model_path: str):\n        self.model = load_model(model_path)\n\n    def predict(self, source: Node, target: Node) -&gt; float:\n        features = self._extract_features(source, target)\n        # Features: token overlap, edit distance, file proximity,\n        # node types, language pair, etc.\n        return self.model.predict_proba(features)[0][1]\n</code></pre>"},{"location":"explanation/concepts/confidence-levels/#long-term-confidence-decay","title":"Long-term: Confidence Decay","text":"<p>Reduce confidence for stale matches:</p> <pre><code>def calculate_with_decay(self, edge: Edge, last_confirmed: datetime) -&gt; float:\n    \"\"\"Reduce confidence for old, unconfirmed matches.\"\"\"\n    days_since_confirmation = (datetime.now() - last_confirmed).days\n    decay_factor = 0.99 ** (days_since_confirmation / 30)  # 1% decay per month\n    return edge.confidence * decay_factor\n</code></pre>"},{"location":"explanation/concepts/cross-domain-deps/","title":"Cross-Domain Dependencies","text":"<p>The \"glue\" problem Jnkn solves.</p>"},{"location":"explanation/concepts/cross-domain-deps/#the-problem","title":"The Problem","text":"<p>Modern systems span multiple domains:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Python    \u2502     \u2502  Terraform  \u2502     \u2502 Kubernetes  \u2502\n\u2502   Service   \u2502     \u2502   Infra     \u2502     \u2502  Manifests  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                    ??? Dependencies ???\n</code></pre> <p>Each domain has its own tools:</p> Domain Tools Blind Spot Python pytest, mypy Doesn't know about infra Terraform terraform plan Doesn't know what code uses resources Kubernetes kubectl, helm Doesn't know what secrets code expects <p>No tool checks across domains.</p>"},{"location":"explanation/concepts/cross-domain-deps/#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"explanation/concepts/cross-domain-deps/#scenario-1-renamed-variable","title":"Scenario 1: Renamed Variable","text":"<pre><code># Before\noutput \"db_host\" { value = aws_rds.main.endpoint }\n\n# After (renamed)\noutput \"database_host\" { value = aws_rds.main.endpoint }\n</code></pre> <pre><code># This will break!\nDB_HOST = os.getenv(\"DB_HOST\")  # Still expects old name\n</code></pre> <p>Terraform plan: \u2705 Success Python tests: \u2705 Success (mocked) Production: \ud83d\udca5 Crash</p>"},{"location":"explanation/concepts/cross-domain-deps/#scenario-2-deleted-secret","title":"Scenario 2: Deleted Secret","text":"<pre><code># Removed from Kubernetes\n# apiVersion: v1\n# kind: Secret\n# metadata:\n#   name: api-credentials\n</code></pre> <pre><code># Still expects the secret\nAPI_KEY = os.environ[\"API_KEY\"]  # KeyError in production\n</code></pre>"},{"location":"explanation/concepts/cross-domain-deps/#scenario-3-schema-change","title":"Scenario 3: Schema Change","text":"<pre><code>-- dbt model change\n-- Renamed column: user_id \u2192 customer_id\n</code></pre> <pre><code># API still queries old column\nquery = \"SELECT user_id FROM fct_orders\"  # Column doesn't exist\n</code></pre>"},{"location":"explanation/concepts/cross-domain-deps/#why-this-happens","title":"Why This Happens","text":"<ol> <li>No explicit links \u2014 Code references env vars by string, not by import</li> <li>Convention-based \u2014 <code>DATABASE_URL</code> in code is expected to match infra output</li> <li>Different ownership \u2014 Platform team manages infra, app team manages code</li> <li>Async changes \u2014 Changes merged at different times</li> </ol>"},{"location":"explanation/concepts/cross-domain-deps/#how-jnkn-helps","title":"How Jnkn Helps","text":"<p>Jnkn creates implicit links based on naming:</p> <pre><code>graph LR\n    subgraph Python\n        E[env:DATABASE_URL]\n    end\n\n    subgraph Terraform\n        T[output.database_url]\n    end\n\n    E -.-&gt;|\"token match&lt;br/&gt;0.92 confidence\"| T</code></pre> <p>Now when you run:</p> <pre><code>jnkn blast infra:output.database_url\n</code></pre> <p>You see:</p> <pre><code>Impacted: env:DATABASE_URL, file://src/config.py\n</code></pre>"},{"location":"explanation/concepts/cross-domain-deps/#the-stitching-process","title":"The Stitching Process","text":"<pre><code>graph TD\n    P[Parse All Files] --&gt; N[Extract Nodes]\n    N --&gt; T[Tokenize Names]\n    T --&gt; M[Match Tokens]\n    M --&gt; C[Calculate Confidence]\n    C --&gt; E[Create Edges]</code></pre> <ol> <li>Parse \u2014 Find all env vars, resources, etc.</li> <li>Tokenize \u2014 <code>DATABASE_URL</code> \u2192 <code>[database, url]</code></li> <li>Match \u2014 Find nodes with overlapping tokens</li> <li>Score \u2014 Calculate match confidence</li> <li>Link \u2014 Create edges above threshold</li> </ol>"},{"location":"explanation/concepts/cross-domain-deps/#limitations","title":"Limitations","text":"<p>Jnkn uses lexical matching, not semantic understanding:</p> Can Detect Cannot Detect <code>DB_HOST</code> \u2194 <code>db_host</code> IAM role \u2192 S3 permission <code>REDIS_URL</code> \u2194 <code>redis_cluster</code> Table size \u2192 IOPS limit Name-based connections Behavioral dependencies <p>For semantic understanding, you need runtime observability or explicit documentation.</p>"},{"location":"explanation/concepts/cross-domain-deps/#best-practices","title":"Best Practices","text":"<ol> <li>Use consistent naming \u2014 <code>DATABASE_URL</code> everywhere, not <code>DB_URL</code> in one place</li> <li>Run Jnkn in CI \u2014 Catch cross-domain breaks before merge</li> <li>Review high-impact changes \u2014 Pay attention to blast radius</li> <li>Document exceptions \u2014 Suppress known false positives with reasons</li> </ol>"},{"location":"explanation/concepts/dependency-graph/","title":"Dependency Graph","text":"<p>The core data structure of Jnkn.</p>"},{"location":"explanation/concepts/dependency-graph/#what-is-it","title":"What Is It?","text":"<p>The dependency graph is a directed graph that maps the relationships between every artifact in your stack. It serves as the \"source of truth\" for impact analysis.</p> <p>Crucially, Jnkn distinguishes between Dependency Direction (structural) and Impact Direction (semantic):</p> <ul> <li>Dependency: <code>App</code> \u2192 reads \u2192 <code>Env Var</code> (The App depends on the Env Var)</li> <li>Impact: <code>Env Var</code> \u2192 impacts \u2192 <code>App</code> (Changing the Env Var breaks the App)</li> </ul> <pre><code>graph LR\n    subgraph \"Infrastructure Layer\"\n        T[infra:aws_db_instance] --\"PROVIDES\"--&gt; E[env:DB_HOST]\n    end\n    subgraph \"Application Layer\"\n        C[file://config.py] --\"READS\"--&gt; E\n    end\n\n    style E fill:#f9f,stroke:#333,stroke-width:2px</code></pre>"},{"location":"explanation/concepts/dependency-graph/#the-igraph-interface","title":"The IGraph Interface","text":"<p>The graph is implemented via a strict IGraph Protocol, decoupling the analysis logic from the underlying storage.</p> <ul> <li>Backend Agnostic: Currently backed by high-performance rustworkx in memory, but swappable for NetworkX or Neo4j.</li> <li>Semantic Awareness: The graph itself understands lineage logic (e.g., traversing <code>READS</code> edges in reverse to find downstream consumers).</li> </ul>"},{"location":"explanation/concepts/dependency-graph/#nodes","title":"Nodes","text":"<p>Nodes represent unique artifacts across your stack.</p> <pre><code>{\n  \"id\": \"env:DATABASE_URL\",\n  \"name\": \"DATABASE_URL\",\n  \"type\": \"env_var\",\n  \"tokens\": [\"database\", \"url\"],\n  \"metadata\": {\n    \"source\": \"os.getenv\",\n    \"file\": \"src/config.py\"\n  }\n}\n</code></pre>"},{"location":"explanation/concepts/dependency-graph/#node-types","title":"Node Types","text":"Type Represents Example ID <code>code_file</code> Source Code File <code>file://src/app.py</code> <code>env_var</code> Configuration Key <code>env:DATABASE_URL</code> <code>infra_resource</code> Infrastructure Resource <code>infra:aws_rds:main</code> <code>config_key</code> Terraform Output <code>infra:output:db_host</code> <code>data_asset</code> Table or S3 Path <code>data:public.users</code>"},{"location":"explanation/concepts/dependency-graph/#edges-semantics","title":"Edges &amp; Semantics","text":"<p>Edges are directed, but their semantic meaning dictates how analysis tools traverse them.</p> Edge Type Direction Semantic Flow Example <code>PROVIDES</code> A \u2192 B Downstream (A impacts B) Infra \u2192 Env Var <code>WRITES</code> A \u2192 B Downstream (A impacts B) Job \u2192 Table <code>READS</code> A \u2192 B Upstream (B impacts A) Code \u2192 Env Var <code>DEPENDS_ON</code> A \u2192 B Upstream (B impacts A) Job \u2192 Upstream Job"},{"location":"explanation/concepts/dependency-graph/#querying-the-graph","title":"Querying the Graph","text":""},{"location":"explanation/concepts/dependency-graph/#impact-analysis-blast-radius","title":"Impact Analysis (Blast Radius)","text":"<p>Calculates the \"Blast Radius\" by following data flow, not just dependencies.</p> <pre><code># Finds everything that breaks if this Env Var changes\njnkn blast env:DATABASE_URL\n</code></pre>"},{"location":"explanation/concepts/dependency-graph/#lineage-tracing","title":"Lineage Tracing","text":"<p>Traces the full path of data from Infrastructure to Code to Data Assets.</p> <pre><code># Traces: Infra -&gt; Output -&gt; Env Var -&gt; App Code\njnkn trace infra:aws_db_instance:main file://src/app.py\n</code></pre>"},{"location":"explanation/concepts/dependency-graph/#storage","title":"Storage","text":"<ul> <li>Persistence: Serialized to SQLite (<code>.jnkn/jnkn.db</code>) for caching.</li> <li>Runtime: Hydrated into an in-memory <code>rustworkx</code> graph for sub-millisecond traversal.</li> </ul>"},{"location":"explanation/concepts/tokens-and-matching/","title":"Token Matching","text":"<p>Fuzzy matching logic for discovering cross-domain dependencies.</p>"},{"location":"explanation/concepts/tokens-and-matching/#current-design","title":"Current Design","text":"<p>jnkn's core innovation is connecting artifacts across technology domains (Python code, Terraform infrastructure, Kubernetes configs, dbt models) through heuristic token matching. When exact references don't exist in code, we infer relationships by matching naming patterns.</p>"},{"location":"explanation/concepts/tokens-and-matching/#the-problem","title":"The Problem","text":"<p>Consider this real-world scenario:</p> <pre><code># Python application code\ndb_host = os.getenv(\"PAYMENT_DB_HOST\")\n</code></pre> <pre><code># Terraform infrastructure\noutput \"payment_db_host\" {\n  value = aws_db_instance.payment.address\n}\n</code></pre> <p>There's no import statement, no explicit reference\u2014just a naming convention that humans recognize as related. jnkn's token matching engine detects this relationship automatically.</p>"},{"location":"explanation/concepts/tokens-and-matching/#normalization","title":"Normalization","text":"<p>Before matching, names are normalized to remove formatting differences:</p> <pre><code>@staticmethod\ndef normalize(name: str) -&gt; str:\n    \"\"\"\n    Normalize a name by lowercasing and removing separators.\n\n    Examples:\n        \"Payment_DB_Host\" -&gt; \"paymentdbhost\"\n        \"payment-db-host\" -&gt; \"paymentdbhost\"\n        \"PAYMENT.DB.HOST\" -&gt; \"paymentdbhost\"\n    \"\"\"\n    result = name.lower()\n    for sep in [\"_\", \".\", \"-\", \"/\", \":\"]:\n        result = result.replace(sep, \"\")\n    return result\n</code></pre> <p>This allows matching across different casing conventions: - <code>PAYMENT_DB_HOST</code> (SCREAMING_SNAKE_CASE in env vars) - <code>payment_db_host</code> (snake_case in Terraform) - <code>PaymentDbHost</code> (PascalCase in some configs)</p>"},{"location":"explanation/concepts/tokens-and-matching/#tokenization","title":"Tokenization","text":"<p>Names are split into constituent tokens for partial matching:</p> <pre><code>@staticmethod\ndef tokenize(name: str) -&gt; List[str]:\n    \"\"\"\n    Split a name into constituent tokens.\n\n    Examples:\n        \"Payment_DB_Host\" -&gt; ['payment', 'db', 'host']\n        \"PAYMENT_DATABASE_CONNECTION_URL\" -&gt; ['payment', 'database', 'connection', 'url']\n    \"\"\"\n    normalized = name.lower()\n    for sep in [\"_\", \".\", \"-\", \"/\", \":\"]:\n        normalized = normalized.replace(sep, \" \")\n    return [t.strip() for t in normalized.split() if t.strip()]\n</code></pre> <p>Tokens are stored on nodes during parsing and indexed for O(1) lookups during stitching.</p>"},{"location":"explanation/concepts/tokens-and-matching/#match-strategies","title":"Match Strategies","text":"<p>The <code>MatchConfig</code> defines multiple matching strategies with different confidence weights:</p> <pre><code>class MatchConfig:\n    def __init__(self):\n        self.min_confidence = 0.5          # Minimum score to create edge\n        self.min_token_overlap = 2         # Minimum shared tokens\n        self.min_token_length = 2          # Ignore single-char tokens\n\n        self.strategy_weights = {\n            MatchStrategy.EXACT: 1.0,       # \"db_host\" == \"db_host\"\n            MatchStrategy.NORMALIZED: 0.95, # \"DB_HOST\" matches \"db_host\"\n            MatchStrategy.TOKEN_OVERLAP: 0.85,  # Share 2+ significant tokens\n            MatchStrategy.SUFFIX: 0.75,     # \"primary_db_host\" ends with \"db_host\"\n            MatchStrategy.PREFIX: 0.75,     # \"db_host_primary\" starts with \"db_host\"\n            MatchStrategy.CONTAINS: 0.6,    # \"my_db_host_v2\" contains \"db_host\"\n        }\n</code></pre>"},{"location":"explanation/concepts/tokens-and-matching/#jaccard-similarity","title":"Jaccard Similarity","text":"<p>Token overlap uses Jaccard similarity coefficient:</p> <pre><code>@staticmethod\ndef significant_token_overlap(\n    tokens1: List[str],\n    tokens2: List[str],\n    min_length: int = 2\n) -&gt; Tuple[List[str], float]:\n    \"\"\"\n    Calculate overlap between token lists.\n\n    Returns (overlapping_tokens, jaccard_score)\n    \"\"\"\n    # Filter insignificant tokens\n    sig1 = [t for t in tokens1 if len(t) &gt;= min_length]\n    sig2 = [t for t in tokens2 if len(t) &gt;= min_length]\n\n    set1 = set(sig1)\n    set2 = set(sig2)\n    overlap = set1 &amp; set2\n    union = set1 | set2\n\n    if not union:\n        return [], 0.0\n\n    jaccard = len(overlap) / len(union)\n    return list(overlap), jaccard\n</code></pre>"},{"location":"explanation/concepts/tokens-and-matching/#stitching-rules","title":"Stitching Rules","text":"<p>The <code>EnvVarToInfraRule</code> is the primary stitching rule, connecting environment variables to infrastructure resources:</p> <pre><code>class EnvVarToInfraRule(StitchingRule):\n    \"\"\"Links environment variables to infrastructure resources.\"\"\"\n\n    def apply(self, graph: DependencyGraph) -&gt; List[Edge]:\n        edges = []\n\n        # Get all env var nodes and infra nodes\n        env_nodes = graph.get_nodes_by_type(NodeType.ENV_VAR)\n        infra_nodes = graph.get_nodes_by_type(NodeType.INFRA_RESOURCE)\n        infra_nodes.extend(graph.get_nodes_by_type(NodeType.CONFIG_KEY))\n\n        # Build lookup structures\n        infra_by_normalized: Dict[str, List[Node]] = defaultdict(list)\n        infra_by_tokens: Dict[str, List[Node]] = defaultdict(list)\n\n        for infra in infra_nodes:\n            normalized = TokenMatcher.normalize(infra.name)\n            infra_by_normalized[normalized].append(infra)\n\n            for token in infra.tokens:\n                if len(token) &gt;= self.config.min_token_length:\n                    infra_by_tokens[token].append(infra)\n\n        # Match each env var\n        for env in env_nodes:\n            best_matches = {}\n\n            # Strategy 1: Normalized match (high confidence)\n            for infra in infra_by_normalized.get(TokenMatcher.normalize(env.name), []):\n                match = MatchResult(\n                    source_node=infra.id,\n                    target_node=env.id,\n                    strategy=MatchStrategy.NORMALIZED,\n                    confidence=0.95,\n                    matched_tokens=env.tokens\n                )\n                self._update_best_match(best_matches, infra.id, match)\n\n            # Strategy 2: Token overlap (medium confidence)\n            for infra_id in self._find_token_candidates(env.tokens, infra_by_tokens):\n                infra = graph.get_node(infra_id)\n                overlap, score = TokenMatcher.significant_token_overlap(\n                    env.tokens, infra.tokens, self.config.min_token_length\n                )\n\n                if len(overlap) &gt;= self.config.min_token_overlap:\n                    match = MatchResult(\n                        source_node=infra_id,\n                        target_node=env.id,\n                        strategy=MatchStrategy.TOKEN_OVERLAP,\n                        confidence=min(0.85, score + len(overlap) * 0.1),\n                        matched_tokens=overlap\n                    )\n                    self._update_best_match(best_matches, infra_id, match)\n\n            # Create edges for matches above threshold\n            for infra_id, match in best_matches.items():\n                if match.confidence &gt;= self.config.min_confidence:\n                    edges.append(Edge(\n                        source_id=infra_id,\n                        target_id=env.id,\n                        type=RelationshipType.PROVIDES,\n                        confidence=match.confidence,\n                        metadata={\n                            \"rule\": \"EnvVarToInfraRule\",\n                            \"matched_tokens\": match.matched_tokens,\n                            \"explanation\": match.explanation\n                        }\n                    ))\n\n        return edges\n</code></pre>"},{"location":"explanation/concepts/tokens-and-matching/#best-match-selection","title":"Best Match Selection","text":"<p>When multiple infrastructure resources could match an environment variable, the highest-confidence match wins:</p> <pre><code>def _update_best_match(\n    self,\n    best_matches: Dict[str, MatchResult],\n    target_id: str,\n    match: MatchResult\n):\n    \"\"\"Keep only the highest-confidence match per target.\"\"\"\n    existing = best_matches.get(target_id)\n    if existing is None or match.confidence &gt; existing.confidence:\n        best_matches[target_id] = match\n</code></pre>"},{"location":"explanation/concepts/tokens-and-matching/#infratoinfrarule","title":"InfraToInfraRule","text":"<p>A secondary rule discovers relationships between infrastructure resources based on naming patterns:</p> <pre><code>class InfraToInfraRule(StitchingRule):\n    \"\"\"Links infrastructure resources to each other.\"\"\"\n\n    def apply(self, graph: DependencyGraph) -&gt; List[Edge]:\n        # Determines direction based on resource type hierarchy:\n        # output &gt; variable &gt; data &gt; resource &gt; module\n        pass\n</code></pre>"},{"location":"explanation/concepts/tokens-and-matching/#configuration","title":"Configuration","text":"<p>Token matching behavior is configurable via <code>.jnkn/config.yaml</code>:</p> <pre><code>stitching:\n  min_confidence: 0.5      # Threshold for creating edges\n  min_token_overlap: 2     # Minimum shared tokens\n  min_token_length: 2      # Ignore tiny tokens\n\n  # Override default weights\n  strategy_weights:\n    exact: 1.0\n    normalized: 0.95\n    token_overlap: 0.85\n    suffix: 0.75\n    prefix: 0.75\n    contains: 0.6\n</code></pre>"},{"location":"explanation/concepts/tokens-and-matching/#future-ideas","title":"Future Ideas","text":""},{"location":"explanation/concepts/tokens-and-matching/#short-term-configurable-stop-words","title":"Short-term: Configurable Stop Words","text":"<p>Add project-specific common tokens to ignore:</p> <pre><code>stitching:\n  ignore_tokens:\n    - \"aws\"\n    - \"prod\"\n    - \"staging\"\n    - \"primary\"\n    - \"main\"\n</code></pre>"},{"location":"explanation/concepts/tokens-and-matching/#short-term-semantic-similarity","title":"Short-term: Semantic Similarity","text":"<p>Use edit distance (Levenshtein) for typo tolerance:</p> <pre><code>def fuzzy_match(name1: str, name2: str, threshold: float = 0.8) -&gt; bool:\n    \"\"\"Match names that are similar but not identical.\"\"\"\n    from difflib import SequenceMatcher\n    ratio = SequenceMatcher(None, name1.lower(), name2.lower()).ratio()\n    return ratio &gt;= threshold\n</code></pre>"},{"location":"explanation/concepts/tokens-and-matching/#medium-term-context-aware-matching","title":"Medium-term: Context-Aware Matching","text":"<p>Consider file proximity and project structure:</p> <pre><code>def context_bonus(source_path: str, target_path: str) -&gt; float:\n    \"\"\"Boost confidence for co-located files.\"\"\"\n    source_dir = Path(source_path).parent\n    target_dir = Path(target_path).parent\n\n    if source_dir == target_dir:\n        return 0.1  # Same directory\n    elif source_dir.parent == target_dir.parent:\n        return 0.05  # Sibling directories\n    return 0.0\n</code></pre>"},{"location":"explanation/concepts/tokens-and-matching/#medium-term-pattern-learning","title":"Medium-term: Pattern Learning","text":"<p>Learn project-specific naming conventions from confirmed matches:</p> <pre><code>class PatternLearner:\n    def learn_from_confirmed(self, source_name: str, target_name: str):\n        \"\"\"Extract transformation pattern from user-confirmed match.\"\"\"\n        # e.g., \"PAYMENT_DB_HOST\" -&gt; \"payment_db_host\"\n        # Learn: SCREAMING_SNAKE -&gt; snake_case is a valid pattern\n        pass\n\n    def suggest_matches(self, name: str) -&gt; List[str]:\n        \"\"\"Apply learned patterns to suggest potential matches.\"\"\"\n        pass\n</code></pre>"},{"location":"explanation/concepts/tokens-and-matching/#long-term-embedding-based-matching","title":"Long-term: Embedding-Based Matching","text":"<p>Use code embeddings for semantic similarity beyond lexical matching:</p> <pre><code>class EmbeddingMatcher:\n    def __init__(self, model: str = \"code-search-ada\"):\n        self.encoder = load_embedding_model(model)\n        self.index = None\n\n    def build_index(self, nodes: List[Node]):\n        \"\"\"Build vector index from node names and context.\"\"\"\n        embeddings = []\n        for node in nodes:\n            context = f\"{node.name} {node.path} {' '.join(node.tokens)}\"\n            embeddings.append(self.encoder.encode(context))\n        self.index = build_faiss_index(embeddings)\n\n    def find_similar(self, query: str, k: int = 5) -&gt; List[Tuple[str, float]]:\n        \"\"\"Find semantically similar nodes.\"\"\"\n        query_vec = self.encoder.encode(query)\n        distances, indices = self.index.search(query_vec, k)\n        return [(self.nodes[i].id, 1 - d) for i, d in zip(indices, distances)]\n</code></pre>"},{"location":"explanation/concepts/tokens-and-matching/#long-term-multi-language-convention-mapping","title":"Long-term: Multi-Language Convention Mapping","text":"<p>Map naming conventions across language ecosystems:</p> Source Target Pattern Python (<code>DB_HOST</code>) Terraform (<code>db_host</code>) SCREAMING_SNAKE \u2192 snake_case Kubernetes (<code>db-host</code>) Python (<code>DB_HOST</code>) kebab-case \u2192 SCREAMING_SNAKE JavaScript (<code>dbHost</code>) Terraform (<code>db_host</code>) camelCase \u2192 snake_case <pre><code>class ConventionMapper:\n    def __init__(self):\n        self.transformations = {\n            (\"python\", \"terraform\"): self._screaming_to_snake,\n            (\"kubernetes\", \"python\"): self._kebab_to_screaming,\n            (\"javascript\", \"terraform\"): self._camel_to_snake,\n        }\n\n    def transform(self, name: str, from_lang: str, to_lang: str) -&gt; str:\n        \"\"\"Apply convention transformation.\"\"\"\n        key = (from_lang, to_lang)\n        if key in self.transformations:\n            return self.transformations[key](name)\n        return name\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/","title":"Extensibility","text":"<p>Plugin architecture for adding new parsers, stitching rules, and storage backends.</p>"},{"location":"explanation/design-decisions/extensibility-model/#current-design","title":"Current Design","text":"<p>jnkn is designed around abstract interfaces that allow extending core functionality without modifying the source code. The three primary extension points are parsers (for new languages/formats), stitching rules (for new relationship types), and storage adapters (for alternative backends).</p>"},{"location":"explanation/design-decisions/extensibility-model/#parser-registry","title":"Parser Registry","text":"<p>The <code>ParserRegistry</code> manages language parsers with support for both direct registration and entry-point discovery:</p> <pre><code>class ParserRegistry:\n    def __init__(self):\n        self._parsers: Dict[str, LanguageParser] = {}\n        self._extension_map: Dict[str, str] = {}\n        self._parser_factories: Dict[str, Type[LanguageParser]] = {}\n\n    def register(self, parser: LanguageParser) -&gt; None:\n        \"\"\"Register a parser instance.\"\"\"\n        name = parser.name\n        self._parsers[name] = parser\n\n        for ext in parser.extensions:\n            self._extension_map[ext.lower()] = name\n\n    def register_factory(self, name: str, factory: Type[LanguageParser]) -&gt; None:\n        \"\"\"Register a parser class for lazy instantiation.\"\"\"\n        self._parser_factories[name] = factory\n\n    def get_parser_for_extension(self, extension: str) -&gt; Optional[LanguageParser]:\n        \"\"\"Get the parser for a file extension.\"\"\"\n        parser_name = self._extension_map.get(extension.lower())\n        return self.get_parser(parser_name) if parser_name else None\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#languageparser-abstract-base-class","title":"LanguageParser Abstract Base Class","text":"<p>All parsers implement the <code>LanguageParser</code> interface:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass LanguageParser(ABC):\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Unique parser identifier (e.g., 'python', 'terraform').\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def extensions(self) -&gt; List[str]:\n        \"\"\"File extensions this parser handles (e.g., ['.py', '.pyw']).\"\"\"\n        pass\n\n    @abstractmethod\n    def parse(self, file_path: Path, content: str) -&gt; ParseResult:\n        \"\"\"\n        Parse file content and extract nodes/edges.\n\n        Args:\n            file_path: Path to the file being parsed\n            content: File content as string\n\n        Returns:\n            ParseResult containing discovered nodes and edges\n        \"\"\"\n        pass\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#creating-a-custom-parser","title":"Creating a Custom Parser","text":"<p>Example: Adding support for a new language (Go):</p> <pre><code>from jnkn.parsing.base import LanguageParser, ParseResult\nfrom jnkn.core.types import Node, Edge, NodeType\n\nclass GoParser(LanguageParser):\n    @property\n    def name(self) -&gt; str:\n        return \"go\"\n\n    @property\n    def extensions(self) -&gt; List[str]:\n        return [\".go\"]\n\n    def parse(self, file_path: Path, content: str) -&gt; ParseResult:\n        result = ParseResult()\n\n        # Add file node\n        file_node = Node(\n            id=f\"file://{file_path}\",\n            name=file_path.name,\n            type=NodeType.FILE,\n            path=str(file_path),\n            language=\"go\"\n        )\n        result.add_node(file_node)\n\n        # Extract environment variables: os.Getenv(\"VAR_NAME\")\n        import re\n        for match in re.finditer(r'os\\.Getenv\\([\"\\'](\\w+)[\"\\']\\)', content):\n            var_name = match.group(1)\n            env_node = Node(\n                id=f\"env:{var_name}\",\n                name=var_name,\n                type=NodeType.ENV_VAR,\n                path=str(file_path),\n                language=\"go\",\n                metadata={\"line\": content[:match.start()].count('\\n') + 1}\n            )\n            result.add_node(env_node)\n\n            # Link file to env var\n            result.add_edge(Edge(\n                source_id=file_node.id,\n                target_id=env_node.id,\n                type=RelationshipType.USES\n            ))\n\n        return result\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#entry-point-discovery","title":"Entry-Point Discovery","text":"<p>Parsers can be auto-discovered via Python entry points:</p> <pre><code># pyproject.toml\n[project.entry-points.\"jnkn.parsers\"]\ngo = \"my_package.parsers:GoParser\"\nrust = \"my_package.parsers:RustParser\"\n</code></pre> <p>The registry discovers these automatically:</p> <pre><code>def discover_parsers(self, entry_point_group: str = \"jnkn.parsers\") -&gt; int:\n    \"\"\"Auto-discover parsers via entry points.\"\"\"\n    from importlib.metadata import entry_points\n\n    eps = entry_points(group=entry_point_group)\n    discovered = 0\n\n    for ep in eps:\n        parser_class = ep.load()\n        if issubclass(parser_class, LanguageParser):\n            self.register_factory(ep.name, parser_class)\n            discovered += 1\n\n    return discovered\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#stitchingrule-abstract-base-class","title":"StitchingRule Abstract Base Class","text":"<p>Stitching rules discover cross-domain relationships:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass StitchingRule(ABC):\n    def __init__(self, config: Optional[MatchConfig] = None):\n        self.config = config or MatchConfig()\n\n    @abstractmethod\n    def get_name(self) -&gt; str:\n        \"\"\"Unique rule identifier.\"\"\"\n        pass\n\n    @abstractmethod\n    def apply(self, graph: DependencyGraph) -&gt; List[Edge]:\n        \"\"\"\n        Apply this rule to discover new edges.\n\n        Args:\n            graph: The dependency graph to analyze\n\n        Returns:\n            List of newly discovered edges\n        \"\"\"\n        pass\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#creating-a-custom-stitching-rule","title":"Creating a Custom Stitching Rule","text":"<p>Example: Linking Kubernetes ConfigMap keys to environment variables:</p> <pre><code>class ConfigMapToEnvRule(StitchingRule):\n    def get_name(self) -&gt; str:\n        return \"ConfigMapToEnvRule\"\n\n    def apply(self, graph: DependencyGraph) -&gt; List[Edge]:\n        edges = []\n\n        # Get ConfigMap keys and env vars\n        config_keys = graph.get_nodes_by_type(NodeType.CONFIG_KEY)\n        env_vars = graph.get_nodes_by_type(NodeType.ENV_VAR)\n\n        for config in config_keys:\n            for env in env_vars:\n                # Check if ConfigMap key matches env var name\n                overlap, score = TokenMatcher.significant_token_overlap(\n                    config.tokens, env.tokens\n                )\n\n                if len(overlap) &gt;= 2 and score &gt;= 0.5:\n                    edges.append(Edge(\n                        source_id=config.id,\n                        target_id=env.id,\n                        type=RelationshipType.PROVIDES,\n                        confidence=score,\n                        metadata={\n                            \"rule\": self.get_name(),\n                            \"matched_tokens\": overlap\n                        }\n                    ))\n\n        return edges\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#storageadapter-abstract-base-class","title":"StorageAdapter Abstract Base Class","text":"<p>Storage backends implement the <code>StorageAdapter</code> interface:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass StorageAdapter(ABC):\n    @abstractmethod\n    def save_node(self, node: Node) -&gt; None: ...\n\n    @abstractmethod\n    def save_nodes_batch(self, nodes: List[Node]) -&gt; int: ...\n\n    @abstractmethod\n    def load_node(self, node_id: str) -&gt; Optional[Node]: ...\n\n    @abstractmethod\n    def load_all_nodes(self) -&gt; List[Node]: ...\n\n    @abstractmethod\n    def load_graph(self) -&gt; DependencyGraph: ...\n\n    @abstractmethod\n    def query_descendants(self, node_id: str, max_depth: int = -1) -&gt; List[str]: ...\n\n    @abstractmethod\n    def query_ancestors(self, node_id: str, max_depth: int = -1) -&gt; List[str]: ...\n\n    @abstractmethod\n    def get_stats(self) -&gt; Dict[str, Any]: ...\n\n    @abstractmethod\n    def clear(self) -&gt; None: ...\n\n    @abstractmethod\n    def close(self) -&gt; None: ...\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#built-in-storage-adapters","title":"Built-in Storage Adapters","text":"<p>MemoryStorage: Fast, ephemeral storage for testing:</p> <pre><code>class MemoryStorage(StorageAdapter):\n    def __init__(self):\n        self._nodes: Dict[str, Node] = {}\n        self._edges: Dict[str, Edge] = {}\n        self._graph = DependencyGraph()\n</code></pre> <p>SQLiteStorage: Persistent storage with schema migrations:</p> <pre><code>class SQLiteStorage(StorageAdapter):\n    def __init__(self, db_path: Path):\n        self.db_path = db_path\n        self._init_db()\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#the-stitcher","title":"The Stitcher","text":"<p>The <code>Stitcher</code> class orchestrates rule execution:</p> <pre><code>class Stitcher:\n    def __init__(self, config: Optional[MatchConfig] = None):\n        self.config = config or MatchConfig()\n        self.rules: List[StitchingRule] = [\n            EnvVarToInfraRule(self.config),\n            InfraToInfraRule(self.config),\n        ]\n\n    def stitch(self, graph: DependencyGraph) -&gt; List[Edge]:\n        \"\"\"Apply all rules and return discovered edges.\"\"\"\n        all_edges = []\n        for rule in self.rules:\n            edges = rule.apply(graph)\n            all_edges.extend(edges)\n            for edge in edges:\n                graph.add_edge(edge)\n        return all_edges\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#parserengine","title":"ParserEngine","text":"<p>The <code>ParserEngine</code> orchestrates parsing across files:</p> <pre><code>class ParserEngine:\n    def __init__(self):\n        self._registry = ParserRegistry()\n\n    def register(self, parser: LanguageParser) -&gt; None:\n        self._registry.register(parser)\n\n    def parse_file(self, file_path: Path) -&gt; Iterator[Union[Node, Edge]]:\n        parser = self._registry.get_parser_for_file(file_path)\n        if parser:\n            content = file_path.read_text()\n            result = parser.parse(file_path, content)\n            yield from result.nodes\n            yield from result.edges\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#current-extension-points-summary","title":"Current Extension Points Summary","text":"Extension Point Interface Registration Language Parsers <code>LanguageParser</code> <code>ParserRegistry.register()</code> or entry points Stitching Rules <code>StitchingRule</code> Add to <code>Stitcher.rules</code> list Storage Backends <code>StorageAdapter</code> Pass to analysis components"},{"location":"explanation/design-decisions/extensibility-model/#future-ideas","title":"Future Ideas","text":""},{"location":"explanation/design-decisions/extensibility-model/#short-term-rule-entry-points","title":"Short-term: Rule Entry Points","text":"<p>Enable stitching rules to be discovered via entry points:</p> <pre><code># pyproject.toml\n[project.entry-points.\"jnkn.stitching_rules\"]\nconfigmap_env = \"my_package.rules:ConfigMapToEnvRule\"\nhelm_values = \"my_package.rules:HelmValuesToConfigRule\"\n</code></pre> <pre><code>class RuleRegistry:\n    def discover_rules(self, entry_point_group: str = \"jnkn.stitching_rules\"):\n        for ep in entry_points(group=entry_point_group):\n            rule_class = ep.load()\n            self.register(rule_class())\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#short-term-plugin-configuration","title":"Short-term: Plugin Configuration","text":"<p>Allow plugins to define their own configuration schema:</p> <pre><code># .jnkn/config.yaml\nplugins:\n  go_parser:\n    extract_imports: true\n    ignore_test_files: false\n\n  configmap_rule:\n    min_confidence: 0.6\n    namespace_aware: true\n</code></pre> <pre><code>class LanguageParser(ABC):\n    @abstractmethod\n    def configure(self, config: Dict[str, Any]) -&gt; None:\n        \"\"\"Apply plugin-specific configuration.\"\"\"\n        pass\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#medium-term-hook-system","title":"Medium-term: Hook System","text":"<p>Add lifecycle hooks for custom behavior:</p> <pre><code>class Hook(ABC):\n    @abstractmethod\n    def on_scan_start(self, paths: List[Path]) -&gt; None: ...\n\n    @abstractmethod\n    def on_file_parsed(self, path: Path, result: ParseResult) -&gt; None: ...\n\n    @abstractmethod\n    def on_stitch_complete(self, edges: List[Edge]) -&gt; None: ...\n\n    @abstractmethod\n    def on_analysis_complete(self, result: Dict[str, Any]) -&gt; None: ...\n\nclass HookRegistry:\n    def __init__(self):\n        self._hooks: List[Hook] = []\n\n    def register(self, hook: Hook) -&gt; None:\n        self._hooks.append(hook)\n\n    def emit(self, event: str, *args, **kwargs) -&gt; None:\n        for hook in self._hooks:\n            handler = getattr(hook, event, None)\n            if handler:\n                handler(*args, **kwargs)\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#medium-term-output-formatters","title":"Medium-term: Output Formatters","text":"<p>Pluggable output formats beyond JSON/SARIF/CSV:</p> <pre><code>class OutputFormatter(ABC):\n    @property\n    @abstractmethod\n    def name(self) -&gt; str: ...\n\n    @property\n    @abstractmethod\n    def file_extension(self) -&gt; str: ...\n\n    @abstractmethod\n    def format(self, result: AnalysisResult) -&gt; str: ...\n\n# Example: Custom HTML report formatter\nclass HTMLReportFormatter(OutputFormatter):\n    @property\n    def name(self) -&gt; str:\n        return \"html_report\"\n\n    @property\n    def file_extension(self) -&gt; str:\n        return \".html\"\n\n    def format(self, result: AnalysisResult) -&gt; str:\n        return self._render_template(result)\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#long-term-remote-storage-adapters","title":"Long-term: Remote Storage Adapters","text":"<p>Support for distributed graph storage:</p> <pre><code>class Neo4jStorage(StorageAdapter):\n    def __init__(self, uri: str, auth: Tuple[str, str]):\n        from neo4j import GraphDatabase\n        self._driver = GraphDatabase.driver(uri, auth=auth)\n\n    def query_descendants(self, node_id: str, max_depth: int) -&gt; List[str]:\n        with self._driver.session() as session:\n            result = session.run(\"\"\"\n                MATCH (n {id: $node_id})-[*1..]-&gt;(descendant)\n                RETURN DISTINCT descendant.id\n            \"\"\", node_id=node_id)\n            return [record[\"descendant.id\"] for record in result]\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#long-term-parser-composition","title":"Long-term: Parser Composition","text":"<p>Allow parsers to delegate to other parsers:</p> <pre><code>class CompositeParser(LanguageParser):\n    def __init__(self, parsers: List[LanguageParser]):\n        self._parsers = parsers\n\n    def parse(self, file_path: Path, content: str) -&gt; ParseResult:\n        combined = ParseResult()\n        for parser in self._parsers:\n            result = parser.parse(file_path, content)\n            combined.merge(result)\n        return combined\n\n# Example: Python file with embedded SQL\npython_sql_parser = CompositeParser([\n    PythonParser(),\n    EmbeddedSQLParser()  # Extracts SQL from strings\n])\n</code></pre>"},{"location":"explanation/design-decisions/extensibility-model/#long-term-custom-node-types","title":"Long-term: Custom Node Types","text":"<p>Allow plugins to define new node types:</p> <pre><code># Plugin defines custom type\nclass CustomNodeType(StrEnum):\n    KAFKA_TOPIC = \"kafka_topic\"\n    REDIS_KEY = \"redis_key\"\n    FEATURE_FLAG = \"feature_flag\"\n\n# Register with core types\nNodeTypeRegistry.register(CustomNodeType)\n\n# Now usable in parsers\nnode = Node(\n    id=\"kafka:user-events\",\n    name=\"user-events\",\n    type=CustomNodeType.KAFKA_TOPIC\n)\n</code></pre>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/","title":"Precision vs Recall","text":"<p>Philosophy on false positives and the trade-offs in dependency detection.</p>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#current-design","title":"Current Design","text":"<p>jnkn deliberately prioritizes precision over recall in its default configuration. This means we would rather miss some real dependencies (false negatives) than flood users with spurious connections (false positives). The reasoning is grounded in how developers actually use static analysis tools.</p>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#the-false-positive-problem","title":"The False Positive Problem","text":"<p>Industry research consistently shows that false positive rate is the single most important factor for developer adoption of static analysis tools. Google's Tricorder system found that developers ignore tools that \"cry wolf\"\u2014once trust is lost, the tool becomes noise rather than signal.</p> <p>DeepSource targets a &lt;5% false positive rate through aggressive post-processing and filtering. ESLint's success comes partly from its granular suppression system. SonarQube struggled with adoption until they added \"Quality Gates\" that let teams define acceptable noise levels.</p> <p>jnkn's heuristic token matching is inherently probabilistic. A match between <code>DB_HOST</code> and <code>db_host</code> might be correct, but it might also be coincidental if both are unrelated database configurations in different services.</p>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#the-threshold-decision","title":"The Threshold Decision","text":"<p>The default minimum confidence threshold of 0.5 represents our precision/recall trade-off:</p> <pre><code>class MatchConfig:\n    min_confidence: float = 0.5  # Default threshold\n</code></pre> <p>At this threshold: - Exact matches (confidence 1.0): Always included - Normalized matches (confidence 0.9): Always included - High token overlap (confidence 0.8): Always included - Medium token overlap (confidence 0.6): Usually included - Single token matches (confidence 0.2): Filtered out - Penalized matches: May fall below threshold</p>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#adjustable-thresholds","title":"Adjustable Thresholds","text":"<p>Users can tune the precision/recall trade-off in configuration:</p> <pre><code># .jnkn/config.yaml\n\n# High precision (fewer false positives, may miss some real deps)\nstitching:\n  min_confidence: 0.7\n\n# Balanced (default)\nstitching:\n  min_confidence: 0.5\n\n# High recall (more dependencies found, more false positives)\nstitching:\n  min_confidence: 0.3\n</code></pre>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#penalty-system-as-precision-tool","title":"Penalty System as Precision Tool","text":"<p>The penalty system specifically targets common false positive patterns:</p> <p>Short tokens (<code>db</code>, <code>id</code>, <code>key</code>) match too broadly: <pre><code>if len(token) &lt; 4:\n    confidence *= 0.5  # 50% penalty\n</code></pre></p> <p>Common tokens are generic and unreliable: <pre><code>common_tokens = {\"id\", \"db\", \"host\", \"url\", \"key\", \"name\", \"type\", \"data\", ...}\n\nif all(t in common_tokens for t in matched_tokens):\n    confidence *= 0.7  # 30% penalty\n</code></pre></p> <p>Ambiguous matches where multiple targets exist: <pre><code>if alternative_match_count &gt; 2:\n    confidence *= 0.8 ** (1 + (alternative_match_count - 2) * 0.2)\n</code></pre></p>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#suppression-system","title":"Suppression System","text":"<p>When false positives do occur, the suppression system allows permanent exclusion:</p> <pre><code># .jnkn/suppressions.yaml\nsuppressions:\n  - source_pattern: \"env:*_ID\"\n    target_pattern: \"infra:*\"\n    reason: \"Generic ID variables don't map to infrastructure\"\n\n  - source_pattern: \"env:LOG_LEVEL\"\n    target_pattern: \"infra:*\"\n    reason: \"Application config, not infrastructure\"\n</code></pre> <p>The CLI provides commands for managing suppressions:</p> <pre><code># Add a suppression\njnkn suppress add --source \"env:API_KEY\" --target \"infra:legacy_*\" --reason \"Unrelated\"\n\n# List active suppressions\njnkn suppress list\n\n# Remove a suppression\njnkn suppress remove --id \"supp_abc123\"\n</code></pre>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#explainability-for-triage","title":"Explainability for Triage","text":"<p>When users see unexpected matches, the <code>explain</code> command helps triage:</p> <pre><code>jnkn explain \"env:PAYMENT_DB_HOST\" \"infra:payment_db_host\"\n</code></pre> <p>Output: <pre><code>Match: PAYMENT_DB_HOST \u2192 payment_db_host\nConfidence: 0.90\n\nSignals:\n  \u2713 normalized_match (0.90)\n    \u2192 'paymentdbhost' == 'paymentdbhost'\n\nPenalties: None\n\nThis is likely a TRUE POSITIVE - high confidence, no penalties.\n</code></pre></p> <pre><code>jnkn explain \"env:DB_URL\" \"infra:cache_url\"\n</code></pre> <p>Output: <pre><code>Match: DB_URL \u2192 cache_url\nConfidence: 0.35\n\nSignals:\n  \u2713 single_token (0.20)\n    \u2192 Single token match: ['url']\n\nPenalties:\n  - common_token (\u00d70.70)\n    \u2192 All matched tokens are common: ['url']\n  - ambiguity (\u00d70.64)\n    \u2192 Source has 4 potential matches\n\nThis is likely a FALSE POSITIVE - low confidence, multiple penalties.\nConsider suppressing this match.\n</code></pre></p>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#the-why-not-explanation","title":"The \"Why Not\" Explanation","text":"<p>For debugging missing connections, <code>explain-why-not</code> shows why a match wasn't made:</p> <pre><code>jnkn explain-why-not \"env:CUSTOM_VAR\" \"infra:my_custom_variable\"\n</code></pre> <p>Output: <pre><code>WHY NO MATCH?\n\n\u2717 Score (0.42) is below threshold (0.50)\n\nDetails:\n  Source: env:CUSTOM_VAR\n  Target: infra:my_custom_variable\n  Source tokens: ['custom', 'var']\n  Target tokens: ['my', 'custom', 'variable']\n  Common tokens: ['custom']\n\n  ! Only 1 significant token overlap\n\n  To reach threshold, need +0.08 confidence\n\nOptions:\n  1. Lower min_confidence to 0.4\n  2. Add explicit edge in config\n  3. Rename to improve token alignment\n</code></pre></p>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#trade-off-philosophy","title":"Trade-off Philosophy","text":""},{"location":"explanation/design-decisions/false-positive-tradeoffs/#when-to-prioritize-precision","title":"When to Prioritize Precision","text":"<ul> <li>CI/CD gates: False positives block deployments, creating friction</li> <li>Large codebases: High node counts amplify false positive noise</li> <li>Compliance reporting: False positives create audit overhead</li> <li>Developer trust: Initial adoption requires low noise</li> </ul>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#when-to-prioritize-recall","title":"When to Prioritize Recall","text":"<ul> <li>Security audits: Missing a real dependency could be costly</li> <li>Migration planning: Need comprehensive impact understanding</li> <li>Exploration: Understanding unfamiliar codebases</li> <li>Small projects: Lower base rate makes false positives manageable</li> </ul>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#recommended-configurations","title":"Recommended Configurations","text":"<p>Conservative (CI/CD): <pre><code>stitching:\n  min_confidence: 0.7\n  min_token_overlap: 3\n</code></pre></p> <p>Balanced (General use): <pre><code>stitching:\n  min_confidence: 0.5\n  min_token_overlap: 2\n</code></pre></p> <p>Exploratory (Audits): <pre><code>stitching:\n  min_confidence: 0.3\n  min_token_overlap: 1\n</code></pre></p>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#future-ideas","title":"Future Ideas","text":""},{"location":"explanation/design-decisions/false-positive-tradeoffs/#short-term-confidence-bands-in-output","title":"Short-term: Confidence Bands in Output","text":"<p>Show confidence tiers in CLI output:</p> <pre><code>Blast Radius for infra:payment_db_host:\n\nHIGH CONFIDENCE (0.8+):\n  \u2192 env:PAYMENT_DB_HOST (0.90)\n  \u2192 file://services/payment/config.py (0.85)\n\nMEDIUM CONFIDENCE (0.5-0.8):\n  \u2192 env:DB_HOST (0.62)\n  \u2192 file://services/shared/database.py (0.55)\n\nLOW CONFIDENCE (hidden by default):\n  \u2192 env:HOST (0.35) [use --show-low to display]\n</code></pre>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#short-term-per-rule-thresholds","title":"Short-term: Per-Rule Thresholds","text":"<p>Different rules might warrant different thresholds:</p> <pre><code>stitching:\n  rules:\n    EnvVarToInfraRule:\n      min_confidence: 0.6  # Higher bar for env-to-infra\n    InfraToInfraRule:\n      min_confidence: 0.4  # Lower bar for infra-to-infra\n</code></pre>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#medium-term-feedback-loop","title":"Medium-term: Feedback Loop","text":"<p>Track accuracy over time based on user actions:</p> <pre><code>class FeedbackTracker:\n    def record_suppression(self, edge: Edge):\n        \"\"\"User suppressed = likely false positive.\"\"\"\n        self._false_positives.append(edge)\n\n    def record_confirmation(self, edge: Edge):\n        \"\"\"User confirmed = true positive.\"\"\"\n        self._true_positives.append(edge)\n\n    def calculate_precision(self) -&gt; float:\n        tp = len(self._true_positives)\n        fp = len(self._false_positives)\n        return tp / (tp + fp) if (tp + fp) &gt; 0 else 1.0\n</code></pre>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#medium-term-anomaly-detection","title":"Medium-term: Anomaly Detection","text":"<p>Flag matches that deviate from project patterns:</p> <pre><code>def detect_anomalous_match(edge: Edge, graph: DependencyGraph) -&gt; bool:\n    \"\"\"Flag if match is unusual compared to similar matches.\"\"\"\n    # If most env vars from services/payment/ link to infra:payment_*\n    # but this one links to infra:orders_*, it's anomalous\n    similar_sources = get_similar_sources(edge.source_id, graph)\n    typical_targets = get_typical_targets(similar_sources, graph)\n    return edge.target_id not in typical_targets\n</code></pre>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#long-term-ml-calibrated-thresholds","title":"Long-term: ML-Calibrated Thresholds","text":"<p>Train on labeled data to optimize threshold:</p> <pre><code>def optimize_threshold(\n    labeled_edges: List[Tuple[Edge, bool]],  # (edge, is_correct)\n    target_precision: float = 0.95\n) -&gt; float:\n    \"\"\"Find threshold that achieves target precision.\"\"\"\n    # Sort edges by confidence\n    sorted_edges = sorted(labeled_edges, key=lambda x: x[0].confidence, reverse=True)\n\n    # Find threshold where precision drops below target\n    tp, fp = 0, 0\n    for edge, is_correct in sorted_edges:\n        if is_correct:\n            tp += 1\n        else:\n            fp += 1\n        precision = tp / (tp + fp)\n        if precision &lt; target_precision:\n            return edge.confidence\n\n    return 0.0  # Include all if target met\n</code></pre>"},{"location":"explanation/design-decisions/false-positive-tradeoffs/#long-term-context-aware-precision","title":"Long-term: Context-Aware Precision","text":"<p>Adjust precision dynamically based on context:</p> <pre><code>def context_adjusted_threshold(\n    base_threshold: float,\n    file_path: Path,\n    graph: DependencyGraph\n) -&gt; float:\n    \"\"\"Adjust threshold based on file/project context.\"\"\"\n    # Critical paths need higher precision\n    if \"payment\" in str(file_path) or \"auth\" in str(file_path):\n        return base_threshold + 0.1\n\n    # Test files can tolerate lower precision\n    if \"test\" in str(file_path):\n        return base_threshold - 0.1\n\n    # High-connectivity nodes need higher precision\n    node_degree = len(graph.get_direct_dependents(f\"file://{file_path}\"))\n    if node_degree &gt; 10:\n        return base_threshold + 0.05\n\n    return base_threshold\n</code></pre>"},{"location":"explanation/design-decisions/why-sqlite/","title":"Why SQLite?","text":"<p>Storage strategy.</p>"},{"location":"explanation/design-decisions/why-token-matching/","title":"Why Token Matching","text":"<p>Rationale for our cross-domain linking strategy.</p>"},{"location":"explanation/design-decisions/why-token-matching/#the-problem","title":"The Problem","text":"<p>How do you know <code>env:DATABASE_URL</code> in Python relates to <code>output.database_url</code> in Terraform?</p> <p>There's no explicit link \u2014 it's a convention.</p>"},{"location":"explanation/design-decisions/why-token-matching/#the-alternatives","title":"The Alternatives","text":"Approach Accuracy Effort Coverage Manual annotation Perfect High Limited Semantic analysis High Very high Limited Token matching Good Low Broad"},{"location":"explanation/design-decisions/why-token-matching/#why-not-manual-annotation","title":"Why Not Manual Annotation?","text":"<pre><code># Manual mapping file\nlinks:\n  - source: env:DATABASE_URL\n    target: infra:output.database_url\n  - source: env:REDIS_HOST\n    target: infra:output.redis_endpoint\n</code></pre> <p>Problems:</p> <ol> <li>Doesn't scale \u2014 Hundreds of env vars \u00d7 resources</li> <li>Gets stale \u2014 New vars aren't added</li> <li>Human error \u2014 Typos, missed entries</li> <li>Defeats the purpose \u2014 If you tracked it manually, you wouldn't need Jnkn</li> </ol>"},{"location":"explanation/design-decisions/why-token-matching/#why-not-semantic-analysis","title":"Why Not Semantic Analysis?","text":"<p>Semantic analysis would understand that: - <code>aws_rds_instance</code> provides a database - <code>DATABASE_URL</code> is a database connection string - Therefore, they're related</p> <p>Problems:</p> <ol> <li>Requires domain knowledge \u2014 What does <code>aws_elasticache</code> provide?</li> <li>Computationally expensive \u2014 Need embeddings, inference</li> <li>Brittle \u2014 Custom resources, unusual naming</li> <li>Overkill \u2014 Names usually tell you what you need</li> </ol>"},{"location":"explanation/design-decisions/why-token-matching/#why-token-matching-works","title":"Why Token Matching Works","text":"<p>In practice, teams use consistent naming:</p> <pre><code>Python:          DATABASE_URL\nTerraform:       output.database_url\nKubernetes:      secret.database-url\nConfig file:     database_url\n\nTokens: [database, url] \u2014 identical!\n</code></pre> <p>Token matching exploits this convention.</p>"},{"location":"explanation/design-decisions/why-token-matching/#how-it-works","title":"How It Works","text":""},{"location":"explanation/design-decisions/why-token-matching/#step-1-tokenize","title":"Step 1: Tokenize","text":"<p>Split names on separators and case boundaries:</p> <pre><code>DATABASE_URL     \u2192 [database, url]\ndatabaseUrl      \u2192 [database, url]  \ndatabase-url     \u2192 [database, url]\nPAYMENT_DB_HOST  \u2192 [payment, db, host]\n</code></pre>"},{"location":"explanation/design-decisions/why-token-matching/#step-2-normalize","title":"Step 2: Normalize","text":"<p>Lowercase, remove noise:</p> <pre><code>DATABASE_URL \u2192 [database, url]\nDataBase_URL \u2192 [database, url]\n</code></pre>"},{"location":"explanation/design-decisions/why-token-matching/#step-3-compare","title":"Step 3: Compare","text":"<p>Calculate overlap:</p> <pre><code>Source: [database, url]\nTarget: [database, url]\nOverlap: 100% \u2192 High confidence\n</code></pre>"},{"location":"explanation/design-decisions/why-token-matching/#step-4-score","title":"Step 4: Score","text":"<p>Apply penalties for weak matches:</p> <pre><code>Source: [db, host]\nTarget: [db, host]\nOverlap: 100%\nPenalty: short tokens (db=2 chars)\nFinal: Medium confidence\n</code></pre>"},{"location":"explanation/design-decisions/why-token-matching/#when-it-works-well","title":"When It Works Well","text":"<p>\u2705 Consistent naming \u2014 Teams follow conventions \u2705 Descriptive names \u2014 <code>PAYMENT_DATABASE_HOST</code> not <code>PDH</code> \u2705 Similar patterns \u2014 <code>*_URL</code>, <code>*_HOST</code>, <code>*_KEY</code> </p>"},{"location":"explanation/design-decisions/why-token-matching/#when-it-struggles","title":"When It Struggles","text":"<p>\u274c Abbreviations \u2014 <code>DB</code> doesn't match <code>database</code> \u274c Generic names \u2014 <code>HOST</code> matches everything \u274c Different conventions \u2014 <code>camelCase</code> vs <code>SCREAMING_SNAKE</code> </p> <p>We mitigate with: - Blocked tokens (<code>id</code>, <code>key</code>, etc.) - Confidence thresholds - Suppressions for known false positives</p>"},{"location":"explanation/design-decisions/why-token-matching/#the-tradeoff","title":"The Tradeoff","text":"<p>Token matching is heuristic, not perfect.</p> <pre><code>Precision: ~85-95% (with tuning)\nRecall: ~70-85%\n</code></pre> <p>This is acceptable because:</p> <ol> <li>False positives are visible \u2014 Developers review and suppress</li> <li>False negatives surface elsewhere \u2014 Tests, prod errors</li> <li>The alternative is nothing \u2014 No other tool catches this</li> </ol>"},{"location":"explanation/design-decisions/why-token-matching/#improving-accuracy","title":"Improving Accuracy","text":""},{"location":"explanation/design-decisions/why-token-matching/#organization-level","title":"Organization-Level","text":"<ul> <li>Enforce naming conventions</li> <li>Document env var \u2192 resource mappings</li> <li>Run Jnkn in CI to catch drift</li> </ul>"},{"location":"explanation/design-decisions/why-token-matching/#jnkn-level","title":"Jnkn-Level","text":"<ul> <li>Tune confidence thresholds</li> <li>Add suppressions for known issues</li> <li>Custom rules for domain-specific patterns</li> </ul>"},{"location":"explanation/design-decisions/why-tree-sitter/","title":"Why Tree-Sitter","text":"<p>Rationale for our parsing technology choice.</p>"},{"location":"explanation/design-decisions/why-tree-sitter/#the-alternatives","title":"The Alternatives","text":"Approach Pros Cons Regex Simple, fast Misses context, false positives AST libraries Accurate Language-specific, slow Tree-sitter Accurate, fast, universal Learning curve"},{"location":"explanation/design-decisions/why-tree-sitter/#why-not-regex","title":"Why Not Regex?","text":"<p>Regex works for simple cases:</p> <pre><code># Matches\nos.getenv(\"DATABASE_URL\")\n</code></pre> <p>But fails on edge cases:</p> <pre><code># False positive: it's in a string\ndoc = 'Use os.getenv(\"DATABASE_URL\")'\n\n# False positive: it's a comment\n# os.getenv(\"DATABASE_URL\")\n\n# False positive: different module\nimport mylib.os as os\nos.getenv(\"NOT_STDLIB\")\n\n# Misses: multiline\nos.getenv(\n    \"VALID_VAR\"\n)\n</code></pre> <p>Regex can't understand context.</p>"},{"location":"explanation/design-decisions/why-tree-sitter/#why-not-ast-libraries","title":"Why Not AST Libraries?","text":"<p>Python's <code>ast</code> module is accurate:</p> <pre><code>import ast\n\ntree = ast.parse(code)\nfor node in ast.walk(tree):\n    if isinstance(node, ast.Call):\n        # Check if it's os.getenv\n        ...\n</code></pre> <p>But:</p> <ol> <li>Language-specific \u2014 Need different code for Python, JavaScript, HCL</li> <li>Slow for large files \u2014 Full parse required</li> <li>Fragile \u2014 Syntax errors break entire parse</li> </ol>"},{"location":"explanation/design-decisions/why-tree-sitter/#why-tree-sitter_1","title":"Why Tree-Sitter","text":"<p>Tree-sitter provides:</p>"},{"location":"explanation/design-decisions/why-tree-sitter/#1-universal-query-language","title":"1. Universal Query Language","text":"<p>One query syntax works across languages:</p> <pre><code>; Python\n(call\n  function: (attribute object: (identifier) @obj)\n  (#eq? @obj \"os\"))\n\n; JavaScript (similar pattern)\n(call_expression\n  function: (member_expression object: (identifier) @obj)\n  (#eq? @obj \"process\"))\n</code></pre>"},{"location":"explanation/design-decisions/why-tree-sitter/#2-error-tolerance","title":"2. Error Tolerance","text":"<p>Tree-sitter produces partial ASTs even with syntax errors:</p> <pre><code>def broken(\n    # Missing closing paren - tree-sitter still parses the rest\n\nos.getenv(\"STILL_DETECTED\")  # \u2705 Found\n</code></pre>"},{"location":"explanation/design-decisions/why-tree-sitter/#3-incremental-parsing","title":"3. Incremental Parsing","text":"<p>Only re-parse changed regions:</p> <pre><code># Change line 50 of a 1000-line file\n# Tree-sitter: ~1ms\n# Full re-parse: ~50ms\n</code></pre>"},{"location":"explanation/design-decisions/why-tree-sitter/#4-performance","title":"4. Performance","text":"<p>Tree-sitter is written in C with efficient memory usage:</p> File Size Tree-sitter Python AST 100 lines 0.5ms 2ms 1000 lines 3ms 20ms 10000 lines 25ms 200ms"},{"location":"explanation/design-decisions/why-tree-sitter/#trade-offs","title":"Trade-offs","text":""},{"location":"explanation/design-decisions/why-tree-sitter/#learning-curve","title":"Learning Curve","text":"<p>Tree-sitter queries have unusual syntax:</p> <pre><code>(call\n  function: (attribute\n    object: (identifier) @_obj\n    attribute: (identifier) @_method)\n  arguments: (argument_list (string) @env_var)\n  (#eq? @_obj \"os\")\n  (#eq? @_method \"getenv\"))\n</code></pre> <p>We mitigate this with: - Pre-built queries for common patterns - Regex fallback when tree-sitter isn't available - Documentation and examples</p>"},{"location":"explanation/design-decisions/why-tree-sitter/#dependency","title":"Dependency","text":"<p>Tree-sitter requires native binaries. We handle this by: - Making it optional (<code>jnkn[full]</code>) - Falling back to regex when unavailable - Providing pre-built Docker images</p>"},{"location":"explanation/design-decisions/why-tree-sitter/#the-result","title":"The Result","text":"<p>Tree-sitter enables Jnkn to:</p> <ol> <li>Parse accurately \u2014 Context-aware pattern detection</li> <li>Handle errors \u2014 Graceful degradation on syntax issues</li> <li>Scale \u2014 Fast parsing for large codebases</li> <li>Extend \u2014 Same query language for new languages</li> </ol>"},{"location":"explanation/security/data-handling/","title":"Data Handling","text":"<p>Privacy, data retention, and opt-in telemetry via PostHog using anonymous IDs.</p>"},{"location":"explanation/security/data-handling/#current-design","title":"Current Design","text":"<p>jnkn is designed with privacy as a core principle. The tool processes your code locally, stores data locally, and only sends anonymous usage telemetry if you explicitly opt in.</p>"},{"location":"explanation/security/data-handling/#local-first-architecture","title":"Local-First Architecture","text":"<p>All analysis happens on your machine:</p> <pre><code>Your Codebase \u2192 jnkn CLI \u2192 Local SQLite DB \u2192 Local Output\n                   \u2193\n            (opt-in only)\n                   \u2193\n         Anonymous Telemetry \u2192 PostHog\n</code></pre> <p>What stays local: - Your source code (never transmitted) - The dependency graph (stored in <code>.jnkn/jnkn.db</code>) - Configuration files (<code>.jnkn/config.yaml</code>) - Suppression rules (<code>.jnkn/suppressions.yaml</code>) - All analysis results</p> <p>What is transmitted (if opted in): - Anonymous usage events (command invocations, timing) - Aggregate statistics (node counts, scan duration) - Error categories (not stack traces or code)</p>"},{"location":"explanation/security/data-handling/#telemetry-system","title":"Telemetry System","text":"<p>Telemetry is disabled by default and requires explicit opt-in:</p> <pre><code>class TelemetryClient:\n    @property\n    def is_enabled(self) -&gt; bool:\n        \"\"\"Check if telemetry is enabled in config.\"\"\"\n        if not self.config_path.exists():\n            return False\n\n        with open(self.config_path, \"r\") as f:\n            data = yaml.safe_load(f) or {}\n            # Default to False if not explicitly set\n            return data.get(\"telemetry\", {}).get(\"enabled\", False)\n</code></pre>"},{"location":"explanation/security/data-handling/#opting-in","title":"Opting In","text":"<p>Enable telemetry during initialization or in config:</p> <pre><code># During init\njnkn init\n# Prompts: \"Enable anonymous telemetry? [y/N]\"\n\n# Or manually in config\n</code></pre> <pre><code># .jnkn/config.yaml\ntelemetry:\n  enabled: true\n  distinct_id: \"a1b2c3d4-...\"  # Auto-generated anonymous UUID\n</code></pre>"},{"location":"explanation/security/data-handling/#anonymous-identification","title":"Anonymous Identification","text":"<p>Each installation gets a random UUID that cannot be traced back to you:</p> <pre><code>@property\ndef distinct_id(self) -&gt; str:\n    \"\"\"Get or generate persistent anonymous ID.\"\"\"\n    if self._distinct_id:\n        return self._distinct_id\n\n    # Try to load from config\n    if self.config_path.exists():\n        with open(self.config_path, \"r\") as f:\n            data = yaml.safe_load(f) or {}\n            self._distinct_id = data.get(\"telemetry\", {}).get(\"distinct_id\")\n\n    # Generate new if not found\n    if not self._distinct_id:\n        self._distinct_id = str(uuid.uuid4())\n\n    return self._distinct_id\n</code></pre> <p>The ID is: - A random UUID (e.g., <code>f47ac10b-58cc-4372-a567-0e02b2c3d479</code>) - Not linked to your name, email, or any PII - Stored locally in your config file - Consistent across sessions (for usage patterns) - Deletable by removing the config entry</p>"},{"location":"explanation/security/data-handling/#what-we-collect","title":"What We Collect","text":"<p>When telemetry is enabled, we track:</p> <pre><code>def track(self, event_name: str, properties: Dict[str, Any] = None):\n    payload = {\n        \"api_key\": POSTHOG_API_KEY,\n        \"event\": event_name,\n        \"properties\": {\n            \"distinct_id\": self.distinct_id,     # Anonymous UUID\n            \"$lib\": \"jnkn-cli\",                  # Client identifier\n            \"$os\": platform.system(),            # OS name (Linux/Darwin/Windows)\n            \"$python_version\": platform.python_version(),  # Python version\n            \"timestamp\": datetime.utcnow().isoformat(),\n            **(properties or {})\n        }\n    }\n</code></pre> <p>Events tracked:</p> Event Properties Purpose <code>scan_complete</code> <code>node_count</code>, <code>edge_count</code>, <code>duration_ms</code>, <code>languages</code> Understand usage patterns <code>blast_radius_query</code> <code>depth</code>, <code>result_count</code> Optimize algorithms <code>stitch_complete</code> <code>edge_count</code>, <code>rule_names</code> Rule effectiveness <code>error</code> <code>error_type</code> (not message) Bug prioritization <code>cli_command</code> <code>command_name</code> Feature usage <p>What we never collect: - Source code or file contents - File paths or names - Environment variable names/values - Node or edge identifiers - Error messages or stack traces - Git repository names or URLs - IP addresses (PostHog anonymizes)</p>"},{"location":"explanation/security/data-handling/#transmission-mechanism","title":"Transmission Mechanism","text":"<p>Telemetry uses fire-and-forget HTTP requests that don't block the CLI:</p> <pre><code>def track(self, event_name: str, properties: Dict[str, Any] = None):\n    if not self.is_enabled:\n        return\n\n    # Non-blocking: runs in background thread\n    thread = threading.Thread(target=self._send_request, args=(payload,))\n    thread.daemon = False  # Let it finish if possible\n    thread.start()\n\ndef _send_request(self, payload: Dict[str, Any]):\n    try:\n        data = json.dumps(payload).encode(\"utf-8\")\n        req = request.Request(\n            f\"{POSTHOG_HOST}/capture/\",\n            data=data,\n            headers={\"Content-Type\": \"application/json\"}\n        )\n        with request.urlopen(req, timeout=5.0) as _:\n            pass\n    except Exception:\n        # Silent fail - telemetry should never break functionality\n        pass\n</code></pre> <p>Key properties: - Non-blocking: Runs in background thread - Fail-safe: Silent failure, no impact on CLI - Timeout: 5 second limit prevents hangs - Graceful shutdown: Waits for pending requests on exit</p>"},{"location":"explanation/security/data-handling/#data-retention","title":"Data Retention","text":"<p>Local data (<code>.jnkn/</code> directory): - Retained indefinitely until you delete it - Delete with <code>rm -rf .jnkn/</code> or <code>jnkn clean</code> - Not synced to any cloud service</p> <p>Telemetry data (PostHog): - Retained for 2 years (PostHog default) - Aggregated for analysis, not stored per-event long-term - No way to link anonymous ID to individual users - GDPR-compliant (PostHog EU hosting option)</p>"},{"location":"explanation/security/data-handling/#opting-out","title":"Opting Out","text":"<p>Disable telemetry at any time:</p> <pre><code># .jnkn/config.yaml\ntelemetry:\n  enabled: false\n</code></pre> <p>Or delete the distinct_id to reset:</p> <pre><code># .jnkn/config.yaml\ntelemetry:\n  enabled: false\n  # Remove distinct_id line entirely\n</code></pre>"},{"location":"explanation/security/data-handling/#network-behavior","title":"Network Behavior","text":"<p>When telemetry is enabled, jnkn makes HTTPS requests to: - <code>app.posthog.com</code> (telemetry events)</p> <p>When telemetry is disabled: - Zero network requests - jnkn works entirely offline - No phone-home or license checks</p>"},{"location":"explanation/security/data-handling/#cicd-environments","title":"CI/CD Environments","text":"<p>In CI/CD, telemetry is typically disabled:</p> <pre><code># GitHub Actions\n- name: Run jnkn\n  run: |\n    jnkn scan .\n  env:\n    JNKN_TELEMETRY: \"false\"  # Environment variable override\n</code></pre> <p>Or via config committed to repo:</p> <pre><code># .jnkn/config.yaml (committed)\ntelemetry:\n  enabled: false\n</code></pre>"},{"location":"explanation/security/data-handling/#data-flow-diagram","title":"Data Flow Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         YOUR MACHINE                                 \u2502\n\u2502                                                                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502              \u2502     \u2502              \u2502     \u2502                      \u2502 \u2502\n\u2502  \u2502  Your Code   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   jnkn CLI   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  .jnkn/jnkn.db      \u2502 \u2502\n\u2502  \u2502  (private)   \u2502     \u2502  (analysis)  \u2502     \u2502  (local storage)     \u2502 \u2502\n\u2502  \u2502              \u2502     \u2502              \u2502     \u2502                      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                              \u2502                                       \u2502\n\u2502                              \u2502 (if telemetry enabled)               \u2502\n\u2502                              \u25bc                                       \u2502\n\u2502                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                             \u2502\n\u2502                    \u2502 Anonymous Events \u2502                             \u2502\n\u2502                    \u2502 - command used   \u2502                             \u2502\n\u2502                    \u2502 - node count     \u2502                             \u2502\n\u2502                    \u2502 - timing         \u2502                             \u2502\n\u2502                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             \u2502\n\u2502                              \u2502                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                               \u2502 HTTPS (encrypted)\n                               \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                  \u2502\n                    \u2502  PostHog Cloud   \u2502\n                    \u2502  (telemetry)     \u2502\n                    \u2502                  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"explanation/security/data-handling/#future-ideas","title":"Future Ideas","text":""},{"location":"explanation/security/data-handling/#short-term-telemetry-transparency","title":"Short-term: Telemetry Transparency","text":"<p>Show exactly what would be sent before enabling:</p> <pre><code>jnkn telemetry preview\n</code></pre> <p>Output: <pre><code>If telemetry were enabled, the following would be sent:\n\nEvent: scan_complete\nProperties:\n  distinct_id: f47ac10b-58cc-4372-a567-0e02b2c3d479\n  $os: Darwin\n  $python_version: 3.12.0\n  node_count: 1234\n  edge_count: 567\n  duration_ms: 2340\n  languages: [\"python\", \"terraform\"]\n\nNo source code, file paths, or identifiers are included.\n</code></pre></p>"},{"location":"explanation/security/data-handling/#short-term-telemetry-log","title":"Short-term: Telemetry Log","text":"<p>Log what's being sent for audit:</p> <pre><code>telemetry:\n  enabled: true\n  log_events: true  # Write to .jnkn/telemetry.log\n</code></pre>"},{"location":"explanation/security/data-handling/#medium-term-self-hosted-telemetry","title":"Medium-term: Self-Hosted Telemetry","text":"<p>For enterprises that want usage data without external transmission:</p> <pre><code>telemetry:\n  enabled: true\n  endpoint: \"https://posthog.internal.company.com\"\n  api_key: \"phc_...\"\n</code></pre>"},{"location":"explanation/security/data-handling/#medium-term-aggregate-only-mode","title":"Medium-term: Aggregate-Only Mode","text":"<p>Send only daily aggregates, not per-event data:</p> <pre><code>telemetry:\n  enabled: true\n  mode: \"aggregate\"  # vs \"events\"\n</code></pre> <p>Sends once daily: <pre><code>{\n  \"date\": \"2025-01-15\",\n  \"total_scans\": 47,\n  \"total_nodes_analyzed\": 125000,\n  \"avg_scan_duration_ms\": 3400,\n  \"languages_used\": [\"python\", \"terraform\", \"kubernetes\"]\n}\n</code></pre></p>"},{"location":"explanation/security/data-handling/#long-term-privacy-preserving-analytics","title":"Long-term: Privacy-Preserving Analytics","text":"<p>Use differential privacy for stronger guarantees:</p> <pre><code>def add_noise(value: int, epsilon: float = 1.0) -&gt; int:\n    \"\"\"Add Laplacian noise for differential privacy.\"\"\"\n    import numpy as np\n    noise = np.random.laplace(0, 1/epsilon)\n    return int(value + noise)\n\n# Instead of exact count\n# node_count: 1234\n# Send noisy count\n# node_count: 1241 (within \u00b110 of true value)\n</code></pre>"},{"location":"explanation/security/data-handling/#long-term-local-only-analytics","title":"Long-term: Local-Only Analytics","text":"<p>Aggregate usage locally and display insights without any transmission:</p> <pre><code>jnkn stats\n</code></pre> <p>Output: <pre><code>Your jnkn Usage (local data only):\n\nTotal scans: 147\nFiles analyzed: 45,230\nDependencies discovered: 2,341\nFalse positives suppressed: 23\n\nMost common languages:\n  1. Python (67%)\n  2. Terraform (23%)\n  3. Kubernetes (10%)\n\nAverage scan time: 2.3s\n</code></pre></p>"},{"location":"explanation/security/supply-chain/","title":"Supply Chain","text":"<p>Dependency security for jnkn itself and the codebases it analyzes.</p>"},{"location":"explanation/security/supply-chain/#current-design","title":"Current Design","text":"<p>jnkn takes a minimal-dependency approach to reduce supply chain attack surface. The tool analyzes your codebase's cross-domain dependencies but is itself intentionally lightweight in its third-party dependencies.</p>"},{"location":"explanation/security/supply-chain/#jnkns-dependencies","title":"jnkn's Dependencies","text":"<p>Core dependencies (required for basic functionality):</p> <pre><code># pyproject.toml\ndependencies = [\n    \"click&gt;=8.1.7\",           # CLI framework\n    \"networkx&gt;=3.2.1\",        # Graph operations\n    \"openlineage-python&gt;=1.40.1\",  # Runtime lineage (optional integration)\n    \"pydantic&gt;=2.5.0\",        # Data validation\n    \"pyyaml&gt;=6.0.0\",          # Configuration parsing\n    \"rich&gt;=13.7.0\",           # CLI output formatting\n]\n</code></pre> <p>Optional dependencies (for advanced features):</p> <pre><code>[project.optional-dependencies]\nfull = [\n    \"sqlglot&gt;=20.0.0\",        # SQL parsing for dbt\n    \"tree-sitter==0.21.3\",    # AST parsing\n    \"tree-sitter-languages&gt;=1.10.2\",  # Language grammars\n    \"httpx&gt;=0.27.0\",          # HTTP client\n    \"xxhash&gt;=3.4.1\",          # Fast hashing\n]\n</code></pre>"},{"location":"explanation/security/supply-chain/#dependency-selection-criteria","title":"Dependency Selection Criteria","text":"<p>Each dependency was chosen based on:</p> Dependency Justification Alternatives Considered <code>click</code> Industry standard CLI, extensive testing <code>argparse</code> (stdlib but verbose), <code>typer</code> (newer) <code>networkx</code> Mature graph library, rich algorithms <code>igraph</code> (C-based, harder install), <code>graph-tool</code> <code>pydantic</code> Type validation, serialization <code>dataclasses</code> (less validation), <code>attrs</code> <code>pyyaml</code> YAML parsing standard <code>ruamel.yaml</code> (heavier), stdlib <code>json</code> <code>rich</code> Beautiful CLI output <code>colorama</code> (less features), plain text <code>tree-sitter</code> Fast, accurate AST parsing <code>ast</code> (Python only), regex (error-prone)"},{"location":"explanation/security/supply-chain/#minimal-attack-surface","title":"Minimal Attack Surface","text":"<p>The core install has only 6 direct dependencies. Compare to similar tools:</p> Tool Direct Dependencies Transitive Dependencies jnkn (core) 6 ~20 jnkn (full) 11 ~45 Checkov 25+ 150+ Snyk CLI N/A (Node.js) 300+"},{"location":"explanation/security/supply-chain/#no-native-code-in-core","title":"No Native Code in Core","text":"<p>The core installation avoids native/compiled dependencies: - No C extensions in required dependencies - Works on any Python 3.12+ platform - No compilation step during install</p> <p>The <code>[full]</code> extras add Tree-sitter (which includes C bindings) for advanced parsing, but this is optional.</p>"},{"location":"explanation/security/supply-chain/#pinned-versions-in-lockfile","title":"Pinned Versions in Lockfile","text":"<p>For reproducible builds, we recommend using a lockfile:</p> <pre><code># Generate locked dependencies\npip-compile pyproject.toml -o requirements.lock\n\n# Install from lockfile\npip install -r requirements.lock\n</code></pre>"},{"location":"explanation/security/supply-chain/#vulnerability-scanning","title":"Vulnerability Scanning","text":"<p>jnkn's CI pipeline includes dependency scanning:</p> <pre><code># .github/workflows/security.yml\n- name: Check for vulnerabilities\n  run: |\n    pip install safety pip-audit\n    safety check\n    pip-audit\n</code></pre>"},{"location":"explanation/security/supply-chain/#sbom-generation","title":"SBOM Generation","text":"<p>Generate a Software Bill of Materials for compliance:</p> <pre><code># Install SBOM generator\npip install cyclonedx-bom\n\n# Generate SBOM\ncyclonedx-py environment -o sbom.json\n</code></pre>"},{"location":"explanation/security/supply-chain/#what-jnkn-analyzes-your-codebase","title":"What jnkn Analyzes (Your Codebase)","text":"<p>jnkn's dependency graph focuses on cross-domain implicit dependencies, not package dependencies. However, the analysis does touch security-relevant areas.</p>"},{"location":"explanation/security/supply-chain/#environment-variables","title":"Environment Variables","text":"<p>jnkn extracts environment variable references:</p> <pre><code># Detected: env:DATABASE_URL\ndb_url = os.getenv(\"DATABASE_URL\")\n\n# Detected: env:API_SECRET\nsecret = os.environ[\"API_SECRET\"]\n</code></pre> <p>Security note: jnkn records the name of environment variables, never their values. The graph stores <code>env:DATABASE_URL</code>, not <code>postgres://user:password@host/db</code>.</p>"},{"location":"explanation/security/supply-chain/#infrastructure-references","title":"Infrastructure References","text":"<p>jnkn parses Terraform outputs and resources:</p> <pre><code># Detected: infra:db_connection_string\noutput \"db_connection_string\" {\n  value     = aws_db_instance.main.endpoint\n  sensitive = true\n}\n</code></pre> <p>Security note: jnkn parses HCL structure, not state files. Sensitive values in <code>terraform.tfstate</code> are never accessed.</p>"},{"location":"explanation/security/supply-chain/#kubernetes-secrets-references","title":"Kubernetes Secrets References","text":"<p>jnkn detects references to secrets, not their contents:</p> <pre><code># Detected: config:db-credentials (reference only)\nenv:\n  - name: DB_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        name: db-credentials\n        key: password\n</code></pre>"},{"location":"explanation/security/supply-chain/#what-jnkn-does-not-access","title":"What jnkn Does NOT Access","text":"<ul> <li>Secret values: Only names/references, never actual secrets</li> <li>State files: <code>terraform.tfstate</code>, <code>.terraform/</code> ignored</li> <li>Credentials: <code>.env</code>, <code>*.pem</code>, <code>*_key</code> files ignored by default</li> <li>Git history: Only current working tree, not commit history</li> <li>Network resources: No HTTP calls to resolve references</li> </ul>"},{"location":"explanation/security/supply-chain/#default-ignore-patterns","title":"Default Ignore Patterns","text":"<p>jnkn's default <code>.jnknignore</code> excludes sensitive paths:</p> <pre><code># Secrets and credentials\n.env\n.env.*\n*.pem\n*.key\n**/secrets/\n**/credentials/\n\n# State files\nterraform.tfstate\nterraform.tfstate.*\n.terraform/\n\n# Dependencies (not our focus)\nnode_modules/\nvenv/\n.venv/\n__pycache__/\n\n# Build artifacts\ndist/\nbuild/\n*.egg-info/\n</code></pre>"},{"location":"explanation/security/supply-chain/#secure-usage-recommendations","title":"Secure Usage Recommendations","text":""},{"location":"explanation/security/supply-chain/#cicd-integration","title":"CI/CD Integration","text":"<p>When running in CI/CD, ensure jnkn doesn't have access to secrets:</p> <pre><code># GitHub Actions - run before secrets are injected\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      # Run jnkn BEFORE setting up cloud credentials\n      - name: Analyze dependencies\n        run: |\n          pip install jnkn\n          jnkn scan .\n\n      # Later: deploy with secrets (jnkn not involved)\n      - name: Deploy\n        env:\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        run: ...\n</code></pre>"},{"location":"explanation/security/supply-chain/#air-gapped-environments","title":"Air-Gapped Environments","text":"<p>jnkn works fully offline:</p> <pre><code># Pre-download dependencies\npip download jnkn -d ./packages\n\n# Install offline\npip install --no-index --find-links=./packages jnkn\n\n# Run with telemetry disabled (default)\njnkn scan .\n</code></pre>"},{"location":"explanation/security/supply-chain/#least-privilege","title":"Least Privilege","text":"<p>jnkn only needs read access to source code:</p> <pre><code># Run as read-only user\nsudo -u readonly-user jnkn scan /path/to/code\n\n# Or in container with read-only mount\ndocker run -v /code:/code:ro jnkn scan /code\n</code></pre>"},{"location":"explanation/security/supply-chain/#future-ideas","title":"Future Ideas","text":""},{"location":"explanation/security/supply-chain/#short-term-dependency-hash-verification","title":"Short-term: Dependency Hash Verification","text":"<p>Verify dependencies match known-good hashes:</p> <pre><code># pyproject.toml\n[tool.jnkn.security]\nverify_hashes = true\n</code></pre> <pre><code># requirements.lock with hashes\nclick==8.1.7 \\\n    --hash=sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28\n</code></pre>"},{"location":"explanation/security/supply-chain/#short-term-signed-releases","title":"Short-term: Signed Releases","text":"<p>Sign releases with Sigstore/cosign:</p> <pre><code># Verify release signature\ncosign verify-blob --signature jnkn-0.1.0.tar.gz.sig jnkn-0.1.0.tar.gz\n</code></pre>"},{"location":"explanation/security/supply-chain/#medium-term-sbom-integration","title":"Medium-term: SBOM Integration","text":"<p>Generate SBOM for analyzed codebases:</p> <pre><code>jnkn sbom --format cyclonedx -o project-sbom.json\n</code></pre> <p>Include jnkn-discovered dependencies:</p> <pre><code>{\n  \"bomFormat\": \"CycloneDX\",\n  \"components\": [\n    {\n      \"type\": \"application\",\n      \"name\": \"payment-service\",\n      \"dependencies\": [\n        {\"ref\": \"infra:payment_db\"},\n        {\"ref\": \"env:STRIPE_API_KEY\"},\n        {\"ref\": \"config:kafka-credentials\"}\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"explanation/security/supply-chain/#medium-term-vulnerability-context","title":"Medium-term: Vulnerability Context","text":"<p>Show which vulnerabilities affect your dependency graph:</p> <pre><code>jnkn security --check-vulns\n</code></pre> <p>Output: <pre><code>Vulnerability Impact Analysis:\n\nCVE-2024-1234 in log4j affects:\n  \u2192 file://services/java-api/pom.xml\n  \u2192 Downstream impact: 12 services\n\n  Blast radius:\n    HIGH: payment-service, auth-service\n    MEDIUM: notification-service\n    LOW: analytics-worker (test only)\n</code></pre></p>"},{"location":"explanation/security/supply-chain/#long-term-supply-chain-graph","title":"Long-term: Supply Chain Graph","text":"<p>Extend the dependency graph to include package dependencies:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    jnkn Extended Graph                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                          \u2502\n\u2502  Package Layer:     requests \u2192 urllib3 \u2192 ssl            \u2502\n\u2502                          \u2193                               \u2502\n\u2502  Code Layer:        api_client.py                       \u2502\n\u2502                          \u2193                               \u2502\n\u2502  Infra Layer:       env:API_ENDPOINT \u2192 infra:api_gateway\u2502\n\u2502                                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code># Future: combine package deps with cross-domain deps\njnkn scan --include-packages .\n</code></pre>"},{"location":"explanation/security/supply-chain/#long-term-policy-enforcement","title":"Long-term: Policy Enforcement","text":"<p>Define and enforce supply chain policies:</p> <pre><code># .jnkn/policies/supply-chain.yaml\npolicies:\n  - name: no-unmaintained-deps\n    rule: |\n      package.last_update &gt; now() - 365d\n\n  - name: max-transitive-depth\n    rule: |\n      dependency.depth &lt;= 5\n\n  - name: approved-sources-only\n    rule: |\n      package.source in [\"pypi.org\", \"npm.org\", \"internal.repo\"]\n</code></pre> <pre><code>jnkn check --policy supply-chain.yaml\n</code></pre>"},{"location":"explanation/security/supply-chain/#long-term-reproducible-analysis","title":"Long-term: Reproducible Analysis","text":"<p>Ensure analysis results are reproducible:</p> <pre><code># Generate analysis with provenance\njnkn scan --provenance .\n\n# Output includes:\n# - jnkn version\n# - Dependency hashes\n# - Git commit SHA\n# - Timestamp\n# - Analysis checksum\n</code></pre> <pre><code>{\n  \"provenance\": {\n    \"jnkn_version\": \"0.1.0\",\n    \"jnkn_hash\": \"sha256:abc123...\",\n    \"analyzed_commit\": \"def456...\",\n    \"timestamp\": \"2025-01-15T10:30:00Z\",\n    \"result_hash\": \"sha256:789xyz...\"\n  },\n  \"graph\": { ... }\n}\n</code></pre>"},{"location":"explanation/security/threat-model/","title":"Threat Model","text":"<p>Security considerations for Jnkn.</p>"},{"location":"explanation/security/threat-model/#what-jnkn-does","title":"What Jnkn Does","text":"<ul> <li>Reads source files from disk</li> <li>Parses file contents into an AST</li> <li>Stores graph data in SQLite</li> <li>Outputs analysis results</li> </ul>"},{"location":"explanation/security/threat-model/#what-jnkn-does-not-do","title":"What Jnkn Does NOT Do","text":"<ul> <li>Execute code</li> <li>Make network requests</li> <li>Access credentials or secrets</li> <li>Modify source files</li> </ul>"},{"location":"explanation/security/threat-model/#trust-boundaries","title":"Trust Boundaries","text":"<pre><code>graph TD\n    subgraph Trusted\n        FS[File System]\n        DB[(SQLite DB)]\n        CLI[CLI Output]\n    end\n\n    subgraph Untrusted\n        SF[Source Files]\n        CF[Config Files]\n    end\n\n    SF --&gt; |read| FS\n    CF --&gt; |read| FS\n    FS --&gt; |parse| DB\n    DB --&gt; |query| CLI</code></pre>"},{"location":"explanation/security/threat-model/#trusted","title":"Trusted","text":"<ul> <li>File system \u2014 Jnkn assumes it can read files you point it at</li> <li>SQLite database \u2014 Jnkn controls this file</li> <li>CLI output \u2014 Results are displayed to the user</li> </ul>"},{"location":"explanation/security/threat-model/#untrusted","title":"Untrusted","text":"<ul> <li>Source files \u2014 May contain malicious patterns</li> <li>Config files \u2014 User-controlled</li> </ul>"},{"location":"explanation/security/threat-model/#threat-analysis","title":"Threat Analysis","text":""},{"location":"explanation/security/threat-model/#threat-1-malicious-source-files","title":"Threat 1: Malicious Source Files","text":"<p>Scenario: Attacker includes crafted code that exploits parser.</p> <p>Mitigation: - Tree-sitter is memory-safe (written in C with fuzzing) - Jnkn doesn't execute code, only parses - Regex patterns are bounded</p> <p>Residual risk: Low</p>"},{"location":"explanation/security/threat-model/#threat-2-config-injection","title":"Threat 2: Config Injection","text":"<p>Scenario: Attacker modifies <code>.jnkn/config.yaml</code> to load malicious code.</p> <p>Mitigation: - Extra extractors/rules must be importable Python modules - Requires write access to config file - Requires code execution environment</p> <p>Residual risk: Medium (if attacker has write access, they have larger problems)</p>"},{"location":"explanation/security/threat-model/#threat-3-information-disclosure","title":"Threat 3: Information Disclosure","text":"<p>Scenario: Jnkn output reveals sensitive information.</p> <p>Mitigation: - Jnkn only reports artifact names, not contents - No secret values are extracted - Environment variable names (not values) are detected</p> <p>Residual risk: Low (env var names are not typically secret)</p>"},{"location":"explanation/security/threat-model/#threat-4-denial-of-service","title":"Threat 4: Denial of Service","text":"<p>Scenario: Large or malformed file causes excessive resource usage.</p> <p>Mitigation: - File size limits in parsing - Timeout on tree-sitter operations - Bounded regex execution</p> <p>Residual risk: Low</p>"},{"location":"explanation/security/threat-model/#best-practices","title":"Best Practices","text":""},{"location":"explanation/security/threat-model/#cicd-usage","title":"CI/CD Usage","text":"<pre><code># Run in isolated container\n- name: Jnkn\n  uses: docker://ghcr.io/jnkn-io/jnkn:latest\n  with:\n    args: scan --dir /github/workspace\n</code></pre>"},{"location":"explanation/security/threat-model/#secret-handling","title":"Secret Handling","text":"<p>Jnkn detects env var names, not values. However:</p> <ul> <li>Don't commit <code>.jnkn/jnkn.db</code> if it might contain sensitive paths</li> <li>Use <code>.jnknignore</code> to skip sensitive directories</li> </ul>"},{"location":"explanation/security/threat-model/#dependency-security","title":"Dependency Security","text":"<p>Jnkn's dependencies are: - Scanned with Dependabot - Pinned to specific versions - Audited for known vulnerabilities</p>"},{"location":"explanation/security/threat-model/#reporting-security-issues","title":"Reporting Security Issues","text":"<p>Report vulnerabilities to: security@jnkn.io</p> <p>We follow responsible disclosure with a 90-day window.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get Jnkn running in your project in under 10 minutes.</p>"},{"location":"getting-started/#choose-your-path","title":"Choose Your Path","text":"<ul> <li> <p> Instant Demo</p> <p>Don't have a repo ready? Spin up a full example environment locally to see a perfect graph.</p> <pre><code>jnkn init --demo\n</code></pre> <p> Example Repo</p> </li> <li> <p> Ready to apply to your project?</p> <p>Follow the quickstart to scan your first project.</p> <p> Quickstart</p> </li> <li> <p> Want CI integration?</p> <p>Set up GitHub Actions to check PRs automatically.</p> <p> CI Integration</p> </li> </ul>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>A project with Python, Terraform, or Kubernetes files</li> </ul>"},{"location":"getting-started/#what-youll-learn","title":"What You'll Learn","text":"<ol> <li>Installation \u2014 Multiple ways to install Jnkn</li> <li>Quickstart \u2014 Scan a project and understand the output</li> <li>CI Integration \u2014 Automate impact analysis on pull requests</li> </ol>"},{"location":"getting-started/example-repository/","title":"Example Repository","text":"<p>In this walkthrough, we will spin up a simulated Payment Service stack that includes: - Python application code - Terraform infrastructure definitions - Kubernetes manifests</p> <p>You will see how <code>jnkn</code> stitches these disparate files together into a single dependency graph.</p>"},{"location":"getting-started/example-repository/#1-setup-the-demo","title":"1. Setup the Demo","text":"<p>You don't need to clone anything. Jnkn includes a built-in demo generator.</p> <p>Create a temporary directory and initialize the demo:</p> <pre><code>mkdir try-jnkn\ncd try-jnkn\n\n# This downloads the example structure and configures Jnkn\njnkn init --demo\n````\n\nYou should see:\n\n```text\n\ud83d\udcc2 Created demo project at: .../try-jnkn/jnkn-demo\n\u2728 Initialized successfully!\n</code></pre> <p>Navigate into the generated project:</p> <pre><code>cd jnkn-demo\n</code></pre>"},{"location":"getting-started/example-repository/#2-the-architecture","title":"2. The Architecture","text":"<p>Before we scan, let's look at the \"trap\" waiting in this codebase.</p> <ol> <li> <p>The Code (<code>src/app.py</code>):     The Python app crashes if it can't connect to the database. It expects an environment variable named <code>PAYMENT_DB_HOST</code>.</p> <pre><code>DB_HOST = os.getenv(\"PAYMENT_DB_HOST\")\n</code></pre> </li> <li> <p>The Infrastructure (<code>terraform/main.tf</code>):     Terraform provisions an RDS instance and outputs its address as <code>payment_db_host</code>.</p> <pre><code>output \"payment_db_host\" {\n  value = aws_db_instance.payment_db.address\n}\n</code></pre> </li> <li> <p>The Glue (<code>k8s/deployment.yaml</code>):     Kubernetes injects the secret into the container.</p> <pre><code>- name: PAYMENT_DB_HOST\n  valueFrom:\n    secretKeyRef:\n      name: db-secrets\n      key: host\n</code></pre> </li> </ol> <p>The Problem: There are no explicit imports connecting Terraform to Python. If you rename the Terraform output, <code>terraform plan</code> will pass, but the Python app will crash in production.</p>"},{"location":"getting-started/example-repository/#3-run-the-scan","title":"3. Run the Scan","text":"<p>Let's see if Jnkn can find these hidden connections.</p> <pre><code>jnkn scan\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udd0d Scanning .../try-jnkn/jnkn-demo\n   Parsers loaded: python, terraform, kubernetes\n   Files found: 3\n\u2705 Scan complete\n   Nodes: 7\n   Edges: 7\n   Saved: .jnkn/lineage.json\n</code></pre> <p>Jnkn has parsed the files, tokenized the variable names (e.g., <code>PAYMENT_DB_HOST</code> \u2192 <code>[payment, db, host]</code>), and stitched them together based on matching patterns.</p>"},{"location":"getting-started/example-repository/#4-analyze-blast-radius","title":"4. Analyze Blast Radius","text":"<p>Now, imagine you are the Infrastructure Engineer. You want to refactor the Terraform code.</p> <p>Question: \"What happens if I change the <code>payment_db_host</code> output?\"</p> <p>Run a blast radius check:</p> <pre><code>jnkn blast infra:output.payment_db_host\n</code></pre> <p>You will see exactly what breaks downstream:</p> <pre><code>\ud83d\udca5 Blast Radius Analysis\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nSource: infra:output.payment_db_host\n\n\u2638\ufe0f  Kubernetes (1)\n  \u2022 k8s:default/deployment/payment-service\n\n\ud83d\udc0d Python Code (1)\n  \u2022 src/app.py (Confidence: High)\n</code></pre> <p>Result: You instantly see that changing Terraform impacts both the Kubernetes deployment configuration and the Python application code.</p>"},{"location":"getting-started/example-repository/#5-visualize-the-graph","title":"5. Visualize the Graph","text":"<p>For a high-level view of your architecture, generate an interactive HTML graph.</p> <pre><code>jnkn graph --output my-stack.html\n</code></pre> <p>Open <code>my-stack.html</code> in your browser:</p> <ol> <li>Zoom in to see the nodes.</li> <li>Click on <code>infra:aws_db_instance.payment_db</code>.</li> <li>Follow the arrows to see how data flows from Infrastructure \u2192 Config \u2192 Code.</li> </ol> <p>This visual proof is excellent for explaining architectural dependencies to new team members or during architectural reviews.</p>"},{"location":"getting-started/example-repository/#6-cleanup","title":"6. Cleanup","text":"<p>When you are done, simply remove the directory:</p> <pre><code>cd ..\nrm -rf jnkn-demo\n</code></pre>"},{"location":"getting-started/example-repository/#next-steps","title":"Next Steps","text":"<p>Now that you've seen it work in a perfect environment, try it on your real code.</p> <p> Scan your own repository</p>"},{"location":"getting-started/first-ci-integration/","title":"CI Integration","text":"<p>Add Jnkn to your CI pipeline to catch breaking changes on every PR.</p>"},{"location":"getting-started/first-ci-integration/#github-actions","title":"GitHub Actions","text":"<p>Create <code>.github/workflows/jnkn.yml</code>:</p> <pre><code>name: Jnkn Impact Analysis\n\non:\n  pull_request:\n    paths:\n      - '**.py'\n      - '**.tf'\n      - '**/dbt/**'\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0  # Need history for diff\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install Jnkn\n        run: pip install jnkn[full]\n\n      - name: Scan codebase\n        run: jnkn scan\n\n      - name: Analyze changed files\n        run: |\n          # Get changed files\n          CHANGED=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | grep -E '\\.(py|tf)$' || true)\n\n          if [ -n \"$CHANGED\" ]; then\n            echo \"## \ud83d\udd0d Impact Analysis\" &gt;&gt; $GITHUB_STEP_SUMMARY\n            for file in $CHANGED; do\n              echo \"### $file\" &gt;&gt; $GITHUB_STEP_SUMMARY\n              jnkn blast \"file://$file\" --format markdown &gt;&gt; $GITHUB_STEP_SUMMARY 2&gt;/dev/null || true\n            done\n          fi\n</code></pre>"},{"location":"getting-started/first-ci-integration/#what-this-does","title":"What This Does","text":"<ol> <li>Triggers on PRs that modify Python or Terraform files</li> <li>Scans your codebase to build the dependency graph</li> <li>Analyzes each changed file's blast radius</li> <li>Reports results in the GitHub Actions summary</li> </ol>"},{"location":"getting-started/first-ci-integration/#example-output","title":"Example Output","text":"<p>When a PR modifies <code>terraform/rds.tf</code>:</p> <pre><code>## \ud83d\udd0d Impact Analysis\n\n### terraform/rds.tf\n\n**Blast Radius: 5 artifacts**\n\n| Type | Artifact | Confidence |\n|------|----------|------------|\n| env_var | env:DATABASE_URL | 0.92 |\n| code_file | src/db/connection.py | 0.88 |\n| code_file | src/api/users.py | 0.85 |\n</code></pre>"},{"location":"getting-started/first-ci-integration/#block-on-high-risk-changes","title":"Block on High-Risk Changes","text":"<p>Add a failure condition for high-impact changes:</p> <pre><code>- name: Check impact threshold\n  run: |\n    IMPACT=$(jnkn blast \"file://$FILE\" --format json | jq '.total_impacted')\n    if [ \"$IMPACT\" -gt 10 ]; then\n      echo \"::error::High impact change: $IMPACT artifacts affected\"\n      exit 1\n    fi\n</code></pre>"},{"location":"getting-started/first-ci-integration/#gitlab-ci","title":"GitLab CI","text":"<p>See GitLab CI Integration for GitLab-specific setup.</p>"},{"location":"getting-started/first-ci-integration/#next-steps","title":"Next Steps","text":"<ul> <li> Configure confidence thresholds</li> <li> Manage suppressions</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#recommended-pip","title":"Recommended: pip","text":"<pre><code>pip install jnkn\n</code></pre> <p>For full functionality (tree-sitter parsing, all language support):</p> <pre><code>pip install jnkn[full]\n</code></pre>"},{"location":"getting-started/installation/#alternative-methods","title":"Alternative Methods","text":"pipx (isolated)uvDockerFrom Source <pre><code>pipx install jnkn[full]\n</code></pre> <pre><code>uv tool install jnkn[full]\n</code></pre> <pre><code>docker pull ghcr.io/jnkn-io/jnkn:latest\ndocker run --rm -v $(pwd):/app jnkn scan --dir /app\n</code></pre> <pre><code>git clone https://github.com/bordumb/jnkn.git\ncd jnkn\npip install -e \".[full,dev]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>jnkn --version\n</code></pre> <p>Expected output:</p> <pre><code>jnkn 0.1.0\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"Extra What It Enables <code>full</code> Tree-sitter parsing, all language support <code>dev</code> Testing and development tools"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p> Run your first scan</p>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Scan your first project in 5 minutes.</p>"},{"location":"getting-started/quickstart/#1-initialize","title":"1. Initialize","text":"<pre><code>cd your-project\njnkn init\n</code></pre> <p>This creates <code>.jnkn/config.yaml</code> with sensible defaults.</p>"},{"location":"getting-started/quickstart/#2-scan","title":"2. Scan","text":"<pre><code>jnkn scan\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udd0d Scanning /path/to/your-project\n\ud83d\udcc1 Found 47 files (12 Python, 8 Terraform, 27 other)\n\u2705 Parsed 156 nodes, 89 edges\n\ud83e\uddf5 Stitching cross-domain dependencies...\n\u2705 Created 8 cross-domain links\n</code></pre>"},{"location":"getting-started/quickstart/#3-explore","title":"3. Explore","text":""},{"location":"getting-started/quickstart/#view-statistics","title":"View Statistics","text":"<pre><code>jnkn stats\n</code></pre> <pre><code>\ud83d\udcca Graph Statistics\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nNodes:     156\nEdges:     97\nFiles:     47\n\nBy Type:\n  code_file:       42\n  env_var:         12\n  infra_resource:  8\n</code></pre>"},{"location":"getting-started/quickstart/#calculate-blast-radius","title":"Calculate Blast Radius","text":"<pre><code>jnkn blast env:DATABASE_URL\n</code></pre> <pre><code>{\n  \"source\": \"env:DATABASE_URL\",\n  \"total_impacted\": 3,\n  \"impacted\": [\n    \"file://src/db/connection.py\",\n    \"file://src/api/users.py\",\n    \"infra:aws_db_instance.main\"\n  ]\n}\n</code></pre>"},{"location":"getting-started/quickstart/#explain-a-match","title":"Explain a Match","text":"<pre><code>jnkn explain env:DB_HOST infra:db_host\n</code></pre> <pre><code>Source: env:DB_HOST \u2192 Tokens: [db, host]\nTarget: infra:db_host \u2192 Tokens: [db, host]\n\nConfidence: 0.85 (HIGH)\n  [+0.90] normalized_match: 'dbhost' == 'dbhost'\n  [\u00d70.95] penalty: short token 'db'\n</code></pre>"},{"location":"getting-started/quickstart/#what-just-happened","title":"What Just Happened?","text":"<ol> <li>Parse: Jnkn scanned your Python and Terraform files</li> <li>Extract: Found environment variables and infrastructure resources  </li> <li>Stitch: Linked <code>env:DATABASE_URL</code> to <code>infra:aws_db_instance.main</code> via token matching</li> <li>Store: Saved the graph to <code>.jnkn/jnkn.db</code></li> </ol>"},{"location":"getting-started/quickstart/#common-issues","title":"Common Issues","text":"No env vars found <p>Jnkn looks for specific patterns. Check Supported Patterns to ensure your code matches.</p> Too many false positives <p>Lower the confidence threshold or add suppressions:</p> <pre><code>jnkn suppress add \"env:*_ID\" \"infra:*\" --reason \"ID fields are generic\"\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li> Set up CI integration</li> <li> Learn about blast radius</li> <li> Configure confidence thresholds</li> </ul>"},{"location":"how-to/","title":"How-To Guides","text":"<p>Task-oriented guides for specific goals.</p>"},{"location":"how-to/#scanning","title":"Scanning","text":"<ul> <li>Scan a Monorepo \u2014 Strategies for large codebases</li> <li>Incremental Scanning \u2014 Only scan changed files</li> </ul>"},{"location":"how-to/#analysis","title":"Analysis","text":"<ul> <li>Calculate Blast Radius \u2014 Find downstream impact</li> <li>Compare Branches \u2014 Diff between git refs</li> <li>Export Results \u2014 JSON, SARIF, CSV output</li> </ul>"},{"location":"how-to/#integration","title":"Integration","text":"<ul> <li>GitHub Actions \u2014 CI setup for GitHub</li> <li>GitLab CI \u2014 CI setup for GitLab</li> </ul>"},{"location":"how-to/#configuration","title":"Configuration","text":"<ul> <li>Configure Confidence \u2014 Tune thresholds</li> <li>Manage Suppressions \u2014 Handle false positives</li> </ul>"},{"location":"how-to/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>No Matches Found \u2014 Debug empty results</li> <li>Too Many False Positives \u2014 Improve precision</li> </ul>"},{"location":"how-to/analysis/calculate-blast-radius/","title":"Calculate Blast Radius","text":"<p>Find all artifacts impacted by a change.</p>"},{"location":"how-to/analysis/calculate-blast-radius/#basic-usage","title":"Basic Usage","text":"<pre><code>jnkn blast &lt;artifact-id&gt;\n</code></pre>"},{"location":"how-to/analysis/calculate-blast-radius/#artifact-id-formats","title":"Artifact ID Formats","text":"Type Format Example Environment variable <code>env:NAME</code> <code>env:DATABASE_URL</code> File <code>file://path</code> <code>file://src/config.py</code> Infrastructure <code>infra:name</code> <code>infra:aws_rds.main</code> Kubernetes <code>k8s:ns/kind/name</code> <code>k8s:default/deployment/api</code> dbt model <code>data:model</code> <code>data:fct_orders</code>"},{"location":"how-to/analysis/calculate-blast-radius/#options","title":"Options","text":""},{"location":"how-to/analysis/calculate-blast-radius/#limit-depth","title":"Limit Depth","text":"<p>Only show direct dependencies:</p> <pre><code>jnkn blast env:X --max-depth 1\n</code></pre> <p>Show up to 3 levels:</p> <pre><code>jnkn blast env:X --max-depth 3\n</code></pre>"},{"location":"how-to/analysis/calculate-blast-radius/#multiple-sources","title":"Multiple Sources","text":"<p>Analyze several artifacts at once:</p> <pre><code>jnkn blast env:DATABASE_URL env:REDIS_URL infra:aws_rds.main\n</code></pre>"},{"location":"how-to/analysis/calculate-blast-radius/#filter-by-type","title":"Filter by Type","text":"<p>Only show infrastructure impacts:</p> <pre><code>jnkn blast env:X --type infra\n</code></pre> <p>Only show code files:</p> <pre><code>jnkn blast env:X --type code\n</code></pre>"},{"location":"how-to/analysis/calculate-blast-radius/#output-formats","title":"Output Formats","text":"JSON (default)MarkdownPlainSARIF <pre><code>jnkn blast env:X --format json\n</code></pre> <pre><code>jnkn blast env:X --format markdown\n</code></pre> <pre><code>jnkn blast env:X --format plain\n</code></pre> <pre><code>jnkn blast env:X --format sarif &gt; results.sarif\n</code></pre>"},{"location":"how-to/analysis/calculate-blast-radius/#scripting","title":"Scripting","text":""},{"location":"how-to/analysis/calculate-blast-radius/#check-impact-count","title":"Check Impact Count","text":"<pre><code>IMPACT=$(jnkn blast env:X --format json | jq '.total_impacted_count')\n\nif [ \"$IMPACT\" -gt 10 ]; then\n    echo \"High impact: $IMPACT artifacts\"\n    exit 1\nfi\n</code></pre>"},{"location":"how-to/analysis/calculate-blast-radius/#get-impacted-files-only","title":"Get Impacted Files Only","text":"<pre><code>jnkn blast env:X --format json | jq -r '.impacted_artifacts[]' | grep '^file://'\n</code></pre>"},{"location":"how-to/analysis/calculate-blast-radius/#pipe-to-other-tools","title":"Pipe to Other Tools","text":"<pre><code># Send to Slack\njnkn blast env:X --format markdown | slack-cli post \"#alerts\"\n\n# Create Jira ticket\njnkn blast env:X --format json | jira-create-ticket.sh\n</code></pre>"},{"location":"how-to/analysis/calculate-blast-radius/#understanding-results","title":"Understanding Results","text":"<pre><code>{\n  \"source_artifacts\": [\"env:DATABASE_URL\"],\n  \"total_impacted_count\": 5,\n  \"impacted_artifacts\": [\n    \"file://src/db/connection.py\",\n    \"file://src/api/users.py\",\n    \"infra:aws_db_instance.main\"\n  ],\n  \"breakdown\": {\n    \"code\": [\"file://src/db/connection.py\", \"file://src/api/users.py\"],\n    \"infra\": [\"infra:aws_db_instance.main\"],\n    \"env\": [],\n    \"data\": []\n  },\n  \"max_depth_reached\": 2\n}\n</code></pre> <ul> <li><code>total_impacted_count</code>: Total number of affected artifacts</li> <li><code>impacted_artifacts</code>: Full list of affected artifact IDs</li> <li><code>breakdown</code>: Grouped by artifact type</li> <li><code>max_depth_reached</code>: How many hops the deepest impact is</li> </ul>"},{"location":"how-to/analysis/calculate-blast-radius/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/analysis/calculate-blast-radius/#artifact-not-found","title":"\"Artifact not found\"","text":"<p>The artifact ID may not exist in the graph. Check with:</p> <pre><code>jnkn stats --show-nodes | grep \"DATABASE_URL\"\n</code></pre>"},{"location":"how-to/analysis/calculate-blast-radius/#results-seem-incomplete","title":"Results seem incomplete","text":"<p>Ensure you've scanned all relevant directories:</p> <pre><code>jnkn scan --dir . --full\n</code></pre>"},{"location":"how-to/analysis/compare-branches/","title":"Compare Branches","text":"<p>Analyze dependency changes between git refs.</p>"},{"location":"how-to/analysis/compare-branches/#basic-usage","title":"Basic Usage","text":"<pre><code>jnkn diff main..feature-branch\n</code></pre>"},{"location":"how-to/analysis/compare-branches/#how-it-works","title":"How It Works","text":"<ol> <li>Checks out base ref, scans to temporary database</li> <li>Checks out target ref, scans to another database</li> <li>Compares graphs and reports differences</li> </ol>"},{"location":"how-to/analysis/compare-branches/#output","title":"Output","text":"<pre><code>{\n  \"added_nodes\": [\n    {\"id\": \"env:NEW_VAR\", \"type\": \"env_var\"}\n  ],\n  \"removed_nodes\": [\n    {\"id\": \"env:OLD_VAR\", \"type\": \"env_var\"}\n  ],\n  \"added_edges\": [\n    {\"source\": \"file://src/new.py\", \"target\": \"env:NEW_VAR\"}\n  ],\n  \"removed_edges\": [\n    {\"source\": \"file://src/old.py\", \"target\": \"env:OLD_VAR\"}\n  ],\n  \"summary\": {\n    \"nodes_added\": 1,\n    \"nodes_removed\": 1,\n    \"edges_added\": 1,\n    \"edges_removed\": 1\n  }\n}\n</code></pre>"},{"location":"how-to/analysis/compare-branches/#compare-with-working-directory","title":"Compare with Working Directory","text":"<p>Compare current state against a branch:</p> <pre><code>jnkn diff main\n</code></pre>"},{"location":"how-to/analysis/compare-branches/#ci-integration","title":"CI Integration","text":"<pre><code>- name: Compare with main\n  run: |\n    DIFF=$(jnkn diff origin/main --format json)\n    ADDED=$(echo \"$DIFF\" | jq '.summary.nodes_added')\n    REMOVED=$(echo \"$DIFF\" | jq '.summary.nodes_removed')\n\n    echo \"Added $ADDED nodes, removed $REMOVED nodes\"\n</code></pre>"},{"location":"how-to/analysis/compare-branches/#focus-on-specific-changes","title":"Focus on Specific Changes","text":""},{"location":"how-to/analysis/compare-branches/#new-dependencies-only","title":"New Dependencies Only","text":"<pre><code>jnkn diff main --show added\n</code></pre>"},{"location":"how-to/analysis/compare-branches/#removed-dependencies-only","title":"Removed Dependencies Only","text":"<pre><code>jnkn diff main --show removed\n</code></pre>"},{"location":"how-to/analysis/compare-branches/#filter-by-type","title":"Filter by Type","text":"<pre><code>jnkn diff main --type env_var\n</code></pre>"},{"location":"how-to/analysis/compare-branches/#markdown-report","title":"Markdown Report","text":"<pre><code>jnkn diff main --format markdown\n</code></pre> <p>Output:</p> <pre><code>## Dependency Changes: main \u2192 HEAD\n\n### Added (3)\n| Type | Artifact |\n|------|----------|\n| env_var | env:NEW_API_KEY |\n| code_file | file://src/api/v2.py |\n| edge | src/api/v2.py \u2192 NEW_API_KEY |\n\n### Removed (1)\n| Type | Artifact |\n|------|----------|\n| env_var | env:DEPRECATED_VAR |\n</code></pre>"},{"location":"how-to/analysis/compare-branches/#performance","title":"Performance","text":"<p>Diff requires scanning both branches. For large repos, this can be slow.</p> <p>Tips:</p> <ul> <li>Use <code>--dir</code> to limit scope</li> <li>Cache base branch database in CI</li> <li>Run only on relevant file changes</li> </ul>"},{"location":"how-to/analysis/export-results/","title":"Export Results","text":"<p>Export Jnkn results in various formats for integration with other tools.</p>"},{"location":"how-to/analysis/export-results/#json-export","title":"JSON Export","text":"<p>Default format, suitable for scripting:</p> <pre><code>jnkn blast env:X --format json &gt; results.json\n</code></pre> <pre><code>jnkn stats --format json &gt; stats.json\n</code></pre>"},{"location":"how-to/analysis/export-results/#sarif-export","title":"SARIF Export","text":"<p>For IDE integration and GitHub Advanced Security:</p> <pre><code>jnkn blast env:X --format sarif &gt; jnkn.sarif\n</code></pre> <p>Upload to GitHub:</p> <pre><code>- name: Upload SARIF\n  uses: github/codeql-action/upload-sarif@v3\n  with:\n    sarif_file: jnkn.sarif\n</code></pre>"},{"location":"how-to/analysis/export-results/#csv-export","title":"CSV Export","text":"<p>For spreadsheet analysis:</p> <pre><code>jnkn stats --nodes --format csv &gt; nodes.csv\njnkn stats --edges --format csv &gt; edges.csv\n</code></pre>"},{"location":"how-to/analysis/export-results/#graphml-export","title":"GraphML Export","text":"<p>For visualization in tools like Gephi or yEd:</p> <pre><code>jnkn export --format graphml &gt; graph.graphml\n</code></pre>"},{"location":"how-to/analysis/export-results/#dot-export","title":"DOT Export","text":"<p>For Graphviz visualization:</p> <pre><code>jnkn export --format dot &gt; graph.dot\ndot -Tpng graph.dot -o graph.png\n</code></pre>"},{"location":"how-to/analysis/export-results/#export-full-graph","title":"Export Full Graph","text":"<p>Export all nodes and edges:</p> <pre><code>jnkn export --format json &gt; full-graph.json\n</code></pre> <p>Structure:</p> <pre><code>{\n  \"nodes\": [\n    {\"id\": \"env:DATABASE_URL\", \"type\": \"env_var\", \"metadata\": {...}},\n    ...\n  ],\n  \"edges\": [\n    {\"source\": \"file://src/app.py\", \"target\": \"env:DATABASE_URL\", \"type\": \"reads\"},\n    ...\n  ]\n}\n</code></pre>"},{"location":"how-to/analysis/export-results/#filter-exports","title":"Filter Exports","text":"<p>Export only certain types:</p> <pre><code>jnkn export --type env_var --format json &gt; env-vars.json\njnkn export --type infra --format json &gt; infrastructure.json\n</code></pre>"},{"location":"how-to/analysis/export-results/#pipe-to-other-tools","title":"Pipe to Other Tools","text":"<pre><code># Pretty print\njnkn blast env:X | jq .\n\n# Count impacted files\njnkn blast env:X | jq '.impacted_artifacts | length'\n\n# Get unique impacted types\njnkn blast env:X | jq -r '.breakdown | keys[]'\n</code></pre>"},{"location":"how-to/analysis/query-dependency-graph/","title":"Querying the Graph","text":"<p>Advanced graph traversal.</p>"},{"location":"how-to/configuration/configure-confidence/","title":"Configure Confidence","text":"<p>Tune confidence thresholds to balance precision and recall.</p>"},{"location":"how-to/configuration/configure-confidence/#understanding-confidence","title":"Understanding Confidence","text":"<p>Confidence scores range from 0.0 to 1.0:</p> Score Level Meaning 0.8 - 1.0 HIGH Very likely a real dependency 0.5 - 0.8 MEDIUM Probably related, review recommended 0.0 - 0.5 LOW Weak signal, often false positive"},{"location":"how-to/configuration/configure-confidence/#setting-the-threshold","title":"Setting the Threshold","text":"<p>In <code>.jnkn/config.yaml</code>:</p> <pre><code>stitching:\n  min_confidence: 0.5  # Default\n</code></pre> <p>Higher threshold = fewer matches, fewer false positives:</p> <pre><code>stitching:\n  min_confidence: 0.7  # More conservative\n</code></pre> <p>Lower threshold = more matches, may catch subtle dependencies:</p> <pre><code>stitching:\n  min_confidence: 0.3  # More permissive\n</code></pre>"},{"location":"how-to/configuration/configure-confidence/#per-rule-thresholds","title":"Per-Rule Thresholds","text":"<p>Different rules can have different thresholds:</p> <pre><code>stitching:\n  min_confidence: 0.5  # Default\n\n  rule_overrides:\n    EnvVarToInfraRule:\n      min_confidence: 0.6  # Stricter for env\u2192infra\n    K8sToSecretRule:\n      min_confidence: 0.4  # More permissive for K8s\n</code></pre>"},{"location":"how-to/configuration/configure-confidence/#signal-weights","title":"Signal Weights","text":"<p>Customize how confidence is calculated:</p> <pre><code>stitching:\n  confidence:\n    signals:\n      exact_match: 1.0\n      normalized_match: 0.9\n      token_overlap_high: 0.85\n      token_overlap_medium: 0.7\n      suffix_match: 0.6\n      contains: 0.5\n\n    penalties:\n      short_token: 0.5      # Tokens &lt; 4 chars\n      common_token: 0.7     # Generic words like \"url\", \"host\"\n      ambiguous: 0.8        # Multiple possible matches\n</code></pre>"},{"location":"how-to/configuration/configure-confidence/#token-configuration","title":"Token Configuration","text":""},{"location":"how-to/configuration/configure-confidence/#blocked-tokens","title":"Blocked Tokens","text":"<p>Tokens that provide no matching signal:</p> <pre><code>stitching:\n  blocked_tokens:\n    - id\n    - key\n    - url\n    - host\n    - port\n    - name\n    - value\n    - config\n</code></pre>"},{"location":"how-to/configuration/configure-confidence/#minimum-token-length","title":"Minimum Token Length","text":"<p>Ignore short tokens:</p> <pre><code>stitching:\n  min_token_length: 3  # Default\n</code></pre>"},{"location":"how-to/configuration/configure-confidence/#low-value-tokens","title":"Low-Value Tokens","text":"<p>Tokens that reduce confidence when matched:</p> <pre><code>stitching:\n  low_value_tokens:\n    - aws\n    - prod\n    - dev\n    - main\n    - test\n</code></pre>"},{"location":"how-to/configuration/configure-confidence/#finding-the-right-balance","title":"Finding the Right Balance","text":""},{"location":"how-to/configuration/configure-confidence/#start-conservative","title":"Start Conservative","text":"<pre><code>stitching:\n  min_confidence: 0.7\n</code></pre> <p>Run <code>jnkn scan</code> and check results. If you're missing real dependencies, lower the threshold.</p>"},{"location":"how-to/configuration/configure-confidence/#check-what-youre-missing","title":"Check What You're Missing","text":"<pre><code>jnkn explain env:DATABASE_URL infra:db_instance --why-not\n</code></pre> <p>If the confidence is 0.65 but the match is real, consider lowering your threshold.</p>"},{"location":"how-to/configuration/configure-confidence/#iterate","title":"Iterate","text":"<ol> <li>Scan with current settings</li> <li>Review a sample of matches</li> <li>Adjust threshold or add suppressions</li> <li>Repeat</li> </ol>"},{"location":"how-to/configuration/configure-confidence/#environment-variable-override","title":"Environment Variable Override","text":"<p>Override in CI without changing config:</p> <pre><code>JUNKAN_MIN_CONFIDENCE=0.6 jnkn scan\n</code></pre>"},{"location":"how-to/configuration/environment-variables/","title":"Environment Variables","text":"<p>Configuring Jnkn via env vars.</p>"},{"location":"how-to/configuration/ignore-files/","title":"Ignore Files","text":"<p>Using .jnknignore.</p>"},{"location":"how-to/configuration/manage-suppressions/","title":"Manage Suppressions","text":"<p>Handle false positives by suppressing incorrect matches.</p>"},{"location":"how-to/configuration/manage-suppressions/#add-a-suppression","title":"Add a Suppression","text":""},{"location":"how-to/configuration/manage-suppressions/#specific-match","title":"Specific Match","text":"<pre><code>jnkn suppress add \"env:HOST\" \"infra:ghost_writer\" \\\n  --reason \"Unrelated - ghost_writer is a logging service\"\n</code></pre>"},{"location":"how-to/configuration/manage-suppressions/#pattern-match","title":"Pattern Match","text":"<p>Use globs for broader suppression:</p> <pre><code># All *_ID env vars\njnkn suppress add \"env:*_ID\" \"infra:*\" \\\n  --reason \"ID fields are too generic\"\n\n# All test infrastructure\njnkn suppress add \"*\" \"infra:test_*\" \\\n  --reason \"Test infrastructure\"\n\n# Specific prefix\njnkn suppress add \"env:LEGACY_*\" \"*\" \\\n  --reason \"Legacy vars being deprecated\"\n</code></pre>"},{"location":"how-to/configuration/manage-suppressions/#list-suppressions","title":"List Suppressions","text":"<pre><code>jnkn suppress list\n</code></pre> <pre><code>ID  Source        Target         Reason                  Enabled  Expires\n1   env:HOST      infra:ghost_*  Unrelated services      yes      never\n2   env:*_ID      infra:*        ID fields too generic   yes      never\n3   *             infra:test_*   Test infrastructure     yes      2024-06-01\n</code></pre>"},{"location":"how-to/configuration/manage-suppressions/#remove-a-suppression","title":"Remove a Suppression","text":"<p>By ID:</p> <pre><code>jnkn suppress remove 1\n</code></pre> <p>By pattern:</p> <pre><code>jnkn suppress remove --source \"env:*_ID\" --target \"infra:*\"\n</code></pre>"},{"location":"how-to/configuration/manage-suppressions/#test-a-suppression","title":"Test a Suppression","text":"<p>Check if a match would be suppressed:</p> <pre><code>jnkn suppress test env:USER_ID infra:user_service\n</code></pre> <pre><code>\u2713 Would be SUPPRESSED by rule 2: \"env:*_ID\" \u2192 \"infra:*\"\n  Reason: ID fields too generic\n</code></pre>"},{"location":"how-to/configuration/manage-suppressions/#temporary-suppressions","title":"Temporary Suppressions","text":"<p>Add an expiration date:</p> <pre><code>jnkn suppress add \"env:X\" \"infra:Y\" \\\n  --reason \"Temporary during migration\" \\\n  --expires 2024-06-01\n</code></pre>"},{"location":"how-to/configuration/manage-suppressions/#enabledisable","title":"Enable/Disable","text":"<p>Disable without removing:</p> <pre><code>jnkn suppress disable 2\n</code></pre> <p>Re-enable:</p> <pre><code>jnkn suppress enable 2\n</code></pre>"},{"location":"how-to/configuration/manage-suppressions/#suppressions-file","title":"Suppressions File","text":"<p>Stored in <code>.jnkn/suppressions.yaml</code>:</p> <pre><code>suppressions:\n  - id: 1\n    source_pattern: \"env:HOST\"\n    target_pattern: \"infra:ghost_*\"\n    reason: \"Unrelated services\"\n    enabled: true\n    expires: null\n    created_at: \"2024-01-15T10:30:00Z\"\n\n  - id: 2\n    source_pattern: \"env:*_ID\"\n    target_pattern: \"infra:*\"\n    reason: \"ID fields too generic\"\n    enabled: true\n    expires: null\n    created_at: \"2024-01-15T10:35:00Z\"\n</code></pre> <p>Version Control</p> <p>Commit <code>.jnkn/suppressions.yaml</code> to git so the team shares suppressions.</p>"},{"location":"how-to/configuration/manage-suppressions/#glob-patterns","title":"Glob Patterns","text":"Pattern Matches Doesn't Match <code>env:*</code> <code>env:X</code>, <code>env:DATABASE_URL</code> <code>infra:X</code> <code>env:DB_*</code> <code>env:DB_HOST</code>, <code>env:DB_PORT</code> <code>env:DATABASE</code> <code>*_URL</code> <code>env:DATABASE_URL</code>, <code>infra:api_url</code> <code>env:URL_PREFIX</code> <code>infra:aws_*</code> <code>infra:aws_rds</code>, <code>infra:aws_s3</code> <code>infra:gcp_sql</code>"},{"location":"how-to/configuration/manage-suppressions/#bulk-import","title":"Bulk Import","text":"<p>Import from JSON:</p> <pre><code>cat suppressions.json | jnkn suppress import\n</code></pre> <pre><code>[\n  {\"source\": \"env:*_ID\", \"target\": \"infra:*\", \"reason\": \"Generic\"},\n  {\"source\": \"env:HOST\", \"target\": \"infra:ghost_*\", \"reason\": \"Unrelated\"}\n]\n</code></pre>"},{"location":"how-to/configuration/manage-suppressions/#export","title":"Export","text":"<pre><code>jnkn suppress export --format json &gt; suppressions-backup.json\n</code></pre>"},{"location":"how-to/integration/azure-devops/","title":"Azure DevOps","text":"<p>Setting up Azure Pipelines.</p>"},{"location":"how-to/integration/github-actions/","title":"GitHub Actions","text":"<p>Set up Jnkn in GitHub Actions CI/CD.</p>"},{"location":"how-to/integration/github-actions/#minimal-setup","title":"Minimal Setup","text":"<pre><code>name: Jnkn\non: [pull_request]\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      - run: pip install jnkn[full]\n      - run: jnkn scan\n      - run: jnkn stats\n</code></pre>"},{"location":"how-to/integration/github-actions/#with-pr-comments","title":"With PR Comments","text":"<pre><code>name: Jnkn Impact Analysis\non:\n  pull_request:\n    paths: ['**.py', '**.tf', '**.yaml']\n\npermissions:\n  contents: read\n  pull-requests: write\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n          cache: 'pip'\n\n      - run: pip install jnkn[full]\n\n      - name: Scan and analyze\n        id: jnkn\n        run: |\n          jnkn scan\n\n          # Analyze changed files\n          CHANGED=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | grep -E '\\.(py|tf)$' || true)\n\n          if [ -n \"$CHANGED\" ]; then\n            REPORT=$(mktemp)\n            for f in $CHANGED; do\n              echo \"### \\`$f\\`\" &gt;&gt; $REPORT\n              jnkn blast \"file://$f\" --format markdown &gt;&gt; $REPORT 2&gt;/dev/null || echo \"No dependencies\" &gt;&gt; $REPORT\n            done\n            echo \"report&lt;&lt;EOF\" &gt;&gt; $GITHUB_OUTPUT\n            cat $REPORT &gt;&gt; $GITHUB_OUTPUT\n            echo \"EOF\" &gt;&gt; $GITHUB_OUTPUT\n          fi\n\n      - name: Comment on PR\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const report = `${{ steps.jnkn.outputs.report }}`;\n            if (report) {\n              github.rest.issues.createComment({\n                issue_number: context.issue.number,\n                owner: context.repo.owner,\n                repo: context.repo.repo,\n                body: `## \ud83d\udd0d Jnkn Impact Analysis\\n\\n${report}`\n              });\n            }\n</code></pre>"},{"location":"how-to/integration/github-actions/#with-caching","title":"With Caching","text":"<pre><code>- uses: actions/cache@v4\n  with:\n    path: .jnkn/\n    key: jnkn-${{ runner.os }}-${{ hashFiles('**/*.py', '**/*.tf') }}\n    restore-keys: |\n      jnkn-${{ runner.os }}-\n</code></pre>"},{"location":"how-to/integration/github-actions/#block-on-high-impact","title":"Block on High Impact","text":"<pre><code>- name: Check impact threshold\n  run: |\n    MAX_IMPACT=10\n\n    for f in $(git diff --name-only origin/main...HEAD | grep -E '\\.(py|tf)$'); do\n      IMPACT=$(jnkn blast \"file://$f\" --format json 2&gt;/dev/null | jq '.total_impacted_count // 0')\n      if [ \"$IMPACT\" -gt \"$MAX_IMPACT\" ]; then\n        echo \"::error file=$f::High impact ($IMPACT) exceeds threshold ($MAX_IMPACT)\"\n        exit 1\n      fi\n    done\n</code></pre>"},{"location":"how-to/integration/github-actions/#matrix-strategy","title":"Matrix Strategy","text":"<p>Scan different components in parallel:</p> <pre><code>jobs:\n  analyze:\n    strategy:\n      matrix:\n        component: [backend, frontend, infrastructure]\n    steps:\n      - run: jnkn scan --dir ${{ matrix.component }}/\n</code></pre>"},{"location":"how-to/integration/github-actions/#reusable-workflow","title":"Reusable Workflow","text":"<p>Create <code>.github/workflows/jnkn-reusable.yml</code>:</p> <pre><code>name: Jnkn Reusable\non:\n  workflow_call:\n    inputs:\n      directory:\n        type: string\n        default: '.'\n      threshold:\n        type: number\n        default: 10\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      - run: pip install jnkn[full]\n      - run: jnkn scan --dir ${{ inputs.directory }}\n</code></pre> <p>Use in other workflows:</p> <pre><code>jobs:\n  jnkn:\n    uses: ./.github/workflows/jnkn-reusable.yml\n    with:\n      directory: src/\n      threshold: 5\n</code></pre>"},{"location":"how-to/integration/gitlab-ci/","title":"GitLab CI","text":"<p>Set up Jnkn in GitLab CI/CD pipelines.</p>"},{"location":"how-to/integration/gitlab-ci/#minimal-setup","title":"Minimal Setup","text":"<pre><code># .gitlab-ci.yml\njnkn:\n  image: python:3.11-slim\n  script:\n    - pip install jnkn[full]\n    - jnkn scan\n    - jnkn stats\n  only:\n    changes:\n      - \"**/*.py\"\n      - \"**/*.tf\"\n</code></pre>"},{"location":"how-to/integration/gitlab-ci/#with-caching","title":"With Caching","text":"<pre><code>jnkn:\n  image: python:3.11-slim\n  cache:\n    key: jnkn-$CI_COMMIT_REF_SLUG\n    paths:\n      - .jnkn/\n  script:\n    - pip install jnkn[full]\n    - jnkn scan\n    - jnkn stats\n</code></pre>"},{"location":"how-to/integration/gitlab-ci/#block-pipeline-on-high-impact","title":"Block Pipeline on High Impact","text":"<pre><code>jnkn:\n  image: python:3.11-slim\n  script:\n    - pip install jnkn[full] jq\n    - jnkn scan\n    - |\n      MAX_IMPACT=10\n      FAILED=0\n\n      for f in $(git diff --name-only origin/$CI_MERGE_REQUEST_TARGET_BRANCH_NAME...HEAD | grep -E '\\.(py|tf)$'); do\n        IMPACT=$(jnkn blast \"file://$f\" --format json 2&gt;/dev/null | jq '.total_impacted_count // 0')\n        if [ \"$IMPACT\" -gt \"$MAX_IMPACT\" ]; then\n          echo \"High impact ($IMPACT) for $f\"\n          FAILED=1\n        fi\n      done\n\n      if [ \"$FAILED\" -eq 1 ]; then\n        exit 1\n      fi\n  rules:\n    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n</code></pre>"},{"location":"how-to/integration/gitlab-ci/#docker-image","title":"Docker Image","text":"<p>Use a pre-built image for faster pipelines:</p> <pre><code>jnkn:\n  image: ghcr.io/jnkn-io/jnkn:latest\n  script:\n    - jnkn scan\n    - jnkn stats\n</code></pre>"},{"location":"how-to/integration/gitlab-ci/#parallel-jobs","title":"Parallel Jobs","text":"<pre><code>.jnkn-template:\n  image: python:3.11-slim\n  before_script:\n    - pip install jnkn[full]\n\njnkn-backend:\n  extends: .jnkn-template\n  script:\n    - jnkn scan --dir backend/\n\njnkn-infra:\n  extends: .jnkn-template\n  script:\n    - jnkn scan --dir terraform/\n</code></pre>"},{"location":"how-to/integration/gitlab-ci/#artifacts","title":"Artifacts","text":"<p>Save results for later stages:</p> <pre><code>jnkn:\n  script:\n    - jnkn scan\n    - jnkn stats --format json &gt; stats.json\n    - jnkn export --format json &gt; graph.json\n  artifacts:\n    paths:\n      - stats.json\n      - graph.json\n    expire_in: 1 week\n</code></pre>"},{"location":"how-to/integration/jenkins/","title":"Jenkins","text":"<p>Integrating with Jenkins.</p>"},{"location":"how-to/integration/jira-integration/","title":"Jira Integration","text":"<p>Creating tickets for breaking changes.</p>"},{"location":"how-to/integration/slack-notifications/","title":"Slack Notifications","text":"<p>Sending alerts to Slack.</p>"},{"location":"how-to/scanning/incremental-scanning/","title":"Incremental Scanning","text":"<p>Only re-scan files that have changed since the last scan.</p>"},{"location":"how-to/scanning/incremental-scanning/#how-it-works","title":"How It Works","text":"<p>Jnkn stores a hash of each file. On subsequent scans, unchanged files are skipped:</p> <pre><code>First scan:  847 files parsed\nSecond scan: 12 files parsed (835 unchanged)\n</code></pre>"},{"location":"how-to/scanning/incremental-scanning/#usage","title":"Usage","text":"<p>Incremental scanning is automatic:</p> <pre><code>jnkn scan  # First time: full scan\n# ... make changes ...\njnkn scan  # Only changed files\n</code></pre>"},{"location":"how-to/scanning/incremental-scanning/#force-full-rescan","title":"Force Full Rescan","text":"<p>When you need to regenerate everything:</p> <pre><code>jnkn scan --full\n</code></pre> <p>Use this when:</p> <ul> <li>Upgrading Jnkn (new patterns detected)</li> <li>Changing configuration</li> <li>Debugging issues</li> </ul>"},{"location":"how-to/scanning/incremental-scanning/#check-file-status","title":"Check File Status","text":"<p>See which files would be scanned:</p> <pre><code>jnkn scan --dry-run\n</code></pre> <pre><code>Would scan:\n  src/config.py (modified)\n  src/api/users.py (modified)\n  terraform/rds.tf (new)\n\nWould skip:\n  src/main.py (unchanged)\n  src/utils.py (unchanged)\n  ... 842 more unchanged files\n</code></pre>"},{"location":"how-to/scanning/incremental-scanning/#git-integration","title":"Git Integration","text":"<p>Scan only files changed in a PR:</p> <pre><code># Get changed files from git\nCHANGED=$(git diff --name-only origin/main...HEAD)\n\n# Scan just those files\njnkn scan --files $CHANGED\n</code></pre> <p>Or in CI:</p> <pre><code>- name: Scan changed files\n  run: |\n    CHANGED=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }})\n    jnkn scan --files $CHANGED\n</code></pre>"},{"location":"how-to/scanning/incremental-scanning/#cache-location","title":"Cache Location","text":"<p>The database is stored at <code>.jnkn/jnkn.db</code> by default.</p> <p>To use a different location:</p> <pre><code>jnkn scan --db /path/to/jnkn.db\n</code></pre>"},{"location":"how-to/scanning/incremental-scanning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/scanning/incremental-scanning/#files-not-being-detected-as-changed","title":"Files not being detected as changed","text":"<p>Check the stored hash:</p> <pre><code>jnkn stats --show-hashes | grep myfile.py\n</code></pre>"},{"location":"how-to/scanning/incremental-scanning/#scan-is-slow-despite-few-changes","title":"Scan is slow despite few changes","text":"<p>The stitching phase runs on all nodes, not just changed files. This is necessary to detect new cross-domain links.</p> <p>For very large graphs, consider splitting by domain (see Scan a Monorepo).</p>"},{"location":"how-to/scanning/parallel-scanning/","title":"Parallel Scanning","text":"<p>Optimizing performance.</p>"},{"location":"how-to/scanning/scan-monorepo/","title":"Scan a Monorepo","text":"<p>Strategies for scanning large codebases efficiently.</p>"},{"location":"how-to/scanning/scan-monorepo/#the-challenge","title":"The Challenge","text":"<p>Monorepos can have thousands of files. A full scan may take minutes and produce a huge graph.</p>"},{"location":"how-to/scanning/scan-monorepo/#strategy-1-scope-to-directories","title":"Strategy 1: Scope to Directories","text":"<p>Scan only relevant directories:</p> <pre><code>jnkn scan --dir src/services --dir terraform/\n</code></pre> <p>Or use <code>.jnknignore</code>:</p> <pre><code># .jnknignore\nnode_modules/\nvendor/\n*.test.py\n*_test.go\n__pycache__/\n.git/\ndocs/\nscripts/\n</code></pre>"},{"location":"how-to/scanning/scan-monorepo/#strategy-2-incremental-scanning","title":"Strategy 2: Incremental Scanning","text":"<p>Only re-scan changed files:</p> <pre><code># First scan (full)\njnkn scan\n\n# Subsequent scans (incremental)\njnkn scan  # Automatically detects unchanged files via hash\n</code></pre> <p>Force full rescan if needed:</p> <pre><code>jnkn scan --full\n</code></pre>"},{"location":"how-to/scanning/scan-monorepo/#strategy-3-parallel-scanning","title":"Strategy 3: Parallel Scanning","text":"<p>Use multiple cores:</p> <pre><code>jnkn scan --jobs 8\n</code></pre> <p>Or auto-detect:</p> <pre><code>jnkn scan --jobs auto  # Uses CPU count\n</code></pre>"},{"location":"how-to/scanning/scan-monorepo/#strategy-4-split-by-domain","title":"Strategy 4: Split by Domain","text":"<p>For very large monorepos, maintain separate databases:</p> <pre><code># Backend team\njnkn scan --dir backend/ --db .jnkn/backend.db\n\n# Infrastructure team  \njnkn scan --dir terraform/ --db .jnkn/infra.db\n\n# Query across both\njnkn blast env:DATABASE_URL \\\n  --db .jnkn/backend.db \\\n  --db .jnkn/infra.db\n</code></pre>"},{"location":"how-to/scanning/scan-monorepo/#strategy-5-ci-caching","title":"Strategy 5: CI Caching","text":"<p>Cache the database between CI runs:</p> <pre><code>- uses: actions/cache@v4\n  with:\n    path: .jnkn/\n    key: jnkn-${{ hashFiles('**/*.py', '**/*.tf') }}\n    restore-keys: jnkn-\n</code></pre>"},{"location":"how-to/scanning/scan-monorepo/#performance-tips","title":"Performance Tips","text":"Files Expected Scan Time Recommendation &lt; 100 &lt; 5 seconds Default settings 100-1000 5-30 seconds Enable caching 1000-10000 30s - 3 min Parallel + incremental &gt; 10000 3+ min Split by domain"},{"location":"how-to/scanning/scan-monorepo/#monitoring-scan-performance","title":"Monitoring Scan Performance","text":"<pre><code>jnkn scan --verbose\n</code></pre> <p>Output includes timing:</p> <pre><code>Parsing: 12.3s (847 files)\nStitching: 3.2s (156 rules evaluated)\nStorage: 0.8s (2341 nodes written)\nTotal: 16.3s\n</code></pre>"},{"location":"how-to/scanning/scan-specific-directories/","title":"Filtering Scans","text":"<p>Using include/exclude patterns.</p>"},{"location":"how-to/troubleshooting/no-matches-found/","title":"No Matches Found","text":"<p>Debug why Jnkn isn't detecting dependencies.</p>"},{"location":"how-to/troubleshooting/no-matches-found/#check-what-was-scanned","title":"Check What Was Scanned","text":"<pre><code>jnkn stats\n</code></pre> <p>If <code>Nodes: 0</code>, no files were parsed.</p>"},{"location":"how-to/troubleshooting/no-matches-found/#verify-file-discovery","title":"Verify File Discovery","text":"<pre><code>jnkn scan --verbose\n</code></pre> <p>Look for:</p> <pre><code>Scanning /path/to/project\n  Found 0 Python files\n  Found 0 Terraform files\n</code></pre>"},{"location":"how-to/troubleshooting/no-matches-found/#common-causes","title":"Common Causes","text":"<p>Wrong directory:</p> <pre><code># Make sure you're in the right place\njnkn scan --dir ./src\n</code></pre> <p>Files ignored:</p> <p>Check <code>.jnknignore</code>:</p> <pre><code># This ignores everything!\n*\n</code></pre> <p>Unsupported extension:</p> <p>Jnkn only scans <code>.py</code>, <code>.tf</code>, <code>.yaml</code>, <code>.yml</code> by default.</p>"},{"location":"how-to/troubleshooting/no-matches-found/#verify-pattern-detection","title":"Verify Pattern Detection","text":"<p>For a specific file:</p> <pre><code>jnkn scan --verbose --files src/config.py\n</code></pre> <p>Look for detected patterns:</p> <pre><code>src/config.py:\n  Detected: env:DATABASE_URL (os.getenv)\n  Detected: env:API_KEY (pydantic_settings)\n</code></pre>"},{"location":"how-to/troubleshooting/no-matches-found/#common-causes_1","title":"Common Causes","text":"<p>Pattern not supported:</p> <p>Check Supported Patterns. Your pattern may not be implemented yet.</p> <p>Syntax variation:</p> <pre><code># Supported\nos.getenv(\"DATABASE_URL\")\n\n# NOT supported (variable key)\nkey = \"DATABASE_URL\"\nos.getenv(key)\n</code></pre> <p>Comments or strings:</p> <pre><code># This is NOT detected (it's a comment)\n# os.getenv(\"DATABASE_URL\")\n\n# This is NOT detected (it's a string)\nexample = 'os.getenv(\"DATABASE_URL\")'\n</code></pre>"},{"location":"how-to/troubleshooting/no-matches-found/#check-stitching","title":"Check Stitching","text":"<p>If nodes exist but no cross-domain edges:</p> <pre><code>jnkn stats\n</code></pre> <pre><code>Nodes: 50\nEdges: 45\nCross-domain edges: 0  # Problem!\n</code></pre>"},{"location":"how-to/troubleshooting/no-matches-found/#verify-tokens","title":"Verify Tokens","text":"<pre><code>jnkn explain env:DATABASE_URL infra:db_instance\n</code></pre> <pre><code>Source tokens: [database, url]\nTarget tokens: [db, instance]\nToken overlap: 0/2  # No match!\n</code></pre> <p>Solution: The names are too different. Consider:</p> <ul> <li>Renaming to match conventions</li> <li>Creating a custom stitching rule</li> <li>Lowering confidence threshold</li> </ul>"},{"location":"how-to/troubleshooting/no-matches-found/#check-confidence","title":"Check Confidence","text":"<pre><code>jnkn explain env:DB_HOST infra:db_host\n</code></pre> <pre><code>Confidence: 0.45 (REJECTED)\nThreshold: 0.50\n</code></pre> <p>Solution: Lower the threshold:</p> <pre><code>stitching:\n  min_confidence: 0.4\n</code></pre>"},{"location":"how-to/troubleshooting/no-matches-found/#check-for-suppressions","title":"Check for Suppressions","text":"<p>A suppression might be hiding the match:</p> <pre><code>jnkn suppress test env:DATABASE_URL infra:db_instance\n</code></pre> <pre><code>\u2713 Would be SUPPRESSED by rule 2\n</code></pre> <p>Solution: Remove or disable the suppression:</p> <pre><code>jnkn suppress remove 2\n</code></pre>"},{"location":"how-to/troubleshooting/no-matches-found/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code>JUNKAN_LOG_LEVEL=DEBUG jnkn scan\n</code></pre> <p>This shows detailed parsing and matching information.</p>"},{"location":"how-to/troubleshooting/no-matches-found/#still-stuck","title":"Still Stuck?","text":"<ol> <li>Check GitHub Issues for similar problems</li> <li>Run <code>jnkn feedback</code> to report a bug</li> <li>Ask in the community Slack</li> </ol>"},{"location":"how-to/troubleshooting/parser-errors/","title":"Parser Errors","text":"<p>Handling syntax errors.</p>"},{"location":"how-to/troubleshooting/performance-issues/","title":"Performance Issues","text":"<p>Debugging slow scans.</p>"},{"location":"how-to/troubleshooting/too-many-false-positives/","title":"Too Many False Positives","text":"<p>Reduce incorrect matches when Jnkn links unrelated artifacts.</p>"},{"location":"how-to/troubleshooting/too-many-false-positives/#quick-fixes","title":"Quick Fixes","text":""},{"location":"how-to/troubleshooting/too-many-false-positives/#1-raise-confidence-threshold","title":"1. Raise Confidence Threshold","text":"<pre><code># .jnkn/config.yaml\nstitching:\n  min_confidence: 0.7  # Up from 0.5\n</code></pre>"},{"location":"how-to/troubleshooting/too-many-false-positives/#2-suppress-known-bad-patterns","title":"2. Suppress Known Bad Patterns","text":"<pre><code># Generic patterns that match too much\njnkn suppress add \"env:*_ID\" \"infra:*\" --reason \"ID fields too generic\"\njnkn suppress add \"env:*_KEY\" \"infra:*\" --reason \"KEY fields too generic\"\njnkn suppress add \"env:HOST\" \"*\" --reason \"HOST alone is too generic\"\n</code></pre>"},{"location":"how-to/troubleshooting/too-many-false-positives/#3-block-common-tokens","title":"3. Block Common Tokens","text":"<pre><code># .jnkn/config.yaml\nstitching:\n  blocked_tokens:\n    - id\n    - key\n    - url\n    - host\n    - port\n    - name\n    - config\n    - value\n    - data\n    - type\n</code></pre>"},{"location":"how-to/troubleshooting/too-many-false-positives/#systematic-approach","title":"Systematic Approach","text":""},{"location":"how-to/troubleshooting/too-many-false-positives/#step-1-identify-the-problem","title":"Step 1: Identify the Problem","text":"<p>List all current matches:</p> <pre><code>jnkn stats --edges --type cross_domain\n</code></pre>"},{"location":"how-to/troubleshooting/too-many-false-positives/#step-2-sample-and-review","title":"Step 2: Sample and Review","text":"<p>Pick 10 random matches and verify:</p> <pre><code>jnkn stats --edges --type cross_domain --format json | jq '.[0:10]'\n</code></pre> <p>For each, ask: \"Is this a real dependency?\"</p>"},{"location":"how-to/troubleshooting/too-many-false-positives/#step-3-categorize-false-positives","title":"Step 3: Categorize False Positives","text":"<p>Common categories:</p> Category Example Solution Generic tokens <code>HOST</code> matches everything Block token Naming collision <code>user_id</code> matches <code>uuid</code> Suppress pattern Low confidence 0.51 sneaking through Raise threshold Wrong domain Test \u2192 Prod matches Suppress pattern"},{"location":"how-to/troubleshooting/too-many-false-positives/#step-4-apply-fixes","title":"Step 4: Apply Fixes","text":"<p>For generic tokens:</p> <pre><code>stitching:\n  blocked_tokens:\n    - host\n    - user\n</code></pre> <p>For naming collisions:</p> <pre><code>jnkn suppress add \"env:USER_ID\" \"infra:uuid_*\" --reason \"Collision\"\n</code></pre> <p>For low confidence:</p> <pre><code>stitching:\n  min_confidence: 0.6\n</code></pre> <p>For wrong domain:</p> <pre><code>jnkn suppress add \"*\" \"infra:test_*\" --reason \"Test infra\"\njnkn suppress add \"*\" \"infra:*_dev\" --reason \"Dev infra\"\n</code></pre>"},{"location":"how-to/troubleshooting/too-many-false-positives/#step-5-verify","title":"Step 5: Verify","text":"<p>Rescan and check:</p> <pre><code>jnkn scan --full\njnkn stats --edges --type cross_domain\n</code></pre>"},{"location":"how-to/troubleshooting/too-many-false-positives/#advanced-custom-penalties","title":"Advanced: Custom Penalties","text":"<p>Add penalties for tokens that reduce confidence:</p> <pre><code>stitching:\n  confidence:\n    penalties:\n      short_token: 0.5    # &lt; 4 chars\n      common_token: 0.6   # Generic words\n\n  low_value_tokens:\n    - aws\n    - gcp\n    - prod\n    - dev\n    - staging\n    - main\n    - default\n</code></pre>"},{"location":"how-to/troubleshooting/too-many-false-positives/#advanced-rule-specific-tuning","title":"Advanced: Rule-Specific Tuning","text":"<p>Different rules can have different thresholds:</p> <pre><code>stitching:\n  rule_overrides:\n    EnvVarToInfraRule:\n      min_confidence: 0.7     # Stricter\n      blocked_tokens:\n        - host\n        - port\n\n    K8sToSecretRule:\n      min_confidence: 0.5     # Default\n</code></pre>"},{"location":"how-to/troubleshooting/too-many-false-positives/#measuring-improvement","title":"Measuring Improvement","text":"<p>Track precision over time:</p> <pre><code># Before changes\njnkn stats --edges --type cross_domain | wc -l\n# 150 matches\n\n# After changes  \njnkn stats --edges --type cross_domain | wc -l\n# 45 matches (70% reduction)\n</code></pre> <p>Review the remaining matches to ensure you didn't lose real dependencies.</p>"},{"location":"how-to/troubleshooting/too-many-false-positives/#when-to-accept-some-false-positives","title":"When to Accept Some False Positives","text":"<p>Consider keeping false positives if:</p> <ul> <li>Manual review is fast (&lt; 5 matches per PR)</li> <li>The cost of missing a real dependency is high</li> <li>Your team prefers over-alerting to under-alerting</li> </ul> <p>The goal is useful signal, not zero noise.</p>"},{"location":"legal/license/","title":"License","text":"<p>MIT License details.</p>"},{"location":"legal/security-policy/","title":"Security Policy","text":"<p>Reporting vulnerabilities.</p>"},{"location":"openlineage/design/","title":"OpenLineage Integration Architecture","text":"<p>Version: 1.0.0 Last Updated: December 2024</p> <p>This document describes how Jnkn integrates with OpenLineage to provide complete pre-merge impact analysis for data pipelines.</p>"},{"location":"openlineage/design/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Executive Summary</li> <li>The Problem</li> <li>Why OpenLineage Alone Isn't Enough</li> <li>The Jnkn + OpenLineage Solution</li> <li>Architecture Overview</li> <li>Data Flow</li> <li>Integration Patterns</li> <li>Technical Implementation</li> <li>Confidence Model</li> <li>CI/CD Integration</li> <li>Deployment Options</li> </ol>"},{"location":"openlineage/design/#executive-summary","title":"Executive Summary","text":"<p>The Problem: Data pipeline failures are often caused by upstream code changes that existing tools only detect after downstream damage occurs.</p> <p>The Gap: OpenLineage captures runtime lineage (what happened), but cannot predict the impact of code changes before they're deployed.</p> <p>The Solution: Jnkn combines static code analysis with OpenLineage runtime data to provide pre-merge impact prediction with production-grade confidence.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                                                     \u2502\n\u2502   Code Change \u2192 Jnkn Analysis \u2192 OpenLineage Lookup \u2192 Impact Map   \u2502\n\u2502                                                                     \u2502\n\u2502   \"This PR modifies     \"Which jobs      \"These 4 systems          \u2502\n\u2502    column X\"             consume X?\"      will be affected\"         \u2502\n\u2502                                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"openlineage/design/#the-problem","title":"The Problem","text":""},{"location":"openlineage/design/#silent-data-pipeline-failures","title":"Silent Data Pipeline Failures","text":"<p>Data pipelines fail silently. Unlike application code that throws errors, data issues often manifest as:</p> <ul> <li>Wrong numbers in dashboards</li> <li>Degraded ML model performance</li> <li>Compliance reports with incorrect values</li> <li>Analytics that \"look fine\" but are subtly wrong</li> </ul>"},{"location":"openlineage/design/#the-root-cause","title":"The Root Cause","text":"<p>Most data incidents trace back to upstream changes:</p> <pre><code>graph LR\n    subgraph \"Root Causes\"\n        A[Schema change] \n        B[Column rename]\n        C[Filter logic change]\n        D[Aggregation change]\n    end\n\n    subgraph \"Silent Propagation\"\n        E[No immediate error]\n        F[Downstream jobs run]\n        G[Wrong data written]\n    end\n\n    subgraph \"Late Detection\"\n        H[Dashboard looks wrong]\n        I[Model accuracy drops]\n        J[Audit fails]\n    end\n\n    A --&gt; E --&gt; H\n    B --&gt; F --&gt; I\n    C --&gt; G --&gt; J\n\n    style H fill:#ff6b6b,stroke:#c92a2a,color:#fff\n    style I fill:#ff6b6b,stroke:#c92a2a,color:#fff\n    style J fill:#ff6b6b,stroke:#c92a2a,color:#fff</code></pre>"},{"location":"openlineage/design/#current-state-siloed-tooling","title":"Current State: Siloed Tooling","text":"Tool What It Sees What It Misses Unit Tests Code logic Downstream consumers OpenLineage Runtime dependencies Future code changes Data Quality Tools Anomalies after the fact Root cause in code Code Review Syntax and logic Production dependencies"},{"location":"openlineage/design/#why-openlineage-alone-isnt-enough","title":"Why OpenLineage Alone Isn't Enough","text":"<p>OpenLineage is excellent at capturing what happened. It records:</p> <ul> <li>Which jobs ran</li> <li>What datasets were read/written</li> <li>Column-level lineage (with proper instrumentation)</li> <li>Execution timestamps and durations</li> </ul> <pre><code>sequenceDiagram\n    participant Job as Spark Job\n    participant OL as OpenLineage\n    participant Marquez as Marquez/DataHub\n\n    Job-&gt;&gt;Job: Execute query\n    Job-&gt;&gt;OL: Emit START event\n    Job-&gt;&gt;Job: Read from table_a\n    Job-&gt;&gt;Job: Write to table_b\n    Job-&gt;&gt;OL: Emit COMPLETE event\n    OL-&gt;&gt;Marquez: Store lineage\n\n    Note over Marquez: Now we know table_a \u2192 table_b\n    Note over Marquez: But only AFTER execution</code></pre>"},{"location":"openlineage/design/#the-timing-gap","title":"The Timing Gap","text":"<pre><code>Timeline:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\n\n   PR Created    PR Merged    Job Runs    Failure Detected\n       \u2502             \u2502            \u2502              \u2502\n       \u25bc             \u25bc            \u25bc              \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Code  \u2502    \u2502 Deploy \u2502   \u2502 OpenL. \u2502    \u2502 PagerDuty\u2502\n   \u2502 Review\u2502    \u2502        \u2502   \u2502 Event  \u2502    \u2502 Alert    \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                          \u2502              \u2502\n       \u2502                          \u2502              \u2502\n       \u25bc                          \u25bc              \u25bc\n   No lineage              Lineage recorded   Too late\n   info available          (post-facto)       to prevent\n</code></pre> <p>OpenLineage tells you what broke. It cannot tell you what will break.</p>"},{"location":"openlineage/design/#the-jnkn-openlineage-solution","title":"The Jnkn + OpenLineage Solution","text":"<p>Jnkn bridges the timing gap by combining:</p> <ol> <li>Static Analysis (pre-merge): Parse code to understand what changes</li> <li>Runtime Lineage (OpenLineage): Know actual production dependencies</li> <li>Impact Prediction: Map code changes to affected systems</li> </ol> <pre><code>graph TB\n    subgraph \"Pre-Merge (Jnkn)\"\n        PR[Pull Request]\n        STATIC[Static Analysis]\n        DIFF[Change Detection]\n    end\n\n    subgraph \"Runtime Data (OpenLineage)\"\n        OL_DB[(OpenLineage&lt;br/&gt;Marquez/DataHub)]\n        JOBS[Job Catalog]\n        DEPS[Dependencies]\n    end\n\n    subgraph \"Combined Analysis\"\n        MERGE[Unified Graph]\n        IMPACT[Impact Calculator]\n        DECISION[Gate Decision]\n    end\n\n    PR --&gt; STATIC\n    STATIC --&gt; DIFF\n    DIFF --&gt; MERGE\n\n    OL_DB --&gt; JOBS\n    OL_DB --&gt; DEPS\n    JOBS --&gt; MERGE\n    DEPS --&gt; MERGE\n\n    MERGE --&gt; IMPACT\n    IMPACT --&gt; DECISION\n\n    DECISION --&gt; BLOCK[\u274c Block PR]\n    DECISION --&gt; WARN[\u26a0\ufe0f Warn + Notify]\n    DECISION --&gt; PASS[\u2705 Safe to Merge]\n\n    style MERGE fill:#4dabf7,stroke:#1971c2,color:#fff\n    style BLOCK fill:#ff6b6b,stroke:#c92a2a,color:#fff\n    style WARN fill:#fab005,stroke:#f59f00,color:#000\n    style PASS fill:#40c057,stroke:#2f9e44,color:#fff</code></pre>"},{"location":"openlineage/design/#value-proposition","title":"Value Proposition","text":"Capability OpenLineage Only Jnkn + OpenLineage Know what ran \u2705 \u2705 Know dependencies \u2705 \u2705 Predict impact pre-merge \u274c \u2705 Block risky PRs \u274c \u2705 Auto-notify downstream teams \u274c \u2705 Confidence scoring N/A \u2705 (100% for runtime data)"},{"location":"openlineage/design/#architecture-overview","title":"Architecture Overview","text":""},{"location":"openlineage/design/#system-components","title":"System Components","text":"<pre><code>graph TB\n    subgraph \"Developer Workflow\"\n        DEV[Developer]\n        PR[Pull Request]\n        CI[CI/CD Pipeline]\n    end\n\n    subgraph \"Jnkn Core\"\n        PARSER_ENGINE[Parser Engine]\n\n        subgraph \"Parsers\"\n            PY[Python Parser]\n            SPARK[PySpark Parser]\n            TF[Terraform Parser]\n            OL_PARSER[OpenLineage Parser]\n        end\n\n        GRAPH[Unified Graph]\n        STITCHER[Cross-Domain Stitcher]\n        ANALYZER[Impact Analyzer]\n    end\n\n    subgraph \"Data Sources\"\n        CODE[(Git Repository)]\n        MARQUEZ[(Marquez API)]\n        DATAHUB[(DataHub API)]\n    end\n\n    subgraph \"Outputs\"\n        REPORT[Impact Report]\n        GATE[PR Gate Decision]\n        NOTIFY[Team Notifications]\n    end\n\n    DEV --&gt; PR\n    PR --&gt; CI\n    CI --&gt; PARSER_ENGINE\n\n    CODE --&gt; PY\n    CODE --&gt; SPARK\n    CODE --&gt; TF\n    MARQUEZ --&gt; OL_PARSER\n    DATAHUB --&gt; OL_PARSER\n\n    PY --&gt; GRAPH\n    SPARK --&gt; GRAPH\n    TF --&gt; GRAPH\n    OL_PARSER --&gt; GRAPH\n\n    GRAPH --&gt; STITCHER\n    STITCHER --&gt; ANALYZER\n\n    ANALYZER --&gt; REPORT\n    ANALYZER --&gt; GATE\n    ANALYZER --&gt; NOTIFY\n\n    style OL_PARSER fill:#4dabf7,stroke:#1971c2,color:#fff\n    style GRAPH fill:#40c057,stroke:#2f9e44,color:#fff</code></pre>"},{"location":"openlineage/design/#data-model","title":"Data Model","text":"<pre><code>classDiagram\n    class Node {\n        +str id\n        +str name\n        +NodeType type\n        +Tuple tokens\n        +Dict metadata\n        +str source\n    }\n\n    class Edge {\n        +str source_id\n        +str target_id\n        +RelationshipType type\n        +float confidence\n        +Dict metadata\n    }\n\n    class NodeType {\n        &lt;&lt;enumeration&gt;&gt;\n        CODE_FILE\n        JOB\n        DATA_ASSET\n        COLUMN\n        ENV_VAR\n        INFRA_RESOURCE\n    }\n\n    class RelationshipType {\n        &lt;&lt;enumeration&gt;&gt;\n        READS\n        WRITES\n        TRANSFORMS\n        PROVIDES\n        DEPENDS_ON\n    }\n\n    Node --&gt; NodeType\n    Edge --&gt; RelationshipType\n    Edge --&gt; Node : source\n    Edge --&gt; Node : target</code></pre>"},{"location":"openlineage/design/#data-flow","title":"Data Flow","text":""},{"location":"openlineage/design/#end-to-end-flow","title":"End-to-End Flow","text":"<pre><code>sequenceDiagram\n    participant Dev as Developer\n    participant Git as GitHub/GitLab\n    participant CI as CI Pipeline\n    participant Jnkn as Jnkn\n    participant OL as OpenLineage API\n    participant Slack as Notifications\n\n    Dev-&gt;&gt;Git: Push PR (modifies etl_job.py)\n    Git-&gt;&gt;CI: Trigger pipeline\n    CI-&gt;&gt;Jnkn: Run impact analysis\n\n    rect rgb(240, 248, 255)\n        Note over Jnkn: Static Analysis Phase\n        Jnkn-&gt;&gt;Jnkn: Parse changed files\n        Jnkn-&gt;&gt;Jnkn: Extract columns, tables\n        Jnkn-&gt;&gt;Jnkn: Detect modifications\n    end\n\n    rect rgb(255, 248, 240)\n        Note over Jnkn,OL: Runtime Enrichment Phase\n        Jnkn-&gt;&gt;OL: Query job dependencies\n        OL--&gt;&gt;Jnkn: Return lineage graph\n        Jnkn-&gt;&gt;Jnkn: Merge static + runtime\n    end\n\n    rect rgb(240, 255, 240)\n        Note over Jnkn: Impact Analysis Phase\n        Jnkn-&gt;&gt;Jnkn: Traverse dependency graph\n        Jnkn-&gt;&gt;Jnkn: Identify affected systems\n        Jnkn-&gt;&gt;Jnkn: Calculate risk score\n    end\n\n    alt High Risk\n        Jnkn-&gt;&gt;CI: Block PR\n        Jnkn-&gt;&gt;Slack: Notify affected teams\n        CI-&gt;&gt;Git: Post check failure\n    else Medium Risk\n        Jnkn-&gt;&gt;CI: Warn\n        Jnkn-&gt;&gt;Slack: Notify for awareness\n        CI-&gt;&gt;Git: Post warning comment\n    else Low Risk\n        Jnkn-&gt;&gt;CI: Pass\n        CI-&gt;&gt;Git: Post success\n    end</code></pre>"},{"location":"openlineage/design/#openlineage-data-ingestion","title":"OpenLineage Data Ingestion","text":"<pre><code>flowchart TB\n    subgraph \"OpenLineage Sources\"\n        SPARK_EVENTS[Spark Events]\n        AIRFLOW_EVENTS[Airflow Events]\n        DBT_EVENTS[dbt Events]\n    end\n\n    subgraph \"Lineage Store\"\n        MARQUEZ[Marquez]\n        DATAHUB[DataHub]\n        CUSTOM[Custom Store]\n    end\n\n    subgraph \"Jnkn Ingestion\"\n        OL_PARSER[OpenLineage Parser]\n\n        subgraph \"Extraction\"\n            JOBS[Extract Jobs]\n            DATASETS[Extract Datasets]\n            COLUMNS[Extract Columns]\n            EDGES[Extract Relationships]\n        end\n    end\n\n    subgraph \"Unified Graph\"\n        GRAPH[(Jnkn Graph)]\n    end\n\n    SPARK_EVENTS --&gt; MARQUEZ\n    AIRFLOW_EVENTS --&gt; MARQUEZ\n    DBT_EVENTS --&gt; DATAHUB\n\n    MARQUEZ --&gt; OL_PARSER\n    DATAHUB --&gt; OL_PARSER\n    CUSTOM --&gt; OL_PARSER\n\n    OL_PARSER --&gt; JOBS\n    OL_PARSER --&gt; DATASETS\n    OL_PARSER --&gt; COLUMNS\n    OL_PARSER --&gt; EDGES\n\n    JOBS --&gt; GRAPH\n    DATASETS --&gt; GRAPH\n    COLUMNS --&gt; GRAPH\n    EDGES --&gt; GRAPH\n\n    style OL_PARSER fill:#4dabf7,stroke:#1971c2,color:#fff</code></pre>"},{"location":"openlineage/design/#integration-patterns","title":"Integration Patterns","text":""},{"location":"openlineage/design/#pattern-1-api-based-real-time","title":"Pattern 1: API-Based (Real-Time)","text":"<p>Query OpenLineage at PR time for freshest data.</p> <pre><code># In CI pipeline\nfrom jnkn.parsing.openlineage import OpenLineageParser\n\nparser = OpenLineageParser()\nevents = parser.fetch_from_marquez(\n    base_url=\"http://marquez.internal:5000\",\n    namespace=\"spark-production\"\n)\n\nfor item in parser.parse_events(events):\n    graph.add(item)\n</code></pre> <p>Pros: Always current Cons: Adds latency, requires network access from CI</p>"},{"location":"openlineage/design/#pattern-2-snapshot-based-periodic-sync","title":"Pattern 2: Snapshot-Based (Periodic Sync)","text":"<p>Export OpenLineage data periodically, commit to repo or artifact store.</p> <pre><code># .github/workflows/sync-lineage.yml\nname: Sync OpenLineage\non:\n  schedule:\n    - cron: '0 */6 * * *'  # Every 6 hours\n\njobs:\n  sync:\n    runs-on: ubuntu-latest\n    steps:\n      - run: |\n          curl -o lineage.json \\\n            \"$MARQUEZ_URL/api/v1/namespaces/spark/jobs\"\n      - uses: actions/upload-artifact@v3\n        with:\n          name: lineage-snapshot\n          path: lineage.json\n</code></pre> <p>Pros: Fast CI, no runtime dependency Cons: Up to 6 hours stale</p>"},{"location":"openlineage/design/#pattern-3-hybrid-cached-delta","title":"Pattern 3: Hybrid (Cached + Delta)","text":"<p>Use cached snapshot, fetch only recent changes.</p> <pre><code># Load cached baseline\ncached_graph = load_from_cache(\"lineage-snapshot.json\")\n\n# Fetch only events since last sync\nrecent_events = parser.fetch_from_marquez(\n    base_url=MARQUEZ_URL,\n    since=cached_graph.last_updated\n)\n\n# Merge\nfor item in parser.parse_events(recent_events):\n    cached_graph.add(item)\n</code></pre> <p>Pros: Fast + fresh Cons: More complex implementation</p>"},{"location":"openlineage/design/#technical-implementation","title":"Technical Implementation","text":""},{"location":"openlineage/design/#openlineage-parser","title":"OpenLineage Parser","text":"<pre><code>class OpenLineageParser:\n    \"\"\"\n    Converts OpenLineage events to Jnkn nodes and edges.\n\n    Supports:\n    - Marquez API\n    - DataHub API  \n    - Raw JSON event files\n    - Kafka event streams (planned)\n    \"\"\"\n\n    def parse_events(self, events: List[Dict]) -&gt; Iterator[Node | Edge]:\n        for event in events:\n            # Extract job node\n            yield self._create_job_node(event[\"job\"])\n\n            # Extract input datasets + READS edges\n            for input in event.get(\"inputs\", []):\n                yield self._create_dataset_node(input)\n                yield Edge(\n                    source_id=job_id,\n                    target_id=dataset_id,\n                    type=RelationshipType.READS,\n                    confidence=1.0  # Observed in production\n                )\n\n            # Extract output datasets + WRITES edges\n            for output in event.get(\"outputs\", []):\n                yield self._create_dataset_node(output)\n                yield Edge(\n                    source_id=job_id,\n                    target_id=dataset_id,\n                    type=RelationshipType.WRITES,\n                    confidence=1.0\n                )\n\n            # Extract column lineage if present\n            yield from self._extract_column_lineage(output)\n</code></pre>"},{"location":"openlineage/design/#cross-domain-stitching","title":"Cross-Domain Stitching","text":"<p>Jnkn links code files to OpenLineage jobs via token matching:</p> <pre><code>graph LR\n    subgraph \"Static Analysis\"\n        CODE[file:daily_user_etl.py]\n        TOKENS1[\"tokens: [daily, user, etl]\"]\n    end\n\n    subgraph \"OpenLineage\"\n        JOB[job:spark/daily_user_etl]\n        TOKENS2[\"tokens: [daily, user, etl]\"]\n    end\n\n    subgraph \"Stitching\"\n        MATCH[Token Overlap: 100%]\n        EDGE[Edge: PROVIDES&lt;br/&gt;confidence: 0.95]\n    end\n\n    CODE --&gt; TOKENS1\n    JOB --&gt; TOKENS2\n    TOKENS1 --&gt; MATCH\n    TOKENS2 --&gt; MATCH\n    MATCH --&gt; EDGE\n\n    style EDGE fill:#40c057,stroke:#2f9e44,color:#fff</code></pre>"},{"location":"openlineage/design/#unified-graph-queries","title":"Unified Graph Queries","text":"<pre><code># Find all downstream consumers of a table\ndownstream = graph.get_downstream(\"data:s3/warehouse/dim_users\")\n\n# Result:\n# [\n#   (\"job:spark/user_metrics\", confidence=1.0, path=[...]),\n#   (\"job:spark/exec_dashboard_loader\", confidence=1.0, path=[...]),\n#   (\"data:redshift/exec_dashboard\", confidence=1.0, path=[...]),\n# ]\n\n# Find what code files affect a critical table\nupstream = graph.get_upstream(\"data:redshift/exec_dashboard\")\n\n# Result includes both:\n# - OpenLineage jobs (runtime)\n# - Code files (static analysis)\n</code></pre>"},{"location":"openlineage/design/#confidence-model","title":"Confidence Model","text":""},{"location":"openlineage/design/#confidence-levels-by-source","title":"Confidence Levels by Source","text":"Source Confidence Rationale OpenLineage (runtime) 1.0 Observed in production Static: Direct column reference 0.95 High certainty from code Static: Variable-resolved column 0.80 Resolved at parse time Cross-domain stitch (exact match) 0.95 Name match Cross-domain stitch (token overlap) 0.70-0.85 Fuzzy match Static: Dynamic/unresolved 0.50 Flagged for review"},{"location":"openlineage/design/#confidence-propagation","title":"Confidence Propagation","text":"<pre><code>graph LR\n    A[Code File&lt;br/&gt;conf: 0.95] --&gt;|PROVIDES| B[Job&lt;br/&gt;conf: 1.0]\n    B --&gt;|WRITES| C[Table A&lt;br/&gt;conf: 1.0]\n    C --&gt;|READS| D[Job 2&lt;br/&gt;conf: 1.0]\n    D --&gt;|WRITES| E[Table B&lt;br/&gt;conf: 1.0]\n\n    subgraph \"Path Confidence\"\n        PATH[\"A \u2192 E: min(0.95, 1.0, 1.0, 1.0, 1.0) = 0.95\"]\n    end</code></pre> <p>Path confidence = minimum confidence along the path.</p>"},{"location":"openlineage/design/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"openlineage/design/#github-actions-example","title":"GitHub Actions Example","text":"<pre><code>name: Jnkn Impact Analysis\n\non:\n  pull_request:\n    paths:\n      - 'src/**/*.py'\n      - 'dbt/**/*.sql'\n      - 'terraform/**/*.tf'\n\njobs:\n  impact-analysis:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install Jnkn\n        run: pip install jnkn\n\n      - name: Fetch OpenLineage Data\n        env:\n          MARQUEZ_URL: ${{ secrets.MARQUEZ_URL }}\n        run: |\n          jnkn openlineage fetch \\\n            --url $MARQUEZ_URL \\\n            --output lineage-cache.json\n\n      - name: Run Impact Analysis\n        run: |\n          jnkn analyze \\\n            --changed-files \"${{ github.event.pull_request.changed_files }}\" \\\n            --openlineage lineage-cache.json \\\n            --critical-tables config/critical-tables.yaml \\\n            --output impact-report.json\n\n      - name: Post Results\n        uses: actions/github-script@v6\n        with:\n          script: |\n            const report = require('./impact-report.json');\n\n            let comment = '## \ud83d\udd0d Jnkn Impact Analysis\\n\\n';\n\n            if (report.critical_impact.length &gt; 0) {\n              comment += '### \ud83d\udea8 Critical Systems Affected\\n';\n              for (const item of report.critical_impact) {\n                comment += `- ${item.system} (confidence: ${item.confidence})\\n`;\n              }\n              comment += '\\n**This PR requires approval from data platform team.**\\n';\n            }\n\n            if (report.downstream_count &gt; 0) {\n              comment += `\\n### \ud83d\udcca Downstream Impact\\n`;\n              comment += `- **${report.downstream_count}** jobs affected\\n`;\n              comment += `- **${report.tables_affected}** tables affected\\n`;\n            }\n\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: comment\n            });\n\n      - name: Gate Decision\n        run: |\n          if jnkn gate --report impact-report.json --policy config/policy.yaml; then\n            echo \"\u2705 Safe to merge\"\n          else\n            echo \"\u274c Blocked - requires approval\"\n            exit 1\n          fi\n</code></pre>"},{"location":"openlineage/design/#critical-tables-configuration","title":"Critical Tables Configuration","text":"<pre><code># config/critical-tables.yaml\ncritical:\n  # Executive dashboards\n  - pattern: \"redshift/analytics.exec_*\"\n    severity: critical\n    owners:\n      - \"@data-platform-team\"\n      - \"@analytics-team\"\n    require_approval: true\n\n  # ML feature stores\n  - pattern: \"s3/ml-features/*\"\n    severity: high\n    owners:\n      - \"@ml-engineering\"\n    require_approval: true\n\n  # Compliance/audit\n  - pattern: \"*/compliance.*\"\n    severity: critical\n    owners:\n      - \"@compliance-team\"\n      - \"@data-governance\"\n    require_approval: true\n    notify_always: true\n</code></pre>"},{"location":"openlineage/design/#deployment-options","title":"Deployment Options","text":""},{"location":"openlineage/design/#option-1-standalone-cli","title":"Option 1: Standalone CLI","text":"<pre><code># Install\npip install jnkn\n\n# One-time analysis\njnkn analyze \\\n  --dir ./src \\\n  --openlineage-url http://marquez:5000 \\\n  --output report.json\n</code></pre>"},{"location":"openlineage/design/#option-2-cicd-integration","title":"Option 2: CI/CD Integration","text":"<p>See GitHub Actions example above. Also supports: - GitLab CI - Jenkins - CircleCI - Azure DevOps</p>"},{"location":"openlineage/design/#option-3-service-mode-planned","title":"Option 3: Service Mode (Planned)","text":"<pre><code># docker-compose.yml\nservices:\n  jnkn:\n    image: jnkn/server:latest\n    ports:\n      - \"8080:8080\"\n    environment:\n      - MARQUEZ_URL=http://marquez:5000\n      - GITHUB_TOKEN=${GITHUB_TOKEN}\n    volumes:\n      - ./config:/config\n</code></pre> <p>Provides: - REST API for impact queries - Webhook receiver for PR events - Continuous lineage sync - Web UI for exploration</p>"},{"location":"openlineage/design/#summary","title":"Summary","text":""},{"location":"openlineage/design/#before-reactive-firefighting","title":"Before: Reactive Firefighting","text":"<pre><code>Code Change \u2192 Deploy \u2192 Run \u2192 Failure \u2192 Alert \u2192 Debug \u2192 Fix \u2192 Deploy\n                                         \u2502\n                                    Hours/Days of\n                                    wrong data\n</code></pre>"},{"location":"openlineage/design/#after-proactive-prevention","title":"After: Proactive Prevention","text":"<pre><code>Code Change \u2192 Jnkn + OpenLineage \u2192 Impact Known \u2192 Coordinated Deploy\n                     \u2502\n              Block if critical,\n              notify stakeholders\n</code></pre>"},{"location":"openlineage/design/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>OpenLineage is necessary but not sufficient - It captures runtime truth but can't predict future impact.</p> </li> <li> <p>Static analysis alone misses production reality - Code doesn't know what actually runs in production.</p> </li> <li> <p>The combination is powerful - Static analysis predicts what changes, OpenLineage knows what depends on what, together they prevent incidents.</p> </li> <li> <p>Confidence matters - Runtime lineage has 100% confidence (observed), static analysis ranges from 50-95% (inferred). The unified model tracks this.</p> </li> <li> <p>CI/CD integration is the delivery mechanism - The value is realized when every PR gets automatic impact analysis before merge.</p> </li> </ol>"},{"location":"openlineage/design/#references","title":"References","text":"<ul> <li>OpenLineage Specification</li> <li>Marquez Documentation</li> <li>DataHub Lineage</li> <li>Jnkn Architecture</li> <li>PySpark Column Lineage</li> </ul>"},{"location":"pyspark/design/","title":"Column-Level Lineage Design","text":""},{"location":"pyspark/design/#problem-statement","title":"Problem Statement","text":"<p>Current parser extracts table-level lineage: - <code>process_events.py</code> READS <code>warehouse.dim_users</code> - <code>process_events.py</code> WRITES <code>warehouse.fact_events</code></p> <p>But engineers need column-level lineage: - <code>warehouse.fact_events.user_segment</code> comes from <code>warehouse.dim_users.segment</code> - <code>warehouse.fact_events.total_revenue</code> is computed from <code>staging.events.revenue</code></p>"},{"location":"pyspark/design/#extraction-patterns","title":"Extraction Patterns","text":""},{"location":"pyspark/design/#pattern-1-sql-strings-easiest","title":"Pattern 1: SQL Strings (Easiest)","text":"<pre><code>spark.sql(\"SELECT column_a, column_b FROM database.table WHERE column_a = 'xyz'\")\nspark.sql(\"\"\"\n    SELECT \n        t1.user_id,\n        t1.event_count,\n        t2.segment\n    FROM warehouse.events t1\n    JOIN warehouse.users t2 ON t1.user_id = t2.user_id\n\"\"\")\n</code></pre> <p>Approach: Use sqlglot to parse SQL and extract: - SELECT columns \u2192 output columns - FROM/JOIN tables \u2192 source tables - WHERE/ON columns \u2192 filter columns - Column aliases \u2192 transformations</p>"},{"location":"pyspark/design/#pattern-2-dataframe-method-chains-medium","title":"Pattern 2: DataFrame Method Chains (Medium)","text":"<pre><code>read_df = spark.read.parquet(\"hdfs://path/to/table\")\nfiltered_df = read_df.filter(col(\"column_a\") == 'xyz')\nselected_df = filtered_df.select(\"column_a\", \"column_b\")\nresult = selected_df.withColumn(\"new_col\", col(\"column_a\") + col(\"column_b\"))\n</code></pre> <p>Approach: Track DataFrame transformations: - <code>.select()</code> \u2192 output columns - <code>.filter()</code> / <code>.where()</code> \u2192 filter columns - <code>.withColumn()</code> \u2192 new column + source columns - <code>.groupBy()</code> \u2192 grouping columns - <code>.agg()</code> \u2192 aggregation columns - <code>.join()</code> \u2192 join keys + source columns</p>"},{"location":"pyspark/design/#pattern-3-dynamicvariable-references-hard","title":"Pattern 3: Dynamic/Variable References (Hard)","text":"<pre><code>grouping_fields = [\"column_a\", \"column_b\"]\nfilter_value = 'xyz'\n\nfiltered_df = read_df.filter(col(\"column_a\") == filter_value)\nselected_df = filtered_df.select(grouping_fields)\nselected_df = filtered_df.select(*grouping_fields)\n</code></pre> <p>Approach:  - Track variable assignments (AST analysis) - Resolve list literals to column names - Handle <code>*args</code> unpacking - Mark as \"dynamic\" when truly unresolvable</p>"},{"location":"pyspark/design/#pattern-4-f-expressions-and-col-medium","title":"Pattern 4: F-expressions and col() (Medium)","text":"<pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, sum, avg\n\ndf.select(F.col(\"a\"), F.sum(\"b\").alias(\"total_b\"))\ndf.withColumn(\"ratio\", F.col(\"a\") / F.col(\"b\"))\ndf.filter(F.col(\"status\") == \"active\")\n</code></pre> <p>Approach: Extract column names from: - <code>col(\"name\")</code>, <code>F.col(\"name\")</code> - <code>F.sum(\"name\")</code>, <code>F.avg(\"name\")</code>, etc. - <code>df[\"column\"]</code> bracket notation - <code>.alias(\"new_name\")</code> for output naming</p>"},{"location":"pyspark/design/#data-model","title":"Data Model","text":"<pre><code>@dataclass\nclass ColumnReference:\n    \"\"\"A reference to a column in the code.\"\"\"\n    column_name: str\n    table_name: Optional[str]  # If known\n    line_number: int\n    context: str  # \"select\", \"filter\", \"groupby\", \"join\", \"agg\", \"write\"\n    is_dynamic: bool = False  # True if from variable\n    alias: Optional[str] = None  # If renamed\n\n@dataclass\nclass ColumnLineage:\n    \"\"\"Lineage for a single output column.\"\"\"\n    output_column: str\n    output_table: str\n    source_columns: List[ColumnReference]\n    transformation: Optional[str]  # \"direct\", \"sum\", \"concat\", \"case\", etc.\n\n@dataclass \nclass FileColumnLineage:\n    \"\"\"All column lineage extracted from a file.\"\"\"\n    file_path: str\n    columns_read: List[ColumnReference]\n    columns_written: List[ColumnReference]\n    lineage: List[ColumnLineage]  # Output \u2192 Source mappings\n</code></pre>"},{"location":"pyspark/design/#implementation-strategy","title":"Implementation Strategy","text":""},{"location":"pyspark/design/#phase-1-sql-string-extraction-this-pr","title":"Phase 1: SQL String Extraction (This PR)","text":"<ul> <li>Parse SQL strings with sqlglot</li> <li>Extract SELECT columns, FROM tables, WHERE columns</li> <li>Handle JOINs and subqueries</li> <li>High accuracy, well-defined grammar</li> </ul>"},{"location":"pyspark/design/#phase-2-dataframe-method-tracking-next-pr","title":"Phase 2: DataFrame Method Tracking (Next PR)","text":"<ul> <li>Regex-based extraction of .select(), .filter(), etc.</li> <li>Extract column names from method arguments</li> <li>Track method chains to infer flow</li> </ul>"},{"location":"pyspark/design/#phase-3-variable-resolution-future","title":"Phase 3: Variable Resolution (Future)","text":"<ul> <li>AST-based analysis for variable tracking</li> <li>Resolve list literals and f-strings</li> <li>Mark unresolvable as \"dynamic\"</li> </ul>"},{"location":"pyspark/design/#confidence-levels","title":"Confidence Levels","text":"Pattern Confidence Example SQL string literal HIGH <code>spark.sql(\"SELECT a FROM t\")</code> Direct column ref HIGH <code>.select(\"col_a\", \"col_b\")</code> col() function HIGH <code>.filter(col(\"status\") == \"x\")</code> List variable (literal) MEDIUM <code>cols = [\"a\", \"b\"]; df.select(cols)</code> Config/param variable LOW <code>df.select(config.columns)</code> Dynamic expression UNKNOWN <code>df.select(*get_columns())</code>"},{"location":"pyspark/design/#output-format","title":"Output Format","text":"<pre><code>{\n  \"file\": \"jobs/process_events.py\",\n  \"table_lineage\": {\n    \"reads\": [\"staging.events\", \"warehouse.users\"],\n    \"writes\": [\"warehouse.fact_events\"]\n  },\n  \"column_lineage\": [\n    {\n      \"output\": \"warehouse.fact_events.user_segment\",\n      \"sources\": [\n        {\"table\": \"warehouse.users\", \"column\": \"segment\", \"transform\": \"direct\"}\n      ],\n      \"confidence\": \"high\"\n    },\n    {\n      \"output\": \"warehouse.fact_events.total_revenue\",\n      \"sources\": [\n        {\"table\": \"staging.events\", \"column\": \"revenue\", \"transform\": \"sum\"}\n      ],\n      \"confidence\": \"high\"\n    },\n    {\n      \"output\": \"warehouse.fact_events.event_date\",\n      \"sources\": [\n        {\"table\": \"staging.events\", \"column\": \"event_timestamp\", \"transform\": \"to_date\"}\n      ],\n      \"confidence\": \"high\"\n    }\n  ],\n  \"dynamic_columns\": [\n    {\n      \"location\": \"line 45\",\n      \"pattern\": \"df.select(*grouping_fields)\",\n      \"note\": \"columns from variable 'grouping_fields'\"\n    }\n  ]\n}\n</code></pre>"},{"location":"pyspark/design/#test-cases","title":"Test Cases","text":""},{"location":"pyspark/design/#sql-extraction","title":"SQL Extraction","text":"<pre><code># Simple SELECT\nspark.sql(\"SELECT user_id, name FROM users\")\n# \u2192 reads: users.user_id, users.name\n\n# JOIN\nspark.sql(\"\"\"\n    SELECT a.id, b.value \n    FROM table_a a \n    JOIN table_b b ON a.id = b.id\n\"\"\")\n# \u2192 reads: table_a.id, table_b.value, table_b.id\n\n# Aggregation\nspark.sql(\"SELECT category, SUM(amount) as total FROM sales GROUP BY category\")\n# \u2192 reads: sales.category, sales.amount\n# \u2192 output: total (from SUM(amount))\n</code></pre>"},{"location":"pyspark/design/#dataframe-method-extraction","title":"DataFrame Method Extraction","text":"<pre><code># Select\ndf.select(\"a\", \"b\", \"c\")\n# \u2192 columns: a, b, c\n\n# Filter\ndf.filter(col(\"status\") == \"active\")\n# \u2192 filter column: status\n\n# WithColumn\ndf.withColumn(\"full_name\", concat(col(\"first\"), col(\"last\")))\n# \u2192 output: full_name\n# \u2192 sources: first, last\n# \u2192 transform: concat\n\n# GroupBy + Agg\ndf.groupBy(\"category\").agg(sum(\"amount\").alias(\"total\"))\n# \u2192 grouping: category\n# \u2192 aggregation: amount \u2192 total (sum)\n</code></pre>"},{"location":"pyspark/design/#variable-resolution","title":"Variable Resolution","text":"<pre><code># Resolvable\ncols = [\"a\", \"b\", \"c\"]\ndf.select(cols)\n# \u2192 columns: a, b, c (confidence: medium)\n\n# Partially resolvable\nbase_cols = [\"id\", \"name\"]\ndf.select(*base_cols, \"extra_col\")\n# \u2192 columns: id, name, extra_col\n\n# Not resolvable\ndf.select(get_dynamic_columns())\n# \u2192 columns: UNKNOWN (flag for review)\n</code></pre>"},{"location":"reference/","title":"Reference","text":"<p>Complete reference documentation for Jnkn.</p>"},{"location":"reference/#cli","title":"CLI","text":"<ul> <li>CLI Overview \u2014 All commands and global options</li> </ul>"},{"location":"reference/#configuration","title":"Configuration","text":"<ul> <li>config.yaml \u2014 Main configuration file</li> <li>.jnknignore \u2014 File exclusion patterns</li> <li>suppressions.yaml \u2014 False positive management</li> </ul>"},{"location":"reference/#patterns","title":"Patterns","text":"<ul> <li>Overview \u2014 How pattern detection works</li> <li>Python \u2014 Python environment variable patterns</li> <li>Terraform \u2014 Terraform resource patterns</li> <li>Kubernetes \u2014 Kubernetes reference patterns</li> </ul>"},{"location":"reference/#output","title":"Output","text":"<ul> <li>JSON Schema \u2014 Output format specification</li> </ul>"},{"location":"reference/#glossary","title":"Glossary","text":"<ul> <li>Glossary \u2014 Terms and definitions</li> </ul>"},{"location":"reference/glossary/","title":"Glossary","text":"<p>Terms used in Jnkn documentation.</p>"},{"location":"reference/glossary/#a","title":"A","text":"<p>Artifact : A node in the dependency graph representing code, configuration, or infrastructure. Examples: env var, file, Terraform resource.</p>"},{"location":"reference/glossary/#b","title":"B","text":"<p>Blast Radius : The set of artifacts that would be affected by a change to a given artifact. Computed by traversing downstream dependencies.</p> <p>Blocked Token : A token (like \"id\", \"key\") that provides no signal for matching and is ignored during stitching.</p>"},{"location":"reference/glossary/#c","title":"C","text":"<p>Confidence : A score from 0.0 to 1.0 indicating how likely a cross-domain match is correct. Higher = more certain.</p> <p>Cross-Domain Dependency : A dependency that spans different technology domains (e.g., Python code depending on a Terraform resource).</p>"},{"location":"reference/glossary/#d","title":"D","text":"<p>Dependency Graph : A directed graph where nodes are artifacts and edges are dependencies between them.</p>"},{"location":"reference/glossary/#e","title":"E","text":"<p>Edge : A connection between two nodes in the dependency graph. Has a type (reads, imports, etc.) and optional metadata.</p> <p>Extractor : A component that detects specific patterns in source code (e.g., <code>os.getenv()</code> calls).</p>"},{"location":"reference/glossary/#g","title":"G","text":"<p>Graph : See Dependency Graph.</p>"},{"location":"reference/glossary/#m","title":"M","text":"<p>Matcher : A component that determines if two artifacts should be linked based on name similarity.</p>"},{"location":"reference/glossary/#n","title":"N","text":"<p>Node : An entity in the dependency graph. Has an ID, type, and metadata.</p> <p>Normalized Name : A standardized form of an artifact name used for matching. Example: <code>DATABASE_URL</code> \u2192 <code>databaseurl</code>.</p>"},{"location":"reference/glossary/#p","title":"P","text":"<p>Parser : A component that reads source files and extracts nodes and edges.</p> <p>Pattern : A code construct that Jnkn recognizes (e.g., <code>os.getenv(\"VAR\")</code>).</p> <p>Penalty : A multiplier that reduces confidence (e.g., 0.5 for short tokens).</p>"},{"location":"reference/glossary/#r","title":"R","text":"<p>Rule : See Stitching Rule.</p>"},{"location":"reference/glossary/#s","title":"S","text":"<p>Signal : A factor that increases confidence in a match (e.g., exact token overlap).</p> <p>Stitching : The process of creating edges between nodes from different domains based on name similarity.</p> <p>Stitching Rule : A rule that defines which node types can be matched and how confidence is calculated.</p> <p>Suppression : A user-defined rule to ignore specific matches (false positives).</p>"},{"location":"reference/glossary/#t","title":"T","text":"<p>Token : A word extracted from an artifact name. <code>DATABASE_URL</code> \u2192 <code>[\"database\", \"url\"]</code>.</p> <p>Token Matching : The technique of linking artifacts by comparing their tokenized names.</p>"},{"location":"reference/glossary/#artifact-types","title":"Artifact Types","text":"<p>code_file : A source code file (<code>.py</code>, <code>.js</code>, etc.).</p> <p>code_entity : A function or class definition within a file.</p> <p>env_var : An environment variable.</p> <p>infra_resource : A Terraform resource, variable, output, or data source.</p> <p>k8s_resource : A Kubernetes resource (Deployment, ConfigMap, Secret, etc.).</p> <p>data_asset : A dbt model, source, or seed.</p>"},{"location":"reference/glossary/#edge-types","title":"Edge Types","text":"<p>reads : Source reads a value from target (e.g., file reads env var).</p> <p>imports : Source imports target (e.g., Python import).</p> <p>provides : Source provides target (e.g., Terraform outputs an env var value).</p> <p>configures : Source configures target (e.g., K8s deployment uses ServiceAccount).</p> <p>contains : Source contains target (e.g., file contains function definition).</p> <p>references : Generic reference between artifacts.</p>"},{"location":"reference/api/analysis/blast_radius/","title":"Blast Radius","text":""},{"location":"reference/api/analysis/blast_radius/#jnkn.analysis.blast_radius","title":"<code>jnkn.analysis.blast_radius</code>","text":"<p>Blast Radius Analysis Engine.</p>"},{"location":"reference/api/analysis/blast_radius/#jnkn.analysis.blast_radius-classes","title":"Classes","text":""},{"location":"reference/api/analysis/blast_radius/#jnkn.analysis.blast_radius.BlastRadiusAnalyzer","title":"<code>BlastRadiusAnalyzer</code>","text":"<p>Calculates downstream impact of changes.</p> Source code in <code>src/jnkn/analysis/blast_radius.py</code> <pre><code>class BlastRadiusAnalyzer:\n    \"\"\"\n    Calculates downstream impact of changes.\n    \"\"\"\n    def __init__(self, graph: IGraph):\n        self.graph = graph\n\n    def calculate(self, source_node_ids: List[str], max_depth: int = -1) -&gt; Dict[str, Any]:\n        \"\"\"\n        Identify all nodes that would be impacted if source_nodes changed.\n        \"\"\"\n        # Delegate semantic traversal to the graph implementation\n        impacted_ids = self.graph.get_impacted_nodes(source_node_ids, max_depth)\n\n        # Sort for deterministic output\n        sorted_impacted = sorted(list(impacted_ids))\n\n        return {\n            \"source_artifacts\": source_node_ids,\n            \"impacted_artifacts\": sorted_impacted,\n            \"count\": len(sorted_impacted)\n        }\n</code></pre>"},{"location":"reference/api/analysis/blast_radius/#jnkn.analysis.blast_radius.BlastRadiusAnalyzer-functions","title":"Functions","text":""},{"location":"reference/api/analysis/blast_radius/#jnkn.analysis.blast_radius.BlastRadiusAnalyzer.calculate","title":"<code>calculate(source_node_ids, max_depth=-1)</code>","text":"<p>Identify all nodes that would be impacted if source_nodes changed.</p> Source code in <code>src/jnkn/analysis/blast_radius.py</code> <pre><code>def calculate(self, source_node_ids: List[str], max_depth: int = -1) -&gt; Dict[str, Any]:\n    \"\"\"\n    Identify all nodes that would be impacted if source_nodes changed.\n    \"\"\"\n    # Delegate semantic traversal to the graph implementation\n    impacted_ids = self.graph.get_impacted_nodes(source_node_ids, max_depth)\n\n    # Sort for deterministic output\n    sorted_impacted = sorted(list(impacted_ids))\n\n    return {\n        \"source_artifacts\": source_node_ids,\n        \"impacted_artifacts\": sorted_impacted,\n        \"count\": len(sorted_impacted)\n    }\n</code></pre>"},{"location":"reference/api/analysis/diff/","title":"Diff Analyzer","text":""},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer","title":"<code>jnkn.analysis.diff_analyzer</code>","text":"<p>Diff Analyzer - Analyze semantic changes between two dependency graphs.</p> <p>Identifies: 1. Added/Removed/Modified Nodes (Infrastructure, Code, Data) 2. Added/Removed Edges (Lineage, Dependencies) 3. Impact Assessment (Breaking Changes)</p>"},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer-classes","title":"Classes","text":""},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer.DiffAnalyzer","title":"<code>DiffAnalyzer</code>","text":"<p>Compares two DependencyGraphs to detect structural and semantic changes.</p> Source code in <code>src/jnkn/analysis/diff_analyzer.py</code> <pre><code>class DiffAnalyzer:\n    \"\"\"\n    Compares two DependencyGraphs to detect structural and semantic changes.\n    \"\"\"\n\n    def compare(self, base_graph: IGraph, head_graph: IGraph) -&gt; DiffReport:\n        \"\"\"\n        Compare base (original) vs head (new) graph.\n        \"\"\"\n        report = DiffReport()\n\n        # 1. Map Nodes for O(1) lookup\n        base_nodes = {n.id: n for n in base_graph.iter_nodes()}\n        head_nodes = {n.id: n for n in head_graph.iter_nodes()}\n\n        base_ids = set(base_nodes.keys())\n        head_ids = set(head_nodes.keys())\n\n        # 2. Detect Added/Removed Nodes\n        for nid in head_ids - base_ids:\n            node = head_nodes[nid]\n            report.added_nodes.append(node)\n            report.node_changes.append(NodeChange(\n                node_id=nid, name=node.name, type=node.type, change_type=ChangeType.ADDED\n            ))\n\n        for nid in base_ids - head_ids:\n            node = base_nodes[nid]\n            report.removed_nodes.append(node)\n            report.node_changes.append(NodeChange(\n                node_id=nid, name=node.name, type=node.type, change_type=ChangeType.REMOVED\n            ))\n\n        # 3. Detect Modified Nodes\n        for nid in base_ids.intersection(head_ids):\n            b_node = base_nodes[nid]\n            h_node = head_nodes[nid]\n\n            changes = []\n            if b_node.path != h_node.path:\n                changes.append(f\"Moved: {b_node.path} -&gt; {h_node.path}\")\n\n            if b_node.metadata != h_node.metadata:\n                # Robust metadata diff\n                b_keys = set(b_node.metadata.keys())\n                h_keys = set(h_node.metadata.keys())\n\n                added_keys = h_keys - b_keys\n                removed_keys = b_keys - h_keys\n\n                # Check for value changes in common keys\n                changed_keys = []\n                for k in b_keys.intersection(h_keys):\n                    if b_node.metadata[k] != h_node.metadata[k]:\n                        changed_keys.append(k)\n\n                details = []\n                if added_keys: details.append(f\"Meta added: {added_keys}\")\n                if removed_keys: details.append(f\"Meta removed: {removed_keys}\")\n                if changed_keys: details.append(f\"Meta changed: {changed_keys}\")\n\n                changes.append(\"; \".join(details))\n\n            if changes:\n                report.modified_nodes.append(h_node)\n                report.node_changes.append(NodeChange(\n                    node_id=nid,\n                    name=h_node.name,\n                    type=h_node.type,\n                    change_type=ChangeType.MODIFIED,\n                    details=\", \".join(changes),\n                    old_metadata=b_node.metadata,\n                    new_metadata=h_node.metadata\n                ))\n\n        # 4. Compare Edges\n        # Create a signature for edges: (source, target, type)\n        base_edges = {\n            (e.source_id, e.target_id, e.type) \n            for e in base_graph.iter_edges()\n        }\n        head_edges = {\n            (e.source_id, e.target_id, e.type) \n            for e in head_graph.iter_edges()\n        }\n\n        # Added Edges\n        for (src, tgt, rtype) in head_edges - base_edges:\n            report.edge_changes.append(EdgeChange(\n                source_id=src, target_id=tgt, type=rtype, change_type=ChangeType.ADDED\n            ))\n\n        # Removed Edges\n        for (src, tgt, rtype) in base_edges - head_edges:\n            report.edge_changes.append(EdgeChange(\n                source_id=src, target_id=tgt, type=rtype, change_type=ChangeType.REMOVED\n            ))\n\n        return report\n</code></pre>"},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer.DiffAnalyzer-functions","title":"Functions","text":""},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer.DiffAnalyzer.compare","title":"<code>compare(base_graph, head_graph)</code>","text":"<p>Compare base (original) vs head (new) graph.</p> Source code in <code>src/jnkn/analysis/diff_analyzer.py</code> <pre><code>def compare(self, base_graph: IGraph, head_graph: IGraph) -&gt; DiffReport:\n    \"\"\"\n    Compare base (original) vs head (new) graph.\n    \"\"\"\n    report = DiffReport()\n\n    # 1. Map Nodes for O(1) lookup\n    base_nodes = {n.id: n for n in base_graph.iter_nodes()}\n    head_nodes = {n.id: n for n in head_graph.iter_nodes()}\n\n    base_ids = set(base_nodes.keys())\n    head_ids = set(head_nodes.keys())\n\n    # 2. Detect Added/Removed Nodes\n    for nid in head_ids - base_ids:\n        node = head_nodes[nid]\n        report.added_nodes.append(node)\n        report.node_changes.append(NodeChange(\n            node_id=nid, name=node.name, type=node.type, change_type=ChangeType.ADDED\n        ))\n\n    for nid in base_ids - head_ids:\n        node = base_nodes[nid]\n        report.removed_nodes.append(node)\n        report.node_changes.append(NodeChange(\n            node_id=nid, name=node.name, type=node.type, change_type=ChangeType.REMOVED\n        ))\n\n    # 3. Detect Modified Nodes\n    for nid in base_ids.intersection(head_ids):\n        b_node = base_nodes[nid]\n        h_node = head_nodes[nid]\n\n        changes = []\n        if b_node.path != h_node.path:\n            changes.append(f\"Moved: {b_node.path} -&gt; {h_node.path}\")\n\n        if b_node.metadata != h_node.metadata:\n            # Robust metadata diff\n            b_keys = set(b_node.metadata.keys())\n            h_keys = set(h_node.metadata.keys())\n\n            added_keys = h_keys - b_keys\n            removed_keys = b_keys - h_keys\n\n            # Check for value changes in common keys\n            changed_keys = []\n            for k in b_keys.intersection(h_keys):\n                if b_node.metadata[k] != h_node.metadata[k]:\n                    changed_keys.append(k)\n\n            details = []\n            if added_keys: details.append(f\"Meta added: {added_keys}\")\n            if removed_keys: details.append(f\"Meta removed: {removed_keys}\")\n            if changed_keys: details.append(f\"Meta changed: {changed_keys}\")\n\n            changes.append(\"; \".join(details))\n\n        if changes:\n            report.modified_nodes.append(h_node)\n            report.node_changes.append(NodeChange(\n                node_id=nid,\n                name=h_node.name,\n                type=h_node.type,\n                change_type=ChangeType.MODIFIED,\n                details=\", \".join(changes),\n                old_metadata=b_node.metadata,\n                new_metadata=h_node.metadata\n            ))\n\n    # 4. Compare Edges\n    # Create a signature for edges: (source, target, type)\n    base_edges = {\n        (e.source_id, e.target_id, e.type) \n        for e in base_graph.iter_edges()\n    }\n    head_edges = {\n        (e.source_id, e.target_id, e.type) \n        for e in head_graph.iter_edges()\n    }\n\n    # Added Edges\n    for (src, tgt, rtype) in head_edges - base_edges:\n        report.edge_changes.append(EdgeChange(\n            source_id=src, target_id=tgt, type=rtype, change_type=ChangeType.ADDED\n        ))\n\n    # Removed Edges\n    for (src, tgt, rtype) in base_edges - head_edges:\n        report.edge_changes.append(EdgeChange(\n            source_id=src, target_id=tgt, type=rtype, change_type=ChangeType.REMOVED\n        ))\n\n    return report\n</code></pre>"},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer.DiffReport","title":"<code>DiffReport</code>  <code>dataclass</code>","text":"<p>Complete report of graph differences.</p> Source code in <code>src/jnkn/analysis/diff_analyzer.py</code> <pre><code>@dataclass\nclass DiffReport:\n    \"\"\"Complete report of graph differences.\"\"\"\n    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())\n\n    # Detailed changes\n    node_changes: List[NodeChange] = field(default_factory=list)\n    edge_changes: List[EdgeChange] = field(default_factory=list)\n\n    # Quick access lists\n    added_nodes: List[Node] = field(default_factory=list)\n    removed_nodes: List[Node] = field(default_factory=list)\n    modified_nodes: List[Node] = field(default_factory=list)\n\n    @property\n    def has_breaking_changes(self) -&gt; bool:\n        \"\"\"\n        Heuristic for breaking changes:\n        - Removing an Env Var that is read by code? (Edge removal)\n        - Removing an Infra resource?\n        \"\"\"\n        for nc in self.node_changes:\n            if nc.change_type == ChangeType.REMOVED:\n                # Removing config/infra is usually risky\n                if nc.type in [NodeType.ENV_VAR, NodeType.INFRA_RESOURCE, NodeType.CONFIG_KEY]:\n                    return True\n        return False\n\n    def to_markdown(self) -&gt; str:\n        \"\"\"Generate a human-readable markdown report.\"\"\"\n        lines = [\n            \"# \ud83d\udcca Graph Diff Report\",\n            f\"Generated: {self.timestamp}\",\n            \"\",\n            \"## Summary\",\n            f\"- **Nodes Changed:** {len(self.node_changes)}\",\n            f\"  - Added: {len(self.added_nodes)}\",\n            f\"  - Removed: {len(self.removed_nodes)}\",\n            f\"  - Modified: {len(self.modified_nodes)}\",\n            f\"- **Edges Changed:** {len(self.edge_changes)}\",\n            \"\"\n        ]\n\n        if self.has_breaking_changes:\n            lines.extend([\"### \u26a0\ufe0f POTENTIAL BREAKING CHANGES\", \"\"])\n\n        if self.node_changes:\n            lines.extend([\"## Node Changes\", \"\"])\n            # Group by type\n            by_type = {}\n            for nc in self.node_changes:\n                by_type.setdefault(nc.type.value, []).append(nc)\n\n            for ntype, changes in by_type.items():\n                lines.append(f\"### {ntype.title()}s\")\n                for c in changes:\n                    detail = f\" - {c.details}\" if c.details else \"\"\n                    lines.append(f\"- {str(c)}{detail}\")\n                lines.append(\"\")\n\n        if self.edge_changes:\n            lines.extend([\"## Dependency Changes\", \"\"])\n            for ec in self.edge_changes:\n                lines.append(f\"- {str(ec)}\")\n\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer.DiffReport-attributes","title":"Attributes","text":""},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer.DiffReport.has_breaking_changes","title":"<code>has_breaking_changes</code>  <code>property</code>","text":"<p>Heuristic for breaking changes: - Removing an Env Var that is read by code? (Edge removal) - Removing an Infra resource?</p>"},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer.DiffReport-functions","title":"Functions","text":""},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer.DiffReport.to_markdown","title":"<code>to_markdown()</code>","text":"<p>Generate a human-readable markdown report.</p> Source code in <code>src/jnkn/analysis/diff_analyzer.py</code> <pre><code>def to_markdown(self) -&gt; str:\n    \"\"\"Generate a human-readable markdown report.\"\"\"\n    lines = [\n        \"# \ud83d\udcca Graph Diff Report\",\n        f\"Generated: {self.timestamp}\",\n        \"\",\n        \"## Summary\",\n        f\"- **Nodes Changed:** {len(self.node_changes)}\",\n        f\"  - Added: {len(self.added_nodes)}\",\n        f\"  - Removed: {len(self.removed_nodes)}\",\n        f\"  - Modified: {len(self.modified_nodes)}\",\n        f\"- **Edges Changed:** {len(self.edge_changes)}\",\n        \"\"\n    ]\n\n    if self.has_breaking_changes:\n        lines.extend([\"### \u26a0\ufe0f POTENTIAL BREAKING CHANGES\", \"\"])\n\n    if self.node_changes:\n        lines.extend([\"## Node Changes\", \"\"])\n        # Group by type\n        by_type = {}\n        for nc in self.node_changes:\n            by_type.setdefault(nc.type.value, []).append(nc)\n\n        for ntype, changes in by_type.items():\n            lines.append(f\"### {ntype.title()}s\")\n            for c in changes:\n                detail = f\" - {c.details}\" if c.details else \"\"\n                lines.append(f\"- {str(c)}{detail}\")\n            lines.append(\"\")\n\n    if self.edge_changes:\n        lines.extend([\"## Dependency Changes\", \"\"])\n        for ec in self.edge_changes:\n            lines.append(f\"- {str(ec)}\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer.EdgeChange","title":"<code>EdgeChange</code>  <code>dataclass</code>","text":"<p>Represents a change to a dependency (edge).</p> Source code in <code>src/jnkn/analysis/diff_analyzer.py</code> <pre><code>@dataclass\nclass EdgeChange:\n    \"\"\"Represents a change to a dependency (edge).\"\"\"\n    source_id: str\n    target_id: str\n    type: RelationshipType\n    change_type: ChangeType\n\n    def __str__(self) -&gt; str:\n        arrow = \"--&gt;\"\n        if self.type in [RelationshipType.READS, RelationshipType.DEPENDS_ON]:\n            arrow = \"&lt;--\" # Visual cue for dependency vs flow\n\n        icon = \"\u2795\" if self.change_type == ChangeType.ADDED else \"\ud83d\uddd1\ufe0f\"\n        return f\"{icon} {self.source_id} {arrow} {self.target_id} [{self.type.value}]\"\n</code></pre>"},{"location":"reference/api/analysis/diff/#jnkn.analysis.diff_analyzer.NodeChange","title":"<code>NodeChange</code>  <code>dataclass</code>","text":"<p>Represents a change to a single node.</p> Source code in <code>src/jnkn/analysis/diff_analyzer.py</code> <pre><code>@dataclass\nclass NodeChange:\n    \"\"\"Represents a change to a single node.\"\"\"\n    node_id: str\n    name: str\n    type: NodeType\n    change_type: ChangeType\n    details: str = \"\"\n    old_metadata: Dict[str, Any] = field(default_factory=dict)\n    new_metadata: Dict[str, Any] = field(default_factory=dict)\n\n    def __str__(self) -&gt; str:\n        icon = {\n            ChangeType.ADDED: \"\u2795\",\n            ChangeType.REMOVED: \"\ud83d\uddd1\ufe0f\",\n            ChangeType.MODIFIED: \"\u270f\ufe0f\"\n        }.get(self.change_type, \"?\")\n        return f\"{icon} {self.type.value}: {self.name} ({self.change_type.value})\"\n</code></pre>"},{"location":"reference/api/analysis/explain/","title":"Explain","text":""},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain","title":"<code>jnkn.analysis.explain</code>","text":"<p>Match Explanation Generator for jnkn.</p> <p>This module provides detailed explanations of why matches were made, including all signals considered, their scores, and alternative matches that were rejected.</p> <p>Features: - Detailed breakdown of matching process - Shows all signals and their contributions - Lists alternative matches that were rejected - Human-readable formatted output</p>"},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain-classes","title":"Classes","text":""},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain.AlternativeMatch","title":"<code>AlternativeMatch</code>  <code>dataclass</code>","text":"<p>An alternative match that was considered but rejected.</p> Source code in <code>src/jnkn/analysis/explain.py</code> <pre><code>@dataclass\nclass AlternativeMatch:\n    \"\"\"An alternative match that was considered but rejected.\"\"\"\n    node_id: str\n    node_name: str\n    score: float\n    rejection_reason: str\n    matched_tokens: List[str] = field(default_factory=list)\n</code></pre>"},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain.ExplanationGenerator","title":"<code>ExplanationGenerator</code>","text":"<p>Generate detailed explanations for dependency matches.</p> Usage <p>generator = ExplanationGenerator(graph) explanation = generator.explain(\"env:PAYMENT_DB_HOST\", \"infra:payment_db_host\") print(generator.format(explanation))</p> Source code in <code>src/jnkn/analysis/explain.py</code> <pre><code>class ExplanationGenerator:\n    \"\"\"\n    Generate detailed explanations for dependency matches.\n\n    Usage:\n        generator = ExplanationGenerator(graph)\n        explanation = generator.explain(\"env:PAYMENT_DB_HOST\", \"infra:payment_db_host\")\n        print(generator.format(explanation))\n    \"\"\"\n\n    def __init__(\n        self,\n        graph: Optional[DependencyGraph] = None,\n        calculator: Optional[ConfidenceCalculator] = None,\n        min_confidence: float = 0.5,\n    ):\n        \"\"\"\n        Initialize the explanation generator.\n\n        Args:\n            graph: DependencyGraph instance for looking up nodes\n            calculator: ConfidenceCalculator for scoring matches\n            min_confidence: Minimum confidence threshold for matches\n        \"\"\"\n        self.graph = graph\n        self.calculator = calculator or create_default_calculator()\n        self.min_confidence = min_confidence\n\n    def explain(\n        self,\n        source_id: str,\n        target_id: str,\n        find_alternatives: bool = True,\n    ) -&gt; MatchExplanation:\n        \"\"\"\n        Generate a detailed explanation for a match.\n\n        Args:\n            source_id: Source node ID (e.g., \"env:PAYMENT_DB_HOST\")\n            target_id: Target node ID (e.g., \"infra:payment_db_host\")\n            find_alternatives: Whether to find alternative matches\n\n        Returns:\n            MatchExplanation with all details\n        \"\"\"\n        # Get source node info\n        source_info = self._get_node_info(source_id)\n        target_info = self._get_node_info(target_id)\n\n        # Calculate confidence\n        confidence_result = self.calculator.calculate(\n            source_name=source_info.name,\n            target_name=target_info.name,\n            source_tokens=source_info.tokens,\n            target_tokens=target_info.tokens,\n            source_node_id=source_id,\n            target_node_id=target_id,\n        )\n\n        # Check if edge exists\n        edge_exists = False\n        edge_metadata = {}\n        if self.graph:\n            edge = self.graph.get_edge(source_id, target_id)\n            if edge:\n                edge_exists = True\n                edge_metadata = edge.metadata or {}\n\n        # Find alternative matches\n        alternatives = []\n        if find_alternatives and self.graph:\n            alternatives = self._find_alternatives(source_info, target_id)\n\n        return MatchExplanation(\n            source=source_info,\n            target=target_info,\n            confidence_result=confidence_result,\n            alternatives=alternatives,\n            edge_exists=edge_exists,\n            edge_metadata=edge_metadata,\n        )\n\n    def explain_why_not(\n        self,\n        source_id: str,\n        target_id: str,\n    ) -&gt; str:\n        \"\"\"\n        Explain why a match was NOT made.\n\n        Useful for debugging missing connections.\n\n        Args:\n            source_id: Source node ID\n            target_id: Target node ID\n\n        Returns:\n            Human-readable explanation\n        \"\"\"\n        explanation = self.explain(source_id, target_id, find_alternatives=False)\n\n        lines = []\n        lines.append(\"=\" * 60)\n        lines.append(\"WHY NO MATCH?\")\n        lines.append(\"=\" * 60)\n        lines.append(\"\")\n\n        score = explanation.confidence_result.score\n\n        if score &lt; self.min_confidence:\n            lines.append(f\"X Score ({score:.2f}) is below threshold ({self.min_confidence:.2f})\")\n            lines.append(\"\")\n            lines.append(\"Details:\")\n            lines.append(f\"  Source: {source_id}\")\n            lines.append(f\"  Target: {target_id}\")\n            lines.append(f\"  Source tokens: {explanation.source.tokens}\")\n            lines.append(f\"  Target tokens: {explanation.target.tokens}\")\n            lines.append(\"\")\n\n            # Find common tokens\n            source_set = set(explanation.source.tokens)\n            target_set = set(explanation.target.tokens)\n            common = source_set &amp; target_set\n\n            if not common:\n                lines.append(\"  ! No overlapping tokens found\")\n            else:\n                lines.append(f\"  Common tokens: {list(common)}\")\n\n            # Show what would need to change\n            needed = self.min_confidence - score\n            lines.append(\"\")\n            lines.append(f\"  To reach threshold, need +{needed:.2f} confidence\")\n\n            if explanation.confidence_result.penalties:\n                lines.append(\"\")\n                lines.append(\"  Penalties applied:\")\n                for p in explanation.confidence_result.penalties:\n                    lines.append(f\"    - {p.get('penalty_type')}: x{p.get('multiplier', 1.0):.2f}\")\n\n        elif explanation.edge_exists:\n            lines.append(\"V Match DOES exist!\")\n            lines.append(f\"  Score: {score:.2f}\")\n\n        else:\n            lines.append(f\"? Score ({score:.2f}) is above threshold, but no edge found\")\n            lines.append(\"  This might indicate the stitcher hasn't run yet\")\n\n        lines.append(\"\")\n        lines.append(\"=\" * 60)\n\n        return \"\\n\".join(lines)\n\n    def format(self, explanation: MatchExplanation) -&gt; str:\n        \"\"\"\n        Format an explanation for CLI output.\n\n        Args:\n            explanation: MatchExplanation to format\n\n        Returns:\n            Formatted string for display\n        \"\"\"\n        lines = []\n\n        # Header\n        lines.append(\"=\" * 60)\n        lines.append(\"MATCH EXPLANATION\")\n        lines.append(\"=\" * 60)\n        lines.append(\"\")\n\n        # Source info\n        lines.append(f\"Source: {explanation.source.id}\")\n        lines.append(f\"  Type: {explanation.source.type}\")\n        lines.append(f\"  Tokens: {explanation.source.tokens}\")\n        if explanation.source.path:\n            loc = explanation.source.path\n            if explanation.source.line_number:\n                loc += f\":{explanation.source.line_number}\"\n            lines.append(f\"  Found in: {loc}\")\n        lines.append(\"\")\n\n        # Target info\n        lines.append(f\"Target: {explanation.target.id}\")\n        lines.append(f\"  Type: {explanation.target.type}\")\n        lines.append(f\"  Tokens: {explanation.target.tokens}\")\n        if explanation.target.path:\n            loc = explanation.target.path\n            if explanation.target.line_number:\n                loc += f\":{explanation.target.line_number}\"\n            lines.append(f\"  Found in: {loc}\")\n        lines.append(\"\")\n\n        # Confidence calculation\n        lines.append(\"-\" * 60)\n        lines.append(\"CONFIDENCE CALCULATION\")\n        lines.append(\"-\" * 60)\n        lines.append(\"\")\n\n        lines.append(\"Base signals:\")\n        if explanation.confidence_result.signals:\n            for signal in explanation.confidence_result.signals:\n                weight = signal.get(\"weight\", 0)\n                name = signal.get(\"signal\", \"unknown\")\n                details = signal.get(\"details\", \"\")\n                matched_tokens = signal.get(\"matched_tokens\", [])\n\n                if matched_tokens:\n                    lines.append(f\"  [+{weight:.2f}] {name}: {matched_tokens}\")\n                elif details:\n                    lines.append(f\"  [+{weight:.2f}] {name}\")\n                    lines.append(f\"         {details}\")\n                else:\n                    lines.append(f\"  [+{weight:.2f}] {name}\")\n        else:\n            lines.append(\"  (none matched)\")\n\n        lines.append(\"\")\n        lines.append(\"Penalties:\")\n        if explanation.confidence_result.penalties:\n            for penalty in explanation.confidence_result.penalties:\n                multiplier = penalty.get(\"multiplier\", 1.0)\n                name = penalty.get(\"penalty_type\", \"unknown\")\n                reason = penalty.get(\"reason\", \"\")\n                lines.append(f\"  [x{multiplier:.2f}] {name}\")\n                if reason:\n                    lines.append(f\"         {reason}\")\n        else:\n            lines.append(\"  None applied\")\n\n        lines.append(\"\")\n\n        # Final score\n        score = explanation.confidence_result.score\n        level = self._get_confidence_level(score)\n        lines.append(f\"Final confidence: {score:.2f} ({level})\")\n\n        # Edge status\n        if explanation.edge_exists:\n            lines.append(\"\")\n            lines.append(\"Edge Status: EXISTS in graph\")\n            if explanation.edge_metadata:\n                for key, value in explanation.edge_metadata.items():\n                    if key not in (\"rule\", \"matched_tokens\", \"explanation\"):\n                        lines.append(f\"  {key}: {value}\")\n        else:\n            lines.append(\"\")\n            if score &gt;= self.min_confidence:\n                lines.append(\"Edge Status: Would be created (above threshold)\")\n            else:\n                lines.append(\"Edge Status: Would be REJECTED (below threshold)\")\n\n        # Alternatives\n        if explanation.alternatives:\n            lines.append(\"\")\n            lines.append(\"-\" * 60)\n            lines.append(\"ALTERNATIVE MATCHES CONSIDERED\")\n            lines.append(\"-\" * 60)\n            lines.append(\"\")\n\n            for alt in sorted(explanation.alternatives, key=lambda x: -x.score):\n                lines.append(f\"{alt.node_id} -- Score: {alt.score:.2f} ({alt.rejection_reason})\")\n                if alt.matched_tokens:\n                    lines.append(f\"  Tokens: {alt.matched_tokens}\")\n\n        lines.append(\"\")\n        lines.append(\"=\" * 60)\n\n        return \"\\n\".join(lines)\n\n    def format_brief(self, explanation: MatchExplanation) -&gt; str:\n        \"\"\"\n        Format a brief, single-line explanation.\n\n        Args:\n            explanation: MatchExplanation to format\n\n        Returns:\n            Brief formatted string\n        \"\"\"\n        score = explanation.confidence_result.score\n        level = self._get_confidence_level(score)\n        status = \"EXISTS\" if explanation.edge_exists else \"would be created\" if score &gt;= self.min_confidence else \"rejected\"\n\n        return f\"{explanation.source.id} -&gt; {explanation.target.id}: {score:.2f} ({level}, {status})\"\n\n    def _get_node_info(self, node_id: str) -&gt; NodeInfo:\n        \"\"\"\n        Get node information from graph or infer from ID.\n\n        Args:\n            node_id: Node ID to look up\n\n        Returns:\n            NodeInfo with available details\n        \"\"\"\n        # Try to get from graph\n        if self.graph:\n            node = self.graph.get_node(node_id)\n            if node:\n                return NodeInfo(\n                    id=node.id,\n                    name=node.name,\n                    type=node.type.value,\n                    tokens=list(node.tokens),\n                    path=node.path,\n                    line_number=node.metadata.get(\"line\") if node.metadata else None,\n                    metadata=dict(node.metadata) if node.metadata else {},\n                )\n\n        # Infer from ID\n        name = self._extract_name_from_id(node_id)\n        node_type = self._infer_type_from_id(node_id)\n        tokens = self._tokenize(name)\n\n        return NodeInfo(\n            id=node_id,\n            name=name,\n            type=node_type,\n            tokens=tokens,\n        )\n\n    def _find_alternatives(\n        self,\n        source_info: NodeInfo,\n        actual_target_id: str,\n        max_alternatives: int = 5,\n    ) -&gt; List[AlternativeMatch]:\n        \"\"\"\n        Find alternative matches that were considered.\n\n        Args:\n            source_info: Source node info\n            actual_target_id: The target that was actually matched\n            max_alternatives: Maximum alternatives to return\n\n        Returns:\n            List of alternative matches\n        \"\"\"\n        if not self.graph:\n            return []\n\n        alternatives = []\n\n        # Determine what type of nodes to look at based on source type\n        if source_info.id.startswith(\"env:\"):\n            candidate_types = [NodeType.INFRA_RESOURCE]\n        elif source_info.id.startswith(\"infra:\"):\n            candidate_types = [NodeType.INFRA_RESOURCE, NodeType.ENV_VAR]\n        else:\n            candidate_types = [NodeType.INFRA_RESOURCE, NodeType.ENV_VAR, NodeType.DATA_ASSET]\n\n        # Get candidate nodes\n        candidates = []\n        for node_type in candidate_types:\n            candidates.extend(self.graph.get_nodes_by_type(node_type))\n\n        # Score each candidate\n        for candidate in candidates:\n            if candidate.id == actual_target_id:\n                continue\n\n            result = self.calculator.calculate(\n                source_name=source_info.name,\n                target_name=candidate.name,\n                source_tokens=source_info.tokens,\n                target_tokens=list(candidate.tokens),\n            )\n\n            # Only include if there's some signal\n            if result.score &gt; 0:\n                if result.score &lt; self.min_confidence:\n                    reason = f\"rejected: below threshold ({self.min_confidence})\"\n                else:\n                    reason = \"not selected: lower score than match\"\n\n                alternatives.append(AlternativeMatch(\n                    node_id=candidate.id,\n                    node_name=candidate.name,\n                    score=result.score,\n                    rejection_reason=reason,\n                    matched_tokens=result.matched_tokens,\n                ))\n\n        # Sort by score and limit\n        alternatives.sort(key=lambda x: -x.score)\n        return alternatives[:max_alternatives]\n\n    def _get_confidence_level(self, score: float) -&gt; str:\n        \"\"\"Get human-readable confidence level.\"\"\"\n        if score &gt;= 0.8:\n            return \"HIGH\"\n        elif score &gt;= 0.6:\n            return \"MEDIUM\"\n        elif score &gt;= 0.4:\n            return \"LOW\"\n        else:\n            return \"VERY LOW\"\n\n    @staticmethod\n    def _extract_name_from_id(node_id: str) -&gt; str:\n        \"\"\"Extract node name from ID.\"\"\"\n        # Handle common ID formats\n        # Check for :// first (e.g. file://path/to/file)\n        if \"://\" in node_id:\n            return node_id.split(\"://\", 1)[1]\n        # Then check for : (e.g. env:VAR)\n        if \":\" in node_id:\n            return node_id.split(\":\", 1)[1]\n        return node_id\n\n    @staticmethod\n    def _infer_type_from_id(node_id: str) -&gt; str:\n        \"\"\"Infer node type from ID prefix.\"\"\"\n        if node_id.startswith(\"env:\"):\n            return \"env_var\"\n        elif node_id.startswith(\"infra:\"):\n            return \"infra_resource\"\n        elif node_id.startswith(\"file://\"):\n            return \"code_file\"\n        elif node_id.startswith(\"entity:\"):\n            return \"code_entity\"\n        elif node_id.startswith(\"data:\"):\n            return \"data_asset\"\n        else:\n            return \"unknown\"\n\n    @staticmethod\n    def _tokenize(name: str) -&gt; List[str]:\n        \"\"\"Split name into tokens.\"\"\"\n        normalized = name.lower()\n        for sep in [\"_\", \".\", \"-\", \"/\", \":\"]:\n            normalized = normalized.replace(sep, \" \")\n        return [t.strip() for t in normalized.split() if t.strip()]\n</code></pre>"},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain.ExplanationGenerator-functions","title":"Functions","text":""},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain.ExplanationGenerator.__init__","title":"<code>__init__(graph=None, calculator=None, min_confidence=0.5)</code>","text":"<p>Initialize the explanation generator.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Optional[DependencyGraph]</code> <p>DependencyGraph instance for looking up nodes</p> <code>None</code> <code>calculator</code> <code>Optional[ConfidenceCalculator]</code> <p>ConfidenceCalculator for scoring matches</p> <code>None</code> <code>min_confidence</code> <code>float</code> <p>Minimum confidence threshold for matches</p> <code>0.5</code> Source code in <code>src/jnkn/analysis/explain.py</code> <pre><code>def __init__(\n    self,\n    graph: Optional[DependencyGraph] = None,\n    calculator: Optional[ConfidenceCalculator] = None,\n    min_confidence: float = 0.5,\n):\n    \"\"\"\n    Initialize the explanation generator.\n\n    Args:\n        graph: DependencyGraph instance for looking up nodes\n        calculator: ConfidenceCalculator for scoring matches\n        min_confidence: Minimum confidence threshold for matches\n    \"\"\"\n    self.graph = graph\n    self.calculator = calculator or create_default_calculator()\n    self.min_confidence = min_confidence\n</code></pre>"},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain.ExplanationGenerator.explain","title":"<code>explain(source_id, target_id, find_alternatives=True)</code>","text":"<p>Generate a detailed explanation for a match.</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>Source node ID (e.g., \"env:PAYMENT_DB_HOST\")</p> required <code>target_id</code> <code>str</code> <p>Target node ID (e.g., \"infra:payment_db_host\")</p> required <code>find_alternatives</code> <code>bool</code> <p>Whether to find alternative matches</p> <code>True</code> <p>Returns:</p> Type Description <code>MatchExplanation</code> <p>MatchExplanation with all details</p> Source code in <code>src/jnkn/analysis/explain.py</code> <pre><code>def explain(\n    self,\n    source_id: str,\n    target_id: str,\n    find_alternatives: bool = True,\n) -&gt; MatchExplanation:\n    \"\"\"\n    Generate a detailed explanation for a match.\n\n    Args:\n        source_id: Source node ID (e.g., \"env:PAYMENT_DB_HOST\")\n        target_id: Target node ID (e.g., \"infra:payment_db_host\")\n        find_alternatives: Whether to find alternative matches\n\n    Returns:\n        MatchExplanation with all details\n    \"\"\"\n    # Get source node info\n    source_info = self._get_node_info(source_id)\n    target_info = self._get_node_info(target_id)\n\n    # Calculate confidence\n    confidence_result = self.calculator.calculate(\n        source_name=source_info.name,\n        target_name=target_info.name,\n        source_tokens=source_info.tokens,\n        target_tokens=target_info.tokens,\n        source_node_id=source_id,\n        target_node_id=target_id,\n    )\n\n    # Check if edge exists\n    edge_exists = False\n    edge_metadata = {}\n    if self.graph:\n        edge = self.graph.get_edge(source_id, target_id)\n        if edge:\n            edge_exists = True\n            edge_metadata = edge.metadata or {}\n\n    # Find alternative matches\n    alternatives = []\n    if find_alternatives and self.graph:\n        alternatives = self._find_alternatives(source_info, target_id)\n\n    return MatchExplanation(\n        source=source_info,\n        target=target_info,\n        confidence_result=confidence_result,\n        alternatives=alternatives,\n        edge_exists=edge_exists,\n        edge_metadata=edge_metadata,\n    )\n</code></pre>"},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain.ExplanationGenerator.explain_why_not","title":"<code>explain_why_not(source_id, target_id)</code>","text":"<p>Explain why a match was NOT made.</p> <p>Useful for debugging missing connections.</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>Source node ID</p> required <code>target_id</code> <code>str</code> <p>Target node ID</p> required <p>Returns:</p> Type Description <code>str</code> <p>Human-readable explanation</p> Source code in <code>src/jnkn/analysis/explain.py</code> <pre><code>def explain_why_not(\n    self,\n    source_id: str,\n    target_id: str,\n) -&gt; str:\n    \"\"\"\n    Explain why a match was NOT made.\n\n    Useful for debugging missing connections.\n\n    Args:\n        source_id: Source node ID\n        target_id: Target node ID\n\n    Returns:\n        Human-readable explanation\n    \"\"\"\n    explanation = self.explain(source_id, target_id, find_alternatives=False)\n\n    lines = []\n    lines.append(\"=\" * 60)\n    lines.append(\"WHY NO MATCH?\")\n    lines.append(\"=\" * 60)\n    lines.append(\"\")\n\n    score = explanation.confidence_result.score\n\n    if score &lt; self.min_confidence:\n        lines.append(f\"X Score ({score:.2f}) is below threshold ({self.min_confidence:.2f})\")\n        lines.append(\"\")\n        lines.append(\"Details:\")\n        lines.append(f\"  Source: {source_id}\")\n        lines.append(f\"  Target: {target_id}\")\n        lines.append(f\"  Source tokens: {explanation.source.tokens}\")\n        lines.append(f\"  Target tokens: {explanation.target.tokens}\")\n        lines.append(\"\")\n\n        # Find common tokens\n        source_set = set(explanation.source.tokens)\n        target_set = set(explanation.target.tokens)\n        common = source_set &amp; target_set\n\n        if not common:\n            lines.append(\"  ! No overlapping tokens found\")\n        else:\n            lines.append(f\"  Common tokens: {list(common)}\")\n\n        # Show what would need to change\n        needed = self.min_confidence - score\n        lines.append(\"\")\n        lines.append(f\"  To reach threshold, need +{needed:.2f} confidence\")\n\n        if explanation.confidence_result.penalties:\n            lines.append(\"\")\n            lines.append(\"  Penalties applied:\")\n            for p in explanation.confidence_result.penalties:\n                lines.append(f\"    - {p.get('penalty_type')}: x{p.get('multiplier', 1.0):.2f}\")\n\n    elif explanation.edge_exists:\n        lines.append(\"V Match DOES exist!\")\n        lines.append(f\"  Score: {score:.2f}\")\n\n    else:\n        lines.append(f\"? Score ({score:.2f}) is above threshold, but no edge found\")\n        lines.append(\"  This might indicate the stitcher hasn't run yet\")\n\n    lines.append(\"\")\n    lines.append(\"=\" * 60)\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain.ExplanationGenerator.format","title":"<code>format(explanation)</code>","text":"<p>Format an explanation for CLI output.</p> <p>Parameters:</p> Name Type Description Default <code>explanation</code> <code>MatchExplanation</code> <p>MatchExplanation to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string for display</p> Source code in <code>src/jnkn/analysis/explain.py</code> <pre><code>def format(self, explanation: MatchExplanation) -&gt; str:\n    \"\"\"\n    Format an explanation for CLI output.\n\n    Args:\n        explanation: MatchExplanation to format\n\n    Returns:\n        Formatted string for display\n    \"\"\"\n    lines = []\n\n    # Header\n    lines.append(\"=\" * 60)\n    lines.append(\"MATCH EXPLANATION\")\n    lines.append(\"=\" * 60)\n    lines.append(\"\")\n\n    # Source info\n    lines.append(f\"Source: {explanation.source.id}\")\n    lines.append(f\"  Type: {explanation.source.type}\")\n    lines.append(f\"  Tokens: {explanation.source.tokens}\")\n    if explanation.source.path:\n        loc = explanation.source.path\n        if explanation.source.line_number:\n            loc += f\":{explanation.source.line_number}\"\n        lines.append(f\"  Found in: {loc}\")\n    lines.append(\"\")\n\n    # Target info\n    lines.append(f\"Target: {explanation.target.id}\")\n    lines.append(f\"  Type: {explanation.target.type}\")\n    lines.append(f\"  Tokens: {explanation.target.tokens}\")\n    if explanation.target.path:\n        loc = explanation.target.path\n        if explanation.target.line_number:\n            loc += f\":{explanation.target.line_number}\"\n        lines.append(f\"  Found in: {loc}\")\n    lines.append(\"\")\n\n    # Confidence calculation\n    lines.append(\"-\" * 60)\n    lines.append(\"CONFIDENCE CALCULATION\")\n    lines.append(\"-\" * 60)\n    lines.append(\"\")\n\n    lines.append(\"Base signals:\")\n    if explanation.confidence_result.signals:\n        for signal in explanation.confidence_result.signals:\n            weight = signal.get(\"weight\", 0)\n            name = signal.get(\"signal\", \"unknown\")\n            details = signal.get(\"details\", \"\")\n            matched_tokens = signal.get(\"matched_tokens\", [])\n\n            if matched_tokens:\n                lines.append(f\"  [+{weight:.2f}] {name}: {matched_tokens}\")\n            elif details:\n                lines.append(f\"  [+{weight:.2f}] {name}\")\n                lines.append(f\"         {details}\")\n            else:\n                lines.append(f\"  [+{weight:.2f}] {name}\")\n    else:\n        lines.append(\"  (none matched)\")\n\n    lines.append(\"\")\n    lines.append(\"Penalties:\")\n    if explanation.confidence_result.penalties:\n        for penalty in explanation.confidence_result.penalties:\n            multiplier = penalty.get(\"multiplier\", 1.0)\n            name = penalty.get(\"penalty_type\", \"unknown\")\n            reason = penalty.get(\"reason\", \"\")\n            lines.append(f\"  [x{multiplier:.2f}] {name}\")\n            if reason:\n                lines.append(f\"         {reason}\")\n    else:\n        lines.append(\"  None applied\")\n\n    lines.append(\"\")\n\n    # Final score\n    score = explanation.confidence_result.score\n    level = self._get_confidence_level(score)\n    lines.append(f\"Final confidence: {score:.2f} ({level})\")\n\n    # Edge status\n    if explanation.edge_exists:\n        lines.append(\"\")\n        lines.append(\"Edge Status: EXISTS in graph\")\n        if explanation.edge_metadata:\n            for key, value in explanation.edge_metadata.items():\n                if key not in (\"rule\", \"matched_tokens\", \"explanation\"):\n                    lines.append(f\"  {key}: {value}\")\n    else:\n        lines.append(\"\")\n        if score &gt;= self.min_confidence:\n            lines.append(\"Edge Status: Would be created (above threshold)\")\n        else:\n            lines.append(\"Edge Status: Would be REJECTED (below threshold)\")\n\n    # Alternatives\n    if explanation.alternatives:\n        lines.append(\"\")\n        lines.append(\"-\" * 60)\n        lines.append(\"ALTERNATIVE MATCHES CONSIDERED\")\n        lines.append(\"-\" * 60)\n        lines.append(\"\")\n\n        for alt in sorted(explanation.alternatives, key=lambda x: -x.score):\n            lines.append(f\"{alt.node_id} -- Score: {alt.score:.2f} ({alt.rejection_reason})\")\n            if alt.matched_tokens:\n                lines.append(f\"  Tokens: {alt.matched_tokens}\")\n\n    lines.append(\"\")\n    lines.append(\"=\" * 60)\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain.ExplanationGenerator.format_brief","title":"<code>format_brief(explanation)</code>","text":"<p>Format a brief, single-line explanation.</p> <p>Parameters:</p> Name Type Description Default <code>explanation</code> <code>MatchExplanation</code> <p>MatchExplanation to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>Brief formatted string</p> Source code in <code>src/jnkn/analysis/explain.py</code> <pre><code>def format_brief(self, explanation: MatchExplanation) -&gt; str:\n    \"\"\"\n    Format a brief, single-line explanation.\n\n    Args:\n        explanation: MatchExplanation to format\n\n    Returns:\n        Brief formatted string\n    \"\"\"\n    score = explanation.confidence_result.score\n    level = self._get_confidence_level(score)\n    status = \"EXISTS\" if explanation.edge_exists else \"would be created\" if score &gt;= self.min_confidence else \"rejected\"\n\n    return f\"{explanation.source.id} -&gt; {explanation.target.id}: {score:.2f} ({level}, {status})\"\n</code></pre>"},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain.MatchExplanation","title":"<code>MatchExplanation</code>  <code>dataclass</code>","text":"<p>Complete explanation of a match.</p> Source code in <code>src/jnkn/analysis/explain.py</code> <pre><code>@dataclass\nclass MatchExplanation:\n    \"\"\"Complete explanation of a match.\"\"\"\n    source: NodeInfo\n    target: NodeInfo\n    confidence_result: ConfidenceResult\n    alternatives: List[AlternativeMatch] = field(default_factory=list)\n    edge_exists: bool = False\n    edge_metadata: Dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain.NodeInfo","title":"<code>NodeInfo</code>  <code>dataclass</code>","text":"<p>Information about a node for explanation.</p> Source code in <code>src/jnkn/analysis/explain.py</code> <pre><code>@dataclass\nclass NodeInfo:\n    \"\"\"Information about a node for explanation.\"\"\"\n    id: str\n    name: str\n    type: str\n    tokens: List[str]\n    path: Optional[str] = None\n    line_number: Optional[int] = None\n    metadata: Dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain-functions","title":"Functions","text":""},{"location":"reference/api/analysis/explain/#jnkn.analysis.explain.create_explanation_generator","title":"<code>create_explanation_generator(graph=None, min_confidence=0.5)</code>","text":"<p>Factory function to create an ExplanationGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Optional[DependencyGraph]</code> <p>Optional DependencyGraph</p> <code>None</code> <code>min_confidence</code> <code>float</code> <p>Minimum confidence threshold</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ExplanationGenerator</code> <p>Configured ExplanationGenerator</p> Source code in <code>src/jnkn/analysis/explain.py</code> <pre><code>def create_explanation_generator(\n    graph: Optional[DependencyGraph] = None,\n    min_confidence: float = 0.5,\n) -&gt; ExplanationGenerator:\n    \"\"\"\n    Factory function to create an ExplanationGenerator.\n\n    Args:\n        graph: Optional DependencyGraph\n        min_confidence: Minimum confidence threshold\n\n    Returns:\n        Configured ExplanationGenerator\n    \"\"\"\n    return ExplanationGenerator(\n        graph=graph,\n        calculator=create_default_calculator(),\n        min_confidence=min_confidence,\n    )\n</code></pre>"},{"location":"reference/api/core/confidence/","title":"Confidence Engine","text":""},{"location":"reference/api/core/confidence/#jnkn.core.confidence","title":"<code>jnkn.core.confidence</code>","text":"<p>Confidence Calculation Engine for jnkn.</p> <p>This module provides a sophisticated confidence scoring system for dependency matches. It combines multiple signals (exact match, token overlap, suffix/prefix match, etc.) with configurable weights and penalty factors to produce explainable confidence scores.</p> <p>Key Features: - Multiple confidence signals with configurable weights - Penalty factors for short tokens, common tokens, and ambiguous matches - Human-readable explanations for all matches - Extensible architecture for custom signals</p> <p>Design Principles: - False positives are worse than false negatives - Every match must have an explainable confidence score - Penalties reduce confidence, never increase it</p>"},{"location":"reference/api/core/confidence/#jnkn.core.confidence-classes","title":"Classes","text":""},{"location":"reference/api/core/confidence/#jnkn.core.confidence.ConfidenceCalculator","title":"<code>ConfidenceCalculator</code>","text":"<p>Calculate confidence scores for dependency matches.</p> Source code in <code>src/jnkn/core/confidence.py</code> <pre><code>class ConfidenceCalculator:\n    \"\"\"\n    Calculate confidence scores for dependency matches.\n    \"\"\"\n\n    def __init__(self, config: Optional[ConfidenceConfig] = None):\n        self.config = config or ConfidenceConfig()\n\n    def calculate(\n        self,\n        source_name: str,\n        target_name: str,\n        source_tokens: List[str],\n        target_tokens: List[str],\n        matched_tokens: Optional[List[str]] = None,\n        alternative_match_count: int = 0,\n        source_node_id: str = \"\",\n        target_node_id: str = \"\",\n    ) -&gt; ConfidenceResult:\n        \"\"\"\n        Calculate confidence score for a match between source and target.\n        \"\"\"\n        # Calculate matched tokens if not provided\n        if matched_tokens is None:\n            source_set = set(source_tokens)\n            target_set = set(target_tokens)\n            matched_tokens = list(source_set &amp; target_set)\n\n        # Evaluate all signals\n        signal_results = self._evaluate_signals(\n            source_name, target_name,\n            source_tokens, target_tokens,\n            matched_tokens\n        )\n\n        # Evaluate penalties\n        penalty_results = self._evaluate_penalties(\n            matched_tokens, alternative_match_count\n        )\n\n        # Calculate base score from signals\n        base_score = self._calculate_base_score(signal_results)\n\n        # Apply penalties\n        final_score = self._apply_penalties(base_score, penalty_results)\n\n        # Build explanation\n        explanation = self._build_explanation(\n            source_name, target_name,\n            signal_results, penalty_results,\n            base_score, final_score\n        )\n\n        return ConfidenceResult(\n            score=final_score,\n            signals=[self._signal_to_dict(s) for s in signal_results if s.matched],\n            penalties=[self._penalty_to_dict(p) for p in penalty_results if p.multiplier &lt; 1.0],\n            explanation=explanation,\n            matched_tokens=matched_tokens,\n            source_node_id=source_node_id,\n            target_node_id=target_node_id,\n        )\n\n    def _evaluate_signals(\n        self,\n        source_name: str,\n        target_name: str,\n        source_tokens: List[str],\n        target_tokens: List[str],\n        matched_tokens: List[str],\n    ) -&gt; List[SignalResult]:\n        \"\"\"Evaluate all confidence signals.\"\"\"\n        results = []\n\n        # Signal 1: Exact match\n        exact_match = source_name == target_name\n        results.append(SignalResult(\n            signal=ConfidenceSignal.EXACT_MATCH,\n            weight=self.config.signal_weights[ConfidenceSignal.EXACT_MATCH],\n            matched=exact_match,\n            details=f\"'{source_name}' == '{target_name}'\" if exact_match else \"\",\n        ))\n\n        # Signal 2: Normalized match\n        source_normalized = self._normalize(source_name)\n        target_normalized = self._normalize(target_name)\n        normalized_match = source_normalized == target_normalized\n        results.append(SignalResult(\n            signal=ConfidenceSignal.NORMALIZED_MATCH,\n            weight=self.config.signal_weights[ConfidenceSignal.NORMALIZED_MATCH],\n            matched=normalized_match and not exact_match,  # Don't double count\n            details=f\"'{source_normalized}' == '{target_normalized}'\" if normalized_match else \"\",\n        ))\n\n        # Signal 3 &amp; 4: Token overlap (high vs medium)\n        overlap_count = len(matched_tokens)\n        significant_overlap = [t for t in matched_tokens\n                             if t not in self.config.common_tokens\n                             and len(t) &gt;= self.config.short_token_length]\n\n        high_overlap = len(significant_overlap) &gt;= self.config.min_token_overlap_high\n        medium_overlap = len(significant_overlap) &gt;= self.config.min_token_overlap_medium\n\n        results.append(SignalResult(\n            signal=ConfidenceSignal.TOKEN_OVERLAP_HIGH,\n            weight=self.config.signal_weights[ConfidenceSignal.TOKEN_OVERLAP_HIGH],\n            matched=high_overlap,\n            details=f\"{len(significant_overlap)} significant tokens: {significant_overlap}\" if high_overlap else \"\",\n            matched_tokens=significant_overlap if high_overlap else [],\n        ))\n\n        results.append(SignalResult(\n            signal=ConfidenceSignal.TOKEN_OVERLAP_MEDIUM,\n            weight=self.config.signal_weights[ConfidenceSignal.TOKEN_OVERLAP_MEDIUM],\n            matched=medium_overlap and not high_overlap,  # Don't double count\n            details=f\"{len(significant_overlap)} significant tokens: {significant_overlap}\" if medium_overlap and not high_overlap else \"\",\n            matched_tokens=significant_overlap if medium_overlap and not high_overlap else [],\n        ))\n\n        # Signal 5: Suffix match\n        suffix_match = (target_normalized.endswith(source_normalized) and\n                       len(source_normalized) &gt;= 4 and\n                       not normalized_match)\n        results.append(SignalResult(\n            signal=ConfidenceSignal.SUFFIX_MATCH,\n            weight=self.config.signal_weights[ConfidenceSignal.SUFFIX_MATCH],\n            matched=suffix_match,\n            details=f\"'{target_normalized}' ends with '{source_normalized}'\" if suffix_match else \"\",\n        ))\n\n        # Signal 6: Prefix match\n        prefix_match = (target_normalized.startswith(source_normalized) and\n                       len(source_normalized) &gt;= 4 and\n                       not normalized_match)\n        results.append(SignalResult(\n            signal=ConfidenceSignal.PREFIX_MATCH,\n            weight=self.config.signal_weights[ConfidenceSignal.PREFIX_MATCH],\n            matched=prefix_match,\n            details=f\"'{target_normalized}' starts with '{source_normalized}'\" if prefix_match else \"\",\n        ))\n\n        # Signal 7: Contains\n        contains_match = (source_normalized in target_normalized and\n                        len(source_normalized) &gt;= 4 and\n                        not normalized_match and\n                        not suffix_match and\n                        not prefix_match)\n        results.append(SignalResult(\n            signal=ConfidenceSignal.CONTAINS,\n            weight=self.config.signal_weights[ConfidenceSignal.CONTAINS],\n            matched=contains_match,\n            details=f\"'{target_normalized}' contains '{source_normalized}'\" if contains_match else \"\",\n        ))\n\n        # Signal 8: Single token (weak signal, used when no other match)\n        single_token = (overlap_count == 1 and\n                       not any(r.matched for r in results))\n        results.append(SignalResult(\n            signal=ConfidenceSignal.SINGLE_TOKEN,\n            weight=self.config.signal_weights[ConfidenceSignal.SINGLE_TOKEN],\n            matched=single_token,\n            details=f\"Single token match: {matched_tokens}\" if single_token else \"\",\n            matched_tokens=matched_tokens if single_token else [],\n        ))\n\n        return results\n\n    def _evaluate_penalties(\n        self,\n        matched_tokens: List[str],\n        alternative_match_count: int,\n    ) -&gt; List[PenaltyResult]:\n        \"\"\"Evaluate all penalty factors.\"\"\"\n        results = []\n\n        # Penalty 1: Short tokens\n        short_tokens = [t for t in matched_tokens\n                       if len(t) &lt; self.config.short_token_length]\n        if short_tokens:\n            results.append(PenaltyResult(\n                penalty_type=PenaltyType.SHORT_TOKEN,\n                multiplier=self.config.penalty_multipliers[PenaltyType.SHORT_TOKEN],\n                reason=f\"Short tokens (&lt; {self.config.short_token_length} chars): {short_tokens}\",\n                affected_tokens=short_tokens,\n            ))\n        else:\n            results.append(PenaltyResult(\n                penalty_type=PenaltyType.SHORT_TOKEN,\n                multiplier=1.0,\n                reason=\"No short tokens\",\n            ))\n\n        # Penalty 2: Common tokens\n        common_found = [t for t in matched_tokens if t in self.config.common_tokens]\n        non_common_found = [t for t in matched_tokens if t not in self.config.common_tokens]\n\n        # Only apply penalty if ALL matched tokens are common\n        if common_found and not non_common_found:\n            results.append(PenaltyResult(\n                penalty_type=PenaltyType.COMMON_TOKEN,\n                multiplier=self.config.penalty_multipliers[PenaltyType.COMMON_TOKEN],\n                reason=f\"All matched tokens are common: {common_found}\",\n                affected_tokens=common_found,\n            ))\n        else:\n            results.append(PenaltyResult(\n                penalty_type=PenaltyType.COMMON_TOKEN,\n                multiplier=1.0,\n                reason=\"Has non-common tokens\" if non_common_found else \"No common tokens\",\n            ))\n\n        # Penalty 3: Ambiguity (multiple potential matches)\n        if alternative_match_count &gt; 2:\n            # Stronger penalty for more alternatives\n            penalty = self.config.penalty_multipliers[PenaltyType.AMBIGUITY]\n            penalty = penalty ** (1 + (alternative_match_count - 2) * 0.2)  # Compound penalty\n            results.append(PenaltyResult(\n                penalty_type=PenaltyType.AMBIGUITY,\n                multiplier=max(0.3, penalty),  # Floor at 0.3\n                reason=f\"Source has {alternative_match_count} potential matches\",\n            ))\n        else:\n            results.append(PenaltyResult(\n                penalty_type=PenaltyType.AMBIGUITY,\n                multiplier=1.0,\n                reason=\"Low ambiguity\" if alternative_match_count &lt;= 1 else \"Acceptable ambiguity\",\n            ))\n\n        # Penalty 4: Low-value tokens\n        low_value_found = [t for t in matched_tokens if t in self.config.low_value_tokens]\n        high_value_found = [t for t in matched_tokens\n                           if t not in self.config.low_value_tokens\n                           and t not in self.config.common_tokens]\n\n        # Only apply if majority of tokens are low-value\n        if low_value_found and len(low_value_found) &gt; len(high_value_found):\n            results.append(PenaltyResult(\n                penalty_type=PenaltyType.LOW_VALUE_TOKEN,\n                multiplier=self.config.penalty_multipliers[PenaltyType.LOW_VALUE_TOKEN],\n                reason=f\"Mostly low-value tokens: {low_value_found}\",\n                affected_tokens=low_value_found,\n            ))\n        else:\n            results.append(PenaltyResult(\n                penalty_type=PenaltyType.LOW_VALUE_TOKEN,\n                multiplier=1.0,\n                reason=\"Has high-value tokens\" if high_value_found else \"No low-value tokens\",\n            ))\n\n        return results\n\n    def _calculate_base_score(self, signal_results: List[SignalResult]) -&gt; float:\n        \"\"\"\n        Calculate base score from signal results.\n\n        Uses the maximum weight among matched signals, not sum,\n        to avoid inflated scores from multiple weak signals.\n        \"\"\"\n        matched_weights = [s.weight for s in signal_results if s.matched]\n        if not matched_weights:\n            return 0.0\n\n        # Use max weight as base, with small bonus for additional signals\n        max_weight = max(matched_weights)\n        additional_signals = len(matched_weights) - 1\n        bonus = min(0.1, additional_signals * 0.02)  # Cap bonus at 0.1\n\n        return min(1.0, max_weight + bonus)\n\n    def _apply_penalties(\n        self,\n        base_score: float,\n        penalty_results: List[PenaltyResult],\n    ) -&gt; float:\n        \"\"\"Apply penalty multipliers to base score.\"\"\"\n        score = base_score\n        for penalty in penalty_results:\n            score *= penalty.multiplier\n        return round(score, 4)\n\n    def _build_explanation(\n        self,\n        source_name: str,\n        target_name: str,\n        signal_results: List[SignalResult],\n        penalty_results: List[PenaltyResult],\n        base_score: float,\n        final_score: float,\n    ) -&gt; str:\n        \"\"\"Build human-readable explanation of the confidence calculation.\"\"\"\n        lines = []\n\n        # Header\n        lines.append(f\"Match: {source_name} \u2192 {target_name}\")\n        lines.append(f\"Confidence: {final_score:.2f}\")\n        lines.append(\"\")\n        lines.append(\"Signals:\")\n\n        # Matched signals\n        matched_signals = [s for s in signal_results if s.matched]\n        if matched_signals:\n            for signal in matched_signals:\n                lines.append(f\"  \u2713 {signal.signal.value} ({signal.weight:.2f})\")\n                if signal.details:\n                    lines.append(f\"    \u2192 {signal.details}\")\n        else:\n            lines.append(\"  (none)\")\n\n        # Penalties\n        applied_penalties = [p for p in penalty_results if p.multiplier &lt; 1.0]\n        if applied_penalties:\n            lines.append(\"\")\n            lines.append(\"Penalties:\")\n            for penalty in applied_penalties:\n                lines.append(f\"  - {penalty.penalty_type.value} (\u00d7{penalty.multiplier:.2f})\")\n                if penalty.reason:\n                    lines.append(f\"    \u2192 {penalty.reason}\")\n        else:\n            lines.append(\"\")\n            lines.append(\"Penalties: None\")\n\n        return \"\\n\".join(lines)\n\n    def explain(self, result: ConfidenceResult) -&gt; str:\n        \"\"\"\n        Generate a detailed, formatted explanation of a confidence result.\n        This is a richer version of the explanation included in the result,\n        suitable for CLI output.\n        \"\"\"\n        lines = []\n\n        lines.append(\"\u2550\" * 60)\n        lines.append(\"MATCH EXPLANATION\")\n        lines.append(\"\u2550\" * 60)\n        lines.append(\"\")\n\n        if result.source_node_id:\n            lines.append(f\"Source: {result.source_node_id}\")\n        if result.target_node_id:\n            lines.append(f\"Target: {result.target_node_id}\")\n        if result.matched_tokens:\n            lines.append(f\"Matched Tokens: {result.matched_tokens}\")\n\n        lines.append(\"\")\n        lines.append(\"\u2500\" * 60)\n        lines.append(\"CONFIDENCE CALCULATION\")\n        lines.append(\"\u2500\" * 60)\n        lines.append(\"\")\n\n        lines.append(\"Base signals:\")\n        if result.signals:\n            for signal in result.signals:\n                weight = signal.get(\"weight\", 0)\n                name = signal.get(\"signal\", \"unknown\")\n                details = signal.get(\"details\", \"\")\n                matched_tokens = signal.get(\"matched_tokens\", [])\n\n                if matched_tokens:\n                    lines.append(f\"  [+{weight:.2f}] {name}: {matched_tokens}\")\n                elif details:\n                    lines.append(f\"  [+{weight:.2f}] {name}\")\n                    lines.append(f\"         {details}\")\n                else:\n                    lines.append(f\"  [+{weight:.2f}] {name}\")\n        else:\n            lines.append(\"  (none)\")\n\n        lines.append(\"\")\n        lines.append(\"Penalties:\")\n        if result.penalties:\n            for penalty in result.penalties:\n                multiplier = penalty.get(\"multiplier\", 1.0)\n                name = penalty.get(\"penalty_type\", \"unknown\")\n                reason = penalty.get(\"reason\", \"\")\n                lines.append(f\"  [\u00d7{multiplier:.2f}] {name}\")\n                if reason:\n                    lines.append(f\"         {reason}\")\n        else:\n            lines.append(\"  None applied\")\n\n        lines.append(\"\")\n        confidence_level = self._get_confidence_level(result.score)\n        lines.append(f\"Final confidence: {result.score:.2f} ({confidence_level})\")\n        lines.append(\"\")\n        lines.append(\"\u2550\" * 60)\n\n        return \"\\n\".join(lines)\n\n    def _get_confidence_level(self, score: float) -&gt; str:\n        \"\"\"Get human-readable confidence level.\"\"\"\n        if score &gt;= 0.8:\n            return \"HIGH\"\n        elif score &gt;= 0.6:\n            return \"MEDIUM\"\n        elif score &gt;= 0.4:\n            return \"LOW\"\n        else:\n            return \"VERY LOW\"\n\n    @staticmethod\n    def _normalize(name: str) -&gt; str:\n        \"\"\"Normalize a name by lowercasing and removing separators.\"\"\"\n        result = name.lower()\n        for sep in [\"_\", \".\", \"-\", \"/\", \":\"]:\n            result = result.replace(sep, \"\")\n        return result\n\n    @staticmethod\n    def _signal_to_dict(signal: SignalResult) -&gt; Dict:\n        \"\"\"Convert SignalResult to dictionary.\"\"\"\n        return {\n            \"signal\": signal.signal.value,\n            \"weight\": signal.weight,\n            \"matched\": signal.matched,\n            \"details\": signal.details,\n            \"matched_tokens\": signal.matched_tokens,\n        }\n\n    @staticmethod\n    def _penalty_to_dict(penalty: PenaltyResult) -&gt; Dict:\n        \"\"\"Convert PenaltyResult to dictionary.\"\"\"\n        return {\n            \"penalty_type\": penalty.penalty_type.value,\n            \"multiplier\": penalty.multiplier,\n            \"reason\": penalty.reason,\n            \"affected_tokens\": penalty.affected_tokens,\n        }\n</code></pre>"},{"location":"reference/api/core/confidence/#jnkn.core.confidence.ConfidenceCalculator-functions","title":"Functions","text":""},{"location":"reference/api/core/confidence/#jnkn.core.confidence.ConfidenceCalculator.calculate","title":"<code>calculate(source_name, target_name, source_tokens, target_tokens, matched_tokens=None, alternative_match_count=0, source_node_id='', target_node_id='')</code>","text":"<p>Calculate confidence score for a match between source and target.</p> Source code in <code>src/jnkn/core/confidence.py</code> <pre><code>def calculate(\n    self,\n    source_name: str,\n    target_name: str,\n    source_tokens: List[str],\n    target_tokens: List[str],\n    matched_tokens: Optional[List[str]] = None,\n    alternative_match_count: int = 0,\n    source_node_id: str = \"\",\n    target_node_id: str = \"\",\n) -&gt; ConfidenceResult:\n    \"\"\"\n    Calculate confidence score for a match between source and target.\n    \"\"\"\n    # Calculate matched tokens if not provided\n    if matched_tokens is None:\n        source_set = set(source_tokens)\n        target_set = set(target_tokens)\n        matched_tokens = list(source_set &amp; target_set)\n\n    # Evaluate all signals\n    signal_results = self._evaluate_signals(\n        source_name, target_name,\n        source_tokens, target_tokens,\n        matched_tokens\n    )\n\n    # Evaluate penalties\n    penalty_results = self._evaluate_penalties(\n        matched_tokens, alternative_match_count\n    )\n\n    # Calculate base score from signals\n    base_score = self._calculate_base_score(signal_results)\n\n    # Apply penalties\n    final_score = self._apply_penalties(base_score, penalty_results)\n\n    # Build explanation\n    explanation = self._build_explanation(\n        source_name, target_name,\n        signal_results, penalty_results,\n        base_score, final_score\n    )\n\n    return ConfidenceResult(\n        score=final_score,\n        signals=[self._signal_to_dict(s) for s in signal_results if s.matched],\n        penalties=[self._penalty_to_dict(p) for p in penalty_results if p.multiplier &lt; 1.0],\n        explanation=explanation,\n        matched_tokens=matched_tokens,\n        source_node_id=source_node_id,\n        target_node_id=target_node_id,\n    )\n</code></pre>"},{"location":"reference/api/core/confidence/#jnkn.core.confidence.ConfidenceCalculator.explain","title":"<code>explain(result)</code>","text":"<p>Generate a detailed, formatted explanation of a confidence result. This is a richer version of the explanation included in the result, suitable for CLI output.</p> Source code in <code>src/jnkn/core/confidence.py</code> <pre><code>def explain(self, result: ConfidenceResult) -&gt; str:\n    \"\"\"\n    Generate a detailed, formatted explanation of a confidence result.\n    This is a richer version of the explanation included in the result,\n    suitable for CLI output.\n    \"\"\"\n    lines = []\n\n    lines.append(\"\u2550\" * 60)\n    lines.append(\"MATCH EXPLANATION\")\n    lines.append(\"\u2550\" * 60)\n    lines.append(\"\")\n\n    if result.source_node_id:\n        lines.append(f\"Source: {result.source_node_id}\")\n    if result.target_node_id:\n        lines.append(f\"Target: {result.target_node_id}\")\n    if result.matched_tokens:\n        lines.append(f\"Matched Tokens: {result.matched_tokens}\")\n\n    lines.append(\"\")\n    lines.append(\"\u2500\" * 60)\n    lines.append(\"CONFIDENCE CALCULATION\")\n    lines.append(\"\u2500\" * 60)\n    lines.append(\"\")\n\n    lines.append(\"Base signals:\")\n    if result.signals:\n        for signal in result.signals:\n            weight = signal.get(\"weight\", 0)\n            name = signal.get(\"signal\", \"unknown\")\n            details = signal.get(\"details\", \"\")\n            matched_tokens = signal.get(\"matched_tokens\", [])\n\n            if matched_tokens:\n                lines.append(f\"  [+{weight:.2f}] {name}: {matched_tokens}\")\n            elif details:\n                lines.append(f\"  [+{weight:.2f}] {name}\")\n                lines.append(f\"         {details}\")\n            else:\n                lines.append(f\"  [+{weight:.2f}] {name}\")\n    else:\n        lines.append(\"  (none)\")\n\n    lines.append(\"\")\n    lines.append(\"Penalties:\")\n    if result.penalties:\n        for penalty in result.penalties:\n            multiplier = penalty.get(\"multiplier\", 1.0)\n            name = penalty.get(\"penalty_type\", \"unknown\")\n            reason = penalty.get(\"reason\", \"\")\n            lines.append(f\"  [\u00d7{multiplier:.2f}] {name}\")\n            if reason:\n                lines.append(f\"         {reason}\")\n    else:\n        lines.append(\"  None applied\")\n\n    lines.append(\"\")\n    confidence_level = self._get_confidence_level(result.score)\n    lines.append(f\"Final confidence: {result.score:.2f} ({confidence_level})\")\n    lines.append(\"\")\n    lines.append(\"\u2550\" * 60)\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/api/core/confidence/#jnkn.core.confidence.ConfidenceConfig","title":"<code>ConfidenceConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for confidence calculation.</p> Source code in <code>src/jnkn/core/confidence.py</code> <pre><code>class ConfidenceConfig(BaseModel):\n    \"\"\"\n    Configuration for confidence calculation.\n    \"\"\"\n    signal_weights: Dict[str, float] = Field(default_factory=lambda: {\n        ConfidenceSignal.EXACT_MATCH: 1.0,\n        ConfidenceSignal.NORMALIZED_MATCH: 0.9,\n        ConfidenceSignal.TOKEN_OVERLAP_HIGH: 0.8,\n        ConfidenceSignal.TOKEN_OVERLAP_MEDIUM: 0.6,\n        ConfidenceSignal.SUFFIX_MATCH: 0.7,\n        ConfidenceSignal.PREFIX_MATCH: 0.7,\n        ConfidenceSignal.CONTAINS: 0.4,\n        ConfidenceSignal.SINGLE_TOKEN: 0.2,\n    })\n\n    penalty_multipliers: Dict[str, float] = Field(default_factory=lambda: {\n        PenaltyType.SHORT_TOKEN: 0.5,\n        PenaltyType.COMMON_TOKEN: 0.7,\n        PenaltyType.AMBIGUITY: 0.8,\n        PenaltyType.LOW_VALUE_TOKEN: 0.6,\n    })\n\n    short_token_length: int = 4\n    min_token_overlap_high: int = 3\n    min_token_overlap_medium: int = 2\n\n    common_tokens: Set[str] = Field(default_factory=lambda: {\n        \"id\", \"db\", \"host\", \"url\", \"key\", \"name\", \"type\", \"data\",\n        \"info\", \"temp\", \"test\", \"api\", \"app\", \"env\", \"var\", \"val\",\n        \"config\", \"setting\", \"path\", \"port\", \"user\", \"password\",\n        \"secret\", \"token\", \"auth\", \"log\", \"file\", \"dir\", \"src\",\n        \"dst\", \"in\", \"out\", \"err\", \"msg\", \"str\", \"int\", \"num\",\n    })\n\n    low_value_tokens: Set[str] = Field(default_factory=lambda: {\n        \"aws\", \"gcp\", \"azure\", \"main\", \"default\", \"primary\",\n        \"production\", \"prod\", \"staging\", \"dev\", \"development\",\n        \"internal\", \"external\", \"public\", \"private\", \"local\",\n        \"remote\", \"master\", \"slave\", \"read\", \"write\",\n    })\n\n    model_config = ConfigDict(frozen=False)\n</code></pre>"},{"location":"reference/api/core/confidence/#jnkn.core.confidence.ConfidenceResult","title":"<code>ConfidenceResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete result of confidence calculation.</p> Source code in <code>src/jnkn/core/confidence.py</code> <pre><code>class ConfidenceResult(BaseModel):\n    \"\"\"\n    Complete result of confidence calculation.\n    \"\"\"\n    score: float = Field(ge=0.0, le=1.0)\n    signals: List[Dict] = Field(default_factory=list)\n    penalties: List[Dict] = Field(default_factory=list)\n    explanation: str = \"\"\n    matched_tokens: List[str] = Field(default_factory=list)\n    source_node_id: str = \"\"\n    target_node_id: str = \"\"\n\n    model_config = ConfigDict(frozen=False)\n</code></pre>"},{"location":"reference/api/core/confidence/#jnkn.core.confidence-functions","title":"Functions","text":""},{"location":"reference/api/core/confidence/#jnkn.core.confidence.create_default_calculator","title":"<code>create_default_calculator()</code>","text":"<p>Create a ConfidenceCalculator with default configuration.</p> Source code in <code>src/jnkn/core/confidence.py</code> <pre><code>def create_default_calculator() -&gt; ConfidenceCalculator:\n    \"\"\"Create a ConfidenceCalculator with default configuration.\"\"\"\n    return ConfidenceCalculator()\n</code></pre>"},{"location":"reference/api/core/graph/","title":"Dependency Graph","text":""},{"location":"reference/api/core/graph/#jnkn.core.graph","title":"<code>jnkn.core.graph</code>","text":"<p>Graph Implementation backed by rustworkx.</p>"},{"location":"reference/api/core/graph/#jnkn.core.graph-classes","title":"Classes","text":""},{"location":"reference/api/core/graph/#jnkn.core.graph.DependencyGraph","title":"<code>DependencyGraph</code>","text":"<p>               Bases: <code>IGraph</code></p> <p>High-performance in-memory dependency graph using rustworkx.</p> Source code in <code>src/jnkn/core/graph.py</code> <pre><code>class DependencyGraph(IGraph):\n    \"\"\"\n    High-performance in-memory dependency graph using rustworkx.\n    \"\"\"\n\n    def __init__(self):\n        self._graph = rx.PyDiGraph()\n        self._id_to_idx: Dict[str, int] = {}\n        self._idx_to_node: Dict[int, Node] = {}\n        self.token_index = TokenIndex()\n\n    @property\n    def node_count(self) -&gt; int:\n        return self._graph.num_nodes()\n\n    @property\n    def edge_count(self) -&gt; int:\n        return self._graph.num_edges()\n\n    def add_node(self, node: Node) -&gt; None:\n        if node.id in self._id_to_idx:\n            idx = self._id_to_idx[node.id]\n            old_node = self._idx_to_node[idx]\n            self.token_index.remove_node(old_node)\n            self.token_index.index_node(node)\n            self._idx_to_node[idx] = node\n            self._graph[idx] = node\n        else:\n            idx = self._graph.add_node(node)\n            self._id_to_idx[node.id] = idx\n            self._idx_to_node[idx] = node\n            self.token_index.index_node(node)\n\n    def add_edge(self, edge: Edge) -&gt; None:\n        if edge.source_id not in self._id_to_idx or edge.target_id not in self._id_to_idx:\n            return\n\n        src_idx = self._id_to_idx[edge.source_id]\n        tgt_idx = self._id_to_idx[edge.target_id]\n\n        # Avoid duplicate edges of same type\n        existing = False\n        try:\n            edges = self._graph.get_all_edge_data(src_idx, tgt_idx)\n            for e in edges:\n                if e.type == edge.type:\n                    existing = True\n                    break\n        except rx.NoEdgeBetweenNodes:\n            pass\n\n        if not existing:\n            self._graph.add_edge(src_idx, tgt_idx, edge)\n\n    def get_node(self, node_id: str) -&gt; Optional[Node]:\n        idx = self._id_to_idx.get(node_id)\n        if idx is not None:\n            return self._idx_to_node[idx]\n        return None\n\n    def has_node(self, node_id: str) -&gt; bool:\n        return node_id in self._id_to_idx\n\n    def has_edge(self, source_id: str, target_id: str) -&gt; bool:\n        if source_id not in self._id_to_idx or target_id not in self._id_to_idx:\n            return False\n        src_idx = self._id_to_idx[source_id]\n        tgt_idx = self._id_to_idx[target_id]\n        return self._graph.has_edge(src_idx, tgt_idx)\n\n    def remove_node(self, node_id: str) -&gt; None:\n        if node_id in self._id_to_idx:\n            idx = self._id_to_idx[node_id]\n            node = self._idx_to_node[idx]\n            self.token_index.remove_node(node)\n            self._graph.remove_node(idx)\n            del self._id_to_idx[node_id]\n            del self._idx_to_node[idx]\n\n    def find_nodes(self, pattern: str) -&gt; List[str]:\n        return [nid for nid in self._id_to_idx.keys() if pattern in nid]\n\n    def find_nodes_by_tokens(self, tokens: List[str]) -&gt; List[Node]:\n        matched_ids = set()\n        for token in tokens:\n            matched_ids.update(self.token_index.find(token))\n        result = []\n        for nid in matched_ids:\n            node = self.get_node(nid)\n            if node: result.append(node)\n        return result\n\n    def get_nodes_by_type(self, type_filter: Any) -&gt; List[Node]:\n        if not isinstance(type_filter, NodeType):\n            try:\n                type_filter = NodeType(type_filter)\n            except ValueError:\n                return []\n        return [n for n in self._idx_to_node.values() if n.type == type_filter]\n\n    def get_out_edges(self, node_id: str) -&gt; List[Edge]:\n        if node_id not in self._id_to_idx:\n            return []\n        idx = self._id_to_idx[node_id]\n        return [edge_tuple[2] for edge_tuple in self._graph.out_edges(idx)]\n\n    def get_in_edges(self, node_id: str) -&gt; List[Edge]:\n        if node_id not in self._id_to_idx:\n            return []\n        idx = self._id_to_idx[node_id]\n        return [edge_tuple[2] for edge_tuple in self._graph.in_edges(idx)]\n\n    def get_edge(self, source_id: str, target_id: str) -&gt; Optional[Edge]:\n        if source_id not in self._id_to_idx or target_id not in self._id_to_idx:\n            return None\n        src_idx = self._id_to_idx[source_id]\n        tgt_idx = self._id_to_idx[target_id]\n        try:\n            edges = self._graph.get_all_edge_data(src_idx, tgt_idx)\n            if edges: return edges[0]\n        except rx.NoEdgeBetweenNodes:\n            return None\n        return None\n\n    def iter_nodes(self) -&gt; Iterator[Node]:\n        return iter(self._idx_to_node.values())\n\n    def iter_edges(self) -&gt; Iterator[Edge]:\n        return (edge for edge in self._graph.edges())\n\n    def trace(self, source_id: str, target_id: str) -&gt; List[List[str]]:\n        if source_id not in self._id_to_idx or target_id not in self._id_to_idx:\n            return []\n        src_idx = self._id_to_idx[source_id]\n        tgt_idx = self._id_to_idx[target_id]\n        try:\n            paths_indices = rx.all_simple_paths(self._graph, src_idx, tgt_idx)\n            result = []\n            for path in paths_indices:\n                result.append([self._idx_to_node[i].id for i in path])\n            return result\n        except Exception:\n            return []\n\n    def get_descendants(self, node_id: str, max_depth: int = -1) -&gt; Set[str]:\n        if node_id not in self._id_to_idx: return set()\n        start_idx = self._id_to_idx[node_id]\n        descendant_indices = rx.descendants(self._graph, start_idx)\n        return {self._idx_to_node[i].id for i in descendant_indices}\n\n    def get_ancestors(self, node_id: str, max_depth: int = -1) -&gt; Set[str]:\n        if node_id not in self._id_to_idx: return set()\n        start_idx = self._id_to_idx[node_id]\n        ancestor_indices = rx.ancestors(self._graph, start_idx)\n        return {self._idx_to_node[i].id for i in ancestor_indices}\n\n    def get_impacted_nodes(self, source_ids: List[str], max_depth: int = -1) -&gt; Set[str]:\n        \"\"\"\n        Calculate impacted nodes using semantic traversal logic.\n        Traverses:\n        - Outgoing: PROVIDES, WRITES, FLOWS_TO, PROVISIONS\n        - Incoming: READS, DEPENDS_ON (Reverse traversal)\n        \"\"\"\n        # Define semantic sets using string values for robustness\n        FORWARD_TYPES = {\"provides\", \"writes\", \"flows_to\", \"provisions\", \"outputs\"}\n        REVERSE_TYPES = {\"reads\", \"depends_on\", \"calls\"}\n\n        def normalize_type(val: Any) -&gt; str:\n            if hasattr(val, \"value\"): return str(val.value).lower()\n            return str(val).lower()\n\n        impacted = set()\n        queue = list(source_ids)\n        visited = set(source_ids)\n\n        # Track depth if needed (simple BFS level tracking omitted for brevity, using unlimited or basic check)\n        # For simplicity in this implementation, we ignore exact max_depth logic beyond a safety break\n        # or implement simple level tracking.\n\n        current_level = queue\n        depth = 0\n\n        while current_level:\n            if max_depth != -1 and depth &gt;= max_depth:\n                break\n\n            next_level = []\n            for node_id in current_level:\n                # 1. Forward Traversal (Downstream)\n                out_edges = self.get_out_edges(node_id)\n                for edge in out_edges:\n                    r_type = normalize_type(edge.type)\n                    if r_type in FORWARD_TYPES:\n                        neighbor = edge.target_id\n                        if neighbor not in visited:\n                            visited.add(neighbor)\n                            impacted.add(neighbor)\n                            next_level.append(neighbor)\n\n                # 2. Reverse Traversal (Upstream Dependencies)\n                in_edges = self.get_in_edges(node_id)\n                for edge in in_edges:\n                    r_type = normalize_type(edge.type)\n                    if r_type in REVERSE_TYPES:\n                        neighbor = edge.source_id\n                        if neighbor not in visited:\n                            visited.add(neighbor)\n                            impacted.add(neighbor)\n                            next_level.append(neighbor)\n\n            current_level = next_level\n            depth += 1\n\n        return impacted\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        return {\n            \"nodes\": [n.model_dump() for n in self.iter_nodes()],\n            \"edges\": [e.model_dump() for e in self.iter_edges()]\n        }\n</code></pre>"},{"location":"reference/api/core/graph/#jnkn.core.graph.DependencyGraph-functions","title":"Functions","text":""},{"location":"reference/api/core/graph/#jnkn.core.graph.DependencyGraph.get_impacted_nodes","title":"<code>get_impacted_nodes(source_ids, max_depth=-1)</code>","text":"<p>Calculate impacted nodes using semantic traversal logic. Traverses: - Outgoing: PROVIDES, WRITES, FLOWS_TO, PROVISIONS - Incoming: READS, DEPENDS_ON (Reverse traversal)</p> Source code in <code>src/jnkn/core/graph.py</code> <pre><code>def get_impacted_nodes(self, source_ids: List[str], max_depth: int = -1) -&gt; Set[str]:\n    \"\"\"\n    Calculate impacted nodes using semantic traversal logic.\n    Traverses:\n    - Outgoing: PROVIDES, WRITES, FLOWS_TO, PROVISIONS\n    - Incoming: READS, DEPENDS_ON (Reverse traversal)\n    \"\"\"\n    # Define semantic sets using string values for robustness\n    FORWARD_TYPES = {\"provides\", \"writes\", \"flows_to\", \"provisions\", \"outputs\"}\n    REVERSE_TYPES = {\"reads\", \"depends_on\", \"calls\"}\n\n    def normalize_type(val: Any) -&gt; str:\n        if hasattr(val, \"value\"): return str(val.value).lower()\n        return str(val).lower()\n\n    impacted = set()\n    queue = list(source_ids)\n    visited = set(source_ids)\n\n    # Track depth if needed (simple BFS level tracking omitted for brevity, using unlimited or basic check)\n    # For simplicity in this implementation, we ignore exact max_depth logic beyond a safety break\n    # or implement simple level tracking.\n\n    current_level = queue\n    depth = 0\n\n    while current_level:\n        if max_depth != -1 and depth &gt;= max_depth:\n            break\n\n        next_level = []\n        for node_id in current_level:\n            # 1. Forward Traversal (Downstream)\n            out_edges = self.get_out_edges(node_id)\n            for edge in out_edges:\n                r_type = normalize_type(edge.type)\n                if r_type in FORWARD_TYPES:\n                    neighbor = edge.target_id\n                    if neighbor not in visited:\n                        visited.add(neighbor)\n                        impacted.add(neighbor)\n                        next_level.append(neighbor)\n\n            # 2. Reverse Traversal (Upstream Dependencies)\n            in_edges = self.get_in_edges(node_id)\n            for edge in in_edges:\n                r_type = normalize_type(edge.type)\n                if r_type in REVERSE_TYPES:\n                    neighbor = edge.source_id\n                    if neighbor not in visited:\n                        visited.add(neighbor)\n                        impacted.add(neighbor)\n                        next_level.append(neighbor)\n\n        current_level = next_level\n        depth += 1\n\n    return impacted\n</code></pre>"},{"location":"reference/api/core/graph/#jnkn.core.graph.TokenIndex","title":"<code>TokenIndex</code>  <code>dataclass</code>","text":"<p>In-memory index for fast fuzzy searching of nodes by tokens.</p> Source code in <code>src/jnkn/core/graph.py</code> <pre><code>@dataclass\nclass TokenIndex:\n    \"\"\"\n    In-memory index for fast fuzzy searching of nodes by tokens.\n    \"\"\"\n    _token_map: Dict[str, Set[str]] = field(default_factory=lambda: defaultdict(set))\n\n    def index_node(self, node: Node) -&gt; None:\n        if not node.tokens: return\n        for token in node.tokens:\n            self._token_map[token.lower()].add(node.id)\n\n    def find(self, token: str) -&gt; Set[str]:\n        return self._token_map.get(token.lower(), set())\n\n    def remove_node(self, node: Node) -&gt; None:\n        if not node.tokens: return\n        for token in node.tokens:\n            token_lower = token.lower()\n            if token_lower in self._token_map:\n                self._token_map[token_lower].discard(node.id)\n                if not self._token_map[token_lower]:\n                    del self._token_map[token_lower]\n</code></pre>"},{"location":"reference/api/core/storage/","title":"Storage Adapters","text":""},{"location":"reference/api/core/storage/#jnkn.core.storage","title":"<code>jnkn.core.storage</code>","text":"<p>Storage adapters for jnkn.</p> <p>Provides pluggable persistence backends: - SQLiteStorage: Production-ready local persistence - MemoryStorage: Fast ephemeral storage for testing</p>"},{"location":"reference/api/core/storage/#jnkn.core.storage-classes","title":"Classes","text":""},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage","title":"<code>MemoryStorage</code>","text":"<p>               Bases: <code>StorageAdapter</code></p> <p>Ephemeral in-memory storage.</p> <p>Useful for unit testing, CI pipelines, and development.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>class MemoryStorage(StorageAdapter):\n    \"\"\"\n    Ephemeral in-memory storage.\n\n    Useful for unit testing, CI pipelines, and development.\n    \"\"\"\n\n    def __init__(self):\n        self._nodes: Dict[str, Node] = {}\n        self._edges: Dict[str, Edge] = {}\n        self._scan_metadata: Dict[str, ScanMetadata] = {}\n        self._nodes_by_type: Dict[NodeType, Set[str]] = defaultdict(set)\n        self._edges_by_source: Dict[str, Set[str]] = defaultdict(set)\n        self._edges_by_target: Dict[str, Set[str]] = defaultdict(set)\n\n    def _edge_key(self, source: str, target: str, edge_type: str) -&gt; str:\n        \"\"\"Generate unique key for an edge.\"\"\"\n        return f\"{source}|{target}|{edge_type}\"\n\n    def save_node(self, node: Node) -&gt; None:\n        \"\"\"Persist a single node.\"\"\"\n        self._nodes[node.id] = node\n        self._nodes_by_type[node.type].add(node.id)\n\n    def save_nodes_batch(self, nodes: List[Node]) -&gt; int:\n        \"\"\"Persist multiple nodes.\"\"\"\n        for node in nodes:\n            self.save_node(node)\n        return len(nodes)\n\n    def load_node(self, node_id: str) -&gt; Optional[Node]:\n        \"\"\"Load a node by ID.\"\"\"\n        return self._nodes.get(node_id)\n\n    def load_all_nodes(self) -&gt; List[Node]:\n        \"\"\"Load all nodes.\"\"\"\n        return list(self._nodes.values())\n\n    def delete_node(self, node_id: str) -&gt; bool:\n        \"\"\"Delete a node and its edges.\"\"\"\n        if node_id not in self._nodes:\n            return False\n\n        node = self._nodes[node_id]\n        self._nodes_by_type[node.type].discard(node_id)\n        del self._nodes[node_id]\n\n        for edge_key in list(self._edges_by_source.get(node_id, set())):\n            if edge_key in self._edges:\n                edge = self._edges[edge_key]\n                self._edges_by_target[edge.target_id].discard(edge_key)\n                del self._edges[edge_key]\n\n        for edge_key in list(self._edges_by_target.get(node_id, set())):\n            if edge_key in self._edges:\n                edge = self._edges[edge_key]\n                self._edges_by_source[edge.source_id].discard(edge_key)\n                del self._edges[edge_key]\n\n        self._edges_by_source.pop(node_id, None)\n        self._edges_by_target.pop(node_id, None)\n\n        return True\n\n    def delete_nodes_by_file(self, file_path: str) -&gt; int:\n        \"\"\"Delete all nodes from a file.\"\"\"\n        node_ids = [\n            node_id for node_id, node in self._nodes.items()\n            if node.path == file_path\n        ]\n        for node_id in node_ids:\n            self.delete_node(node_id)\n        return len(node_ids)\n\n    def save_edge(self, edge: Edge) -&gt; None:\n        \"\"\"Persist a single edge.\"\"\"\n        key = self._edge_key(edge.source_id, edge.target_id, edge.type.value)\n        self._edges[key] = edge\n        self._edges_by_source[edge.source_id].add(key)\n        self._edges_by_target[edge.target_id].add(key)\n\n    def save_edges_batch(self, edges: List[Edge]) -&gt; int:\n        \"\"\"Persist multiple edges.\"\"\"\n        for edge in edges:\n            self.save_edge(edge)\n        return len(edges)\n\n    def load_all_edges(self) -&gt; List[Edge]:\n        \"\"\"Load all edges.\"\"\"\n        return list(self._edges.values())\n\n    def delete_edges_by_source(self, source_id: str) -&gt; int:\n        \"\"\"Delete all edges from a source.\"\"\"\n        count = 0\n        for edge_key in list(self._edges_by_source.get(source_id, set())):\n            if edge_key in self._edges:\n                edge = self._edges[edge_key]\n                self._edges_by_target[edge.target_id].discard(edge_key)\n                del self._edges[edge_key]\n                count += 1\n        self._edges_by_source.pop(source_id, None)\n        return count\n\n    def load_graph(self) -&gt; DependencyGraph:\n        \"\"\"Hydrate a DependencyGraph.\"\"\"\n        graph = DependencyGraph()\n        for node in self._nodes.values():\n            graph.add_node(node)\n        for edge in self._edges.values():\n            graph.add_edge(edge)\n        return graph\n\n    def query_descendants(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n        \"\"\"Query descendants via BFS.\"\"\"\n        visited: Set[str] = set()\n        queue: List[Tuple[str, int]] = [(node_id, 0)]\n\n        while queue:\n            current, depth = queue.pop(0)\n\n            if max_depth &gt;= 0 and depth &gt;= max_depth:\n                continue\n\n            for edge_key in self._edges_by_source.get(current, set()):\n                edge = self._edges.get(edge_key)\n                if edge and edge.target_id not in visited:\n                    visited.add(edge.target_id)\n                    queue.append((edge.target_id, depth + 1))\n\n        return list(visited)\n\n    def query_ancestors(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n        \"\"\"Query ancestors via BFS.\"\"\"\n        visited: Set[str] = set()\n        queue: List[Tuple[str, int]] = [(node_id, 0)]\n\n        while queue:\n            current, depth = queue.pop(0)\n\n            if max_depth &gt;= 0 and depth &gt;= max_depth:\n                continue\n\n            for edge_key in self._edges_by_target.get(current, set()):\n                edge = self._edges.get(edge_key)\n                if edge and edge.source_id not in visited:\n                    visited.add(edge.source_id)\n                    queue.append((edge.source_id, depth + 1))\n\n        return list(visited)\n\n    def save_scan_metadata(self, metadata: ScanMetadata) -&gt; None:\n        \"\"\"Save scan metadata.\"\"\"\n        self._scan_metadata[metadata.file_path] = metadata\n\n    def get_scan_metadata(self, file_path: str) -&gt; Optional[ScanMetadata]:\n        \"\"\"Get scan metadata for a file.\"\"\"\n        return self._scan_metadata.get(file_path)\n\n    def get_all_scan_metadata(self) -&gt; List[ScanMetadata]:\n        \"\"\"Get all scan metadata.\"\"\"\n        return list(self._scan_metadata.values())\n\n    def delete_scan_metadata(self, file_path: str) -&gt; bool:\n        \"\"\"Delete scan metadata.\"\"\"\n        if file_path in self._scan_metadata:\n            del self._scan_metadata[file_path]\n            return True\n        return False\n\n    def get_schema_version(self) -&gt; int:\n        \"\"\"Memory storage doesn't have schema versions.\"\"\"\n        return 0\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get storage statistics.\"\"\"\n        return {\n            \"total_nodes\": len(self._nodes),\n            \"total_edges\": len(self._edges),\n            \"tracked_files\": len(self._scan_metadata),\n        }\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all data.\"\"\"\n        self._nodes.clear()\n        self._edges.clear()\n        self._scan_metadata.clear()\n        self._nodes_by_type.clear()\n        self._edges_by_source.clear()\n        self._edges_by_target.clear()\n\n    def close(self) -&gt; None:\n        \"\"\"No-op for memory storage.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage-functions","title":"Functions","text":""},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.clear","title":"<code>clear()</code>","text":"<p>Clear all data.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all data.\"\"\"\n    self._nodes.clear()\n    self._edges.clear()\n    self._scan_metadata.clear()\n    self._nodes_by_type.clear()\n    self._edges_by_source.clear()\n    self._edges_by_target.clear()\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.close","title":"<code>close()</code>","text":"<p>No-op for memory storage.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"No-op for memory storage.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.delete_edges_by_source","title":"<code>delete_edges_by_source(source_id)</code>","text":"<p>Delete all edges from a source.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def delete_edges_by_source(self, source_id: str) -&gt; int:\n    \"\"\"Delete all edges from a source.\"\"\"\n    count = 0\n    for edge_key in list(self._edges_by_source.get(source_id, set())):\n        if edge_key in self._edges:\n            edge = self._edges[edge_key]\n            self._edges_by_target[edge.target_id].discard(edge_key)\n            del self._edges[edge_key]\n            count += 1\n    self._edges_by_source.pop(source_id, None)\n    return count\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.delete_node","title":"<code>delete_node(node_id)</code>","text":"<p>Delete a node and its edges.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def delete_node(self, node_id: str) -&gt; bool:\n    \"\"\"Delete a node and its edges.\"\"\"\n    if node_id not in self._nodes:\n        return False\n\n    node = self._nodes[node_id]\n    self._nodes_by_type[node.type].discard(node_id)\n    del self._nodes[node_id]\n\n    for edge_key in list(self._edges_by_source.get(node_id, set())):\n        if edge_key in self._edges:\n            edge = self._edges[edge_key]\n            self._edges_by_target[edge.target_id].discard(edge_key)\n            del self._edges[edge_key]\n\n    for edge_key in list(self._edges_by_target.get(node_id, set())):\n        if edge_key in self._edges:\n            edge = self._edges[edge_key]\n            self._edges_by_source[edge.source_id].discard(edge_key)\n            del self._edges[edge_key]\n\n    self._edges_by_source.pop(node_id, None)\n    self._edges_by_target.pop(node_id, None)\n\n    return True\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.delete_nodes_by_file","title":"<code>delete_nodes_by_file(file_path)</code>","text":"<p>Delete all nodes from a file.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def delete_nodes_by_file(self, file_path: str) -&gt; int:\n    \"\"\"Delete all nodes from a file.\"\"\"\n    node_ids = [\n        node_id for node_id, node in self._nodes.items()\n        if node.path == file_path\n    ]\n    for node_id in node_ids:\n        self.delete_node(node_id)\n    return len(node_ids)\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.delete_scan_metadata","title":"<code>delete_scan_metadata(file_path)</code>","text":"<p>Delete scan metadata.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def delete_scan_metadata(self, file_path: str) -&gt; bool:\n    \"\"\"Delete scan metadata.\"\"\"\n    if file_path in self._scan_metadata:\n        del self._scan_metadata[file_path]\n        return True\n    return False\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.get_all_scan_metadata","title":"<code>get_all_scan_metadata()</code>","text":"<p>Get all scan metadata.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def get_all_scan_metadata(self) -&gt; List[ScanMetadata]:\n    \"\"\"Get all scan metadata.\"\"\"\n    return list(self._scan_metadata.values())\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.get_scan_metadata","title":"<code>get_scan_metadata(file_path)</code>","text":"<p>Get scan metadata for a file.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def get_scan_metadata(self, file_path: str) -&gt; Optional[ScanMetadata]:\n    \"\"\"Get scan metadata for a file.\"\"\"\n    return self._scan_metadata.get(file_path)\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.get_schema_version","title":"<code>get_schema_version()</code>","text":"<p>Memory storage doesn't have schema versions.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def get_schema_version(self) -&gt; int:\n    \"\"\"Memory storage doesn't have schema versions.\"\"\"\n    return 0\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.get_stats","title":"<code>get_stats()</code>","text":"<p>Get storage statistics.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def get_stats(self) -&gt; Dict[str, Any]:\n    \"\"\"Get storage statistics.\"\"\"\n    return {\n        \"total_nodes\": len(self._nodes),\n        \"total_edges\": len(self._edges),\n        \"tracked_files\": len(self._scan_metadata),\n    }\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.load_all_edges","title":"<code>load_all_edges()</code>","text":"<p>Load all edges.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def load_all_edges(self) -&gt; List[Edge]:\n    \"\"\"Load all edges.\"\"\"\n    return list(self._edges.values())\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.load_all_nodes","title":"<code>load_all_nodes()</code>","text":"<p>Load all nodes.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def load_all_nodes(self) -&gt; List[Node]:\n    \"\"\"Load all nodes.\"\"\"\n    return list(self._nodes.values())\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.load_graph","title":"<code>load_graph()</code>","text":"<p>Hydrate a DependencyGraph.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def load_graph(self) -&gt; DependencyGraph:\n    \"\"\"Hydrate a DependencyGraph.\"\"\"\n    graph = DependencyGraph()\n    for node in self._nodes.values():\n        graph.add_node(node)\n    for edge in self._edges.values():\n        graph.add_edge(edge)\n    return graph\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.load_node","title":"<code>load_node(node_id)</code>","text":"<p>Load a node by ID.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def load_node(self, node_id: str) -&gt; Optional[Node]:\n    \"\"\"Load a node by ID.\"\"\"\n    return self._nodes.get(node_id)\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.query_ancestors","title":"<code>query_ancestors(node_id, max_depth=-1)</code>","text":"<p>Query ancestors via BFS.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def query_ancestors(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n    \"\"\"Query ancestors via BFS.\"\"\"\n    visited: Set[str] = set()\n    queue: List[Tuple[str, int]] = [(node_id, 0)]\n\n    while queue:\n        current, depth = queue.pop(0)\n\n        if max_depth &gt;= 0 and depth &gt;= max_depth:\n            continue\n\n        for edge_key in self._edges_by_target.get(current, set()):\n            edge = self._edges.get(edge_key)\n            if edge and edge.source_id not in visited:\n                visited.add(edge.source_id)\n                queue.append((edge.source_id, depth + 1))\n\n    return list(visited)\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.query_descendants","title":"<code>query_descendants(node_id, max_depth=-1)</code>","text":"<p>Query descendants via BFS.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def query_descendants(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n    \"\"\"Query descendants via BFS.\"\"\"\n    visited: Set[str] = set()\n    queue: List[Tuple[str, int]] = [(node_id, 0)]\n\n    while queue:\n        current, depth = queue.pop(0)\n\n        if max_depth &gt;= 0 and depth &gt;= max_depth:\n            continue\n\n        for edge_key in self._edges_by_source.get(current, set()):\n            edge = self._edges.get(edge_key)\n            if edge and edge.target_id not in visited:\n                visited.add(edge.target_id)\n                queue.append((edge.target_id, depth + 1))\n\n    return list(visited)\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.save_edge","title":"<code>save_edge(edge)</code>","text":"<p>Persist a single edge.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def save_edge(self, edge: Edge) -&gt; None:\n    \"\"\"Persist a single edge.\"\"\"\n    key = self._edge_key(edge.source_id, edge.target_id, edge.type.value)\n    self._edges[key] = edge\n    self._edges_by_source[edge.source_id].add(key)\n    self._edges_by_target[edge.target_id].add(key)\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.save_edges_batch","title":"<code>save_edges_batch(edges)</code>","text":"<p>Persist multiple edges.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def save_edges_batch(self, edges: List[Edge]) -&gt; int:\n    \"\"\"Persist multiple edges.\"\"\"\n    for edge in edges:\n        self.save_edge(edge)\n    return len(edges)\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.save_node","title":"<code>save_node(node)</code>","text":"<p>Persist a single node.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def save_node(self, node: Node) -&gt; None:\n    \"\"\"Persist a single node.\"\"\"\n    self._nodes[node.id] = node\n    self._nodes_by_type[node.type].add(node.id)\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.save_nodes_batch","title":"<code>save_nodes_batch(nodes)</code>","text":"<p>Persist multiple nodes.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def save_nodes_batch(self, nodes: List[Node]) -&gt; int:\n    \"\"\"Persist multiple nodes.\"\"\"\n    for node in nodes:\n        self.save_node(node)\n    return len(nodes)\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.MemoryStorage.save_scan_metadata","title":"<code>save_scan_metadata(metadata)</code>","text":"<p>Save scan metadata.</p> Source code in <code>src/jnkn/core/storage/memory.py</code> <pre><code>def save_scan_metadata(self, metadata: ScanMetadata) -&gt; None:\n    \"\"\"Save scan metadata.\"\"\"\n    self._scan_metadata[metadata.file_path] = metadata\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.SQLiteStorage","title":"<code>SQLiteStorage</code>","text":"<p>               Bases: <code>StorageAdapter</code></p> <p>Persistent storage using local SQLite file.</p> Source code in <code>src/jnkn/core/storage/sqlite.py</code> <pre><code>class SQLiteStorage(StorageAdapter):\n    \"\"\"\n    Persistent storage using local SQLite file.\n    \"\"\"\n\n    def __init__(self, db_path: Path):\n        self.db_path = db_path\n        self._init_db()\n\n    @contextmanager\n    def _connection(self):\n        \"\"\"Context manager for database connections with WAL mode.\"\"\"\n        conn = sqlite3.connect(\n            self.db_path,\n            detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES\n        )\n        conn.row_factory = sqlite3.Row\n        # Performance tuning for bulk writes\n        conn.execute(\"PRAGMA journal_mode=WAL\")\n        conn.execute(\"PRAGMA foreign_keys=ON\")\n        conn.execute(\"PRAGMA synchronous=NORMAL\")\n        try:\n            yield conn\n            conn.commit()\n        except Exception:\n            conn.rollback()\n            raise\n        finally:\n            conn.close()\n\n    def _init_db(self) -&gt; None:\n        \"\"\"Initialize database schema with versioning.\"\"\"\n        self.db_path.parent.mkdir(parents=True, exist_ok=True)\n\n        with self._connection() as conn:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS schema_version (\n                    version INTEGER PRIMARY KEY,\n                    applied_at TEXT NOT NULL,\n                    description TEXT\n                )\n            \"\"\")\n\n            current_version = self._get_schema_version_internal(conn)\n\n            if current_version &lt; SCHEMA_VERSION:\n                self._migrate(conn, current_version)\n\n    def _get_schema_version_internal(self, conn: sqlite3.Connection) -&gt; int:\n        \"\"\"Get schema version using existing connection.\"\"\"\n        row = conn.execute(\n            \"SELECT MAX(version) as v FROM schema_version\"\n        ).fetchone()\n        return row[\"v\"] if row and row[\"v\"] else 0\n\n    def _migrate(self, conn: sqlite3.Connection, from_version: int) -&gt; None:\n        \"\"\"Run schema migrations.\"\"\"\n\n        # V1: Base Schema\n        if from_version &lt; 1:\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS nodes (\n                    id TEXT PRIMARY KEY,\n                    name TEXT NOT NULL,\n                    type TEXT NOT NULL,\n                    path TEXT,\n                    language TEXT,\n                    file_hash TEXT,\n                    tokens TEXT,\n                    metadata TEXT,\n                    created_at TEXT NOT NULL\n                )\n            \"\"\")\n\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS edges (\n                    source_id TEXT NOT NULL,\n                    target_id TEXT NOT NULL,\n                    type TEXT NOT NULL,\n                    confidence REAL DEFAULT 1.0,\n                    match_strategy TEXT,\n                    metadata TEXT,\n                    created_at TEXT NOT NULL,\n                    PRIMARY KEY (source_id, target_id, type)\n                )\n            \"\"\")\n\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS scan_metadata (\n                    file_path TEXT PRIMARY KEY,\n                    file_hash TEXT NOT NULL,\n                    last_scanned TEXT NOT NULL,\n                    node_count INTEGER DEFAULT 0,\n                    edge_count INTEGER DEFAULT 0\n                )\n            \"\"\")\n\n            conn.execute(\"CREATE INDEX IF NOT EXISTS idx_nodes_type ON nodes(type)\")\n            conn.execute(\"CREATE INDEX IF NOT EXISTS idx_nodes_path ON nodes(path)\")\n            conn.execute(\"CREATE INDEX IF NOT EXISTS idx_edges_source ON edges(source_id)\")\n            conn.execute(\"CREATE INDEX IF NOT EXISTS idx_edges_target ON edges(target_id)\")\n\n            conn.execute(\"\"\"\n                INSERT INTO schema_version (version, applied_at, description)\n                VALUES (1, ?, 'Initial schema')\n            \"\"\", (datetime.now(timezone.utc).isoformat(),))\n\n        # V2: Confidence Index\n        if from_version &lt; 2:\n            conn.execute(\n                \"CREATE INDEX IF NOT EXISTS idx_edges_confidence ON edges(confidence)\"\n            )\n            conn.execute(\"\"\"\n                INSERT INTO schema_version (version, applied_at, description)\n                VALUES (2, ?, 'Added confidence index')\n            \"\"\", (datetime.now(timezone.utc).isoformat(),))\n\n        # V3: Token Index and High-Confidence View\n        if from_version &lt; 3:\n            # Token Index Table\n            conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS token_index (\n                    token TEXT NOT NULL,\n                    node_id TEXT NOT NULL,\n                    PRIMARY KEY (token, node_id),\n                    FOREIGN KEY (node_id) REFERENCES nodes(id) ON DELETE CASCADE\n                )\n            \"\"\")\n            conn.execute(\"CREATE INDEX IF NOT EXISTS idx_token_lookup ON token_index(token)\")\n\n            # High Confidence View (Pruning optimization)\n            conn.execute(\"\"\"\n                CREATE VIEW IF NOT EXISTS high_confidence_edges AS\n                SELECT * FROM edges WHERE confidence &gt;= 0.7\n            \"\"\")\n\n            conn.execute(\"\"\"\n                INSERT INTO schema_version (version, applied_at, description)\n                VALUES (3, ?, 'Added token_index table and high_confidence_edges view')\n            \"\"\", (datetime.now(timezone.utc).isoformat(),))\n\n    def get_schema_version(self) -&gt; int:\n        with self._connection() as conn:\n            return self._get_schema_version_internal(conn)\n\n    # --- Node Persistence ---\n\n    def save_node(self, node: Node) -&gt; None:\n        \"\"\"Persist a single node.\"\"\"\n        self.save_nodes_batch([node])\n\n    def save_nodes_batch(self, nodes: List[Node]) -&gt; int:\n        \"\"\"Persist multiple nodes and their tokens in a single transaction.\"\"\"\n        if not nodes:\n            return 0\n\n        with self._connection() as conn:\n            # 1. Upsert Nodes\n            conn.executemany(\"\"\"\n                INSERT OR REPLACE INTO nodes \n                (id, name, type, path, language, file_hash, tokens, metadata, created_at)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", [\n                (n.id, n.name, n.type.value, n.path, n.language, n.file_hash,\n                 json.dumps(n.tokens), json.dumps(n.metadata), n.created_at.isoformat())\n                for n in nodes\n            ])\n\n            # 2. Update Token Index\n            # Delete existing tokens for these nodes first to ensure clean state on update\n            node_ids = [n.id for n in nodes]\n            placeholders = \",\".join(\"?\" * len(node_ids))\n            conn.execute(f\"DELETE FROM token_index WHERE node_id IN ({placeholders})\", node_ids)\n\n            # Flatten tokens for batch insert\n            token_entries = []\n            for node in nodes:\n                # Use tokens from the object, defaulting to computed ones if missing\n                # (though model_post_init handles this usually)\n                tokens = node.tokens or []\n                for token in tokens:\n                    token_entries.append((token, node.id))\n\n            if token_entries:\n                conn.executemany(\"\"\"\n                    INSERT OR IGNORE INTO token_index (token, node_id)\n                    VALUES (?, ?)\n                \"\"\", token_entries)\n\n        return len(nodes)\n\n    def load_node(self, node_id: str) -&gt; Optional[Node]:\n        with self._connection() as conn:\n            row = conn.execute(\n                \"SELECT * FROM nodes WHERE id = ?\", (node_id,)\n            ).fetchone()\n            return self._row_to_node(row) if row else None\n\n    def load_all_nodes(self) -&gt; List[Node]:\n        nodes = []\n        with self._connection() as conn:\n            # Use fetchmany generator logic for memory efficiency on massive graphs?\n            # For now, fetchall is standard for prompt compliance.\n            for row in conn.execute(\"SELECT * FROM nodes\").fetchall():\n                try:\n                    nodes.append(self._row_to_node(row))\n                except Exception:\n                    pass\n        return nodes\n\n    def _row_to_node(self, row: sqlite3.Row) -&gt; Node:\n        return Node(\n            id=row[\"id\"],\n            name=row[\"name\"],\n            type=NodeType(row[\"type\"]),\n            path=row[\"path\"],\n            language=row[\"language\"],\n            file_hash=row[\"file_hash\"],\n            tokens=json.loads(row[\"tokens\"]) if row[\"tokens\"] else [],\n            metadata=json.loads(row[\"metadata\"]) if row[\"metadata\"] else {},\n            created_at=datetime.fromisoformat(row[\"created_at\"]),\n        )\n\n    def delete_node(self, node_id: str) -&gt; bool:\n        with self._connection() as conn:\n            # Cascading delete handles token_index via FK\n            conn.execute(\n                \"DELETE FROM edges WHERE source_id = ? OR target_id = ?\",\n                (node_id, node_id)\n            )\n            cursor = conn.execute(\"DELETE FROM nodes WHERE id = ?\", (node_id,))\n            return cursor.rowcount &gt; 0\n\n    def delete_nodes_by_file(self, file_path: str) -&gt; int:\n        with self._connection() as conn:\n            # Find IDs\n            rows = conn.execute(\n                \"SELECT id FROM nodes WHERE path = ?\", (file_path,)\n            ).fetchall()\n            node_ids = [row[\"id\"] for row in rows]\n\n            if not node_ids:\n                return 0\n\n            placeholders = \",\".join(\"?\" * len(node_ids))\n\n            # Delete Edges\n            conn.execute(f\"\"\"\n                DELETE FROM edges \n                WHERE source_id IN ({placeholders}) OR target_id IN ({placeholders})\n            \"\"\", node_ids + node_ids)\n\n            # Delete Nodes (Cascades to token_index)\n            cursor = conn.execute(\n                f\"DELETE FROM nodes WHERE id IN ({placeholders})\", node_ids\n            )\n            return cursor.rowcount\n\n    # --- Edge Persistence ---\n\n    def save_edge(self, edge: Edge) -&gt; None:\n        self.save_edges_batch([edge])\n\n    def save_edges_batch(self, edges: List[Edge]) -&gt; int:\n        if not edges:\n            return 0\n\n        with self._connection() as conn:\n            conn.executemany(\"\"\"\n                INSERT OR REPLACE INTO edges \n                (source_id, target_id, type, confidence, match_strategy, metadata, created_at)\n                VALUES (?, ?, ?, ?, ?, ?, ?)\n            \"\"\", [\n                (e.source_id, e.target_id, e.type.value, e.confidence,\n                 e.match_strategy.value if e.match_strategy else None,\n                 json.dumps(e.metadata), e.created_at.isoformat())\n                for e in edges\n            ])\n        return len(edges)\n\n    def load_all_edges(self, min_confidence: float = 0.0) -&gt; List[Edge]:\n        \"\"\"\n        Load all edges, optionally filtering by confidence.\n        Uses the high_confidence_edges view if threshold &gt;= 0.7.\n        \"\"\"\n        table = \"high_confidence_edges\" if min_confidence &gt;= 0.7 else \"edges\"\n        query = f\"SELECT * FROM {table}\"\n        params = []\n\n        if 0.0 &lt; min_confidence &lt; 0.7:\n            query += \" WHERE confidence &gt;= ?\"\n            params.append(min_confidence)\n\n        edges = []\n        with self._connection() as conn:\n            for row in conn.execute(query, params).fetchall():\n                try:\n                    edges.append(self._row_to_edge(row))\n                except Exception:\n                    pass\n        return edges\n\n    def _row_to_edge(self, row: sqlite3.Row) -&gt; Edge:\n        return Edge(\n            source_id=row[\"source_id\"],\n            target_id=row[\"target_id\"],\n            type=RelationshipType(row[\"type\"]),\n            confidence=row[\"confidence\"],\n            match_strategy=MatchStrategy(row[\"match_strategy\"]) if row[\"match_strategy\"] else None,\n            metadata=json.loads(row[\"metadata\"]) if row[\"metadata\"] else {},\n            created_at=datetime.fromisoformat(row[\"created_at\"]),\n        )\n\n    def delete_edges_by_source(self, source_id: str) -&gt; int:\n        with self._connection() as conn:\n            cursor = conn.execute(\n                \"DELETE FROM edges WHERE source_id = ?\", (source_id,)\n            )\n            return cursor.rowcount\n\n    # --- Scan Metadata Persistence ---\n\n    def save_scan_metadata(self, metadata: ScanMetadata) -&gt; None:\n        with self._connection() as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO scan_metadata \n                (file_path, file_hash, last_scanned, node_count, edge_count)\n                VALUES (?, ?, ?, ?, ?)\n            \"\"\", (\n                metadata.file_path, metadata.file_hash,\n                metadata.last_scanned.isoformat(),\n                metadata.node_count, metadata.edge_count,\n            ))\n\n    def get_scan_metadata(self, file_path: str) -&gt; Optional[ScanMetadata]:\n        with self._connection() as conn:\n            row = conn.execute(\n                \"SELECT * FROM scan_metadata WHERE file_path = ?\", (file_path,)\n            ).fetchone()\n\n            if not row:\n                return None\n\n            return ScanMetadata(\n                file_path=row[\"file_path\"],\n                file_hash=row[\"file_hash\"],\n                last_scanned=datetime.fromisoformat(row[\"last_scanned\"]),\n                node_count=row[\"node_count\"],\n                edge_count=row[\"edge_count\"],\n            )\n\n    def get_all_scan_metadata(self) -&gt; List[ScanMetadata]:\n        with self._connection() as conn:\n            rows = conn.execute(\"SELECT * FROM scan_metadata\").fetchall()\n            return [\n                ScanMetadata(\n                    file_path=row[\"file_path\"],\n                    file_hash=row[\"file_hash\"],\n                    last_scanned=datetime.fromisoformat(row[\"last_scanned\"]),\n                    node_count=row[\"node_count\"],\n                    edge_count=row[\"edge_count\"],\n                )\n                for row in rows\n            ]\n\n    def delete_scan_metadata(self, file_path: str) -&gt; bool:\n        with self._connection() as conn:\n            cursor = conn.execute(\n                \"DELETE FROM scan_metadata WHERE file_path = ?\", (file_path,)\n            )\n            return cursor.rowcount &gt; 0\n\n    # --- Graph Hydration ---\n\n    def load_graph(self) -&gt; DependencyGraph:\n        \"\"\"\n        Hydrate a full DependencyGraph from storage.\n\n        Optimized to load token index from DB rather than rebuilding it.\n        \"\"\"\n        graph = DependencyGraph()\n\n        # Load nodes\n        all_nodes = self.load_all_nodes()\n        for node in all_nodes:\n            graph.add_node(node)\n\n        # Load edges\n        all_edges = self.load_all_edges()\n        for edge in all_edges:\n            graph.add_edge(edge)\n\n        # Note: DependencyGraph.add_node automatically updates the in-memory token index.\n        # Since we just rebuilt the graph object from scratch, its in-memory TokenIndex \n        # is already populated by `add_node`. \n        # We don't need to manually SELECT FROM token_index unless we were doing\n        # a \"lazy load\" graph that didn't hold everything in memory.\n        # However, `token_index` table is crucial for specific SQL-based lookups \n        # (e.g. stitching optimization) without loading the full graph.\n\n        return graph\n\n    # --- Traversal Queries ---\n\n    def query_descendants(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n        \"\"\"Query all descendants using recursive CTE.\"\"\"\n        with self._connection() as conn:\n            if max_depth &lt; 0:\n                rows = conn.execute(\"\"\"\n                    WITH RECURSIVE descendants AS (\n                        SELECT target_id as id, 1 as depth\n                        FROM edges WHERE source_id = ?\n                        UNION\n                        SELECT e.target_id, d.depth + 1\n                        FROM edges e JOIN descendants d ON e.source_id = d.id\n                    )\n                    SELECT DISTINCT id FROM descendants\n                \"\"\", (node_id,)).fetchall()\n            else:\n                rows = conn.execute(\"\"\"\n                    WITH RECURSIVE descendants AS (\n                        SELECT target_id as id, 1 as depth\n                        FROM edges WHERE source_id = ?\n                        UNION\n                        SELECT e.target_id, d.depth + 1\n                        FROM edges e JOIN descendants d ON e.source_id = d.id\n                        WHERE d.depth &lt; ?\n                    )\n                    SELECT DISTINCT id FROM descendants\n                \"\"\", (node_id, max_depth)).fetchall()\n            return [row[\"id\"] for row in rows]\n\n    def query_ancestors(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n        \"\"\"Query all ancestors using recursive CTE.\"\"\"\n        with self._connection() as conn:\n            if max_depth &lt; 0:\n                rows = conn.execute(\"\"\"\n                    WITH RECURSIVE ancestors AS (\n                        SELECT source_id as id, 1 as depth\n                        FROM edges WHERE target_id = ?\n                        UNION\n                        SELECT e.source_id, a.depth + 1\n                        FROM edges e JOIN ancestors a ON e.target_id = a.id\n                    )\n                    SELECT DISTINCT id FROM ancestors\n                \"\"\", (node_id,)).fetchall()\n            else:\n                rows = conn.execute(\"\"\"\n                    WITH RECURSIVE ancestors AS (\n                        SELECT source_id as id, 1 as depth\n                        FROM edges WHERE target_id = ?\n                        UNION\n                        SELECT e.source_id, a.depth + 1\n                        FROM edges e JOIN ancestors a ON e.target_id = a.id\n                        WHERE a.depth &lt; ?\n                    )\n                    SELECT DISTINCT id FROM ancestors\n                \"\"\", (node_id, max_depth)).fetchall()\n            return [row[\"id\"] for row in rows]\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get storage statistics.\"\"\"\n        with self._connection() as conn:\n            node_count = conn.execute(\"SELECT COUNT(*) as c FROM nodes\").fetchone()[\"c\"]\n            edge_count = conn.execute(\"SELECT COUNT(*) as c FROM edges\").fetchone()[\"c\"]\n            file_count = conn.execute(\"SELECT COUNT(*) as c FROM scan_metadata\").fetchone()[\"c\"]\n            token_count = conn.execute(\"SELECT COUNT(*) as c FROM token_index\").fetchone()[\"c\"]\n\n            type_rows = conn.execute(\n                \"SELECT type, COUNT(*) as c FROM nodes GROUP BY type\"\n            ).fetchall()\n\n            edge_type_rows = conn.execute(\n                \"SELECT type, COUNT(*) as c FROM edges GROUP BY type\"\n            ).fetchall()\n\n            return {\n                \"schema_version\": self._get_schema_version_internal(conn),\n                \"total_nodes\": node_count,\n                \"total_edges\": edge_count,\n                \"tracked_files\": file_count,\n                \"indexed_tokens\": token_count,\n                \"nodes_by_type\": {row[\"type\"]: row[\"c\"] for row in type_rows},\n                \"edges_by_type\": {row[\"type\"]: row[\"c\"] for row in edge_type_rows},\n                \"db_size_bytes\": self.db_path.stat().st_size if self.db_path.exists() else 0,\n            }\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all data.\"\"\"\n        with self._connection() as conn:\n            conn.execute(\"DELETE FROM edges\")\n            conn.execute(\"DELETE FROM token_index\") # Explicit, though cascade handles it\n            conn.execute(\"DELETE FROM nodes\")\n            conn.execute(\"DELETE FROM scan_metadata\")\n\n    def close(self) -&gt; None:\n        \"\"\"Close connections if needed.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.SQLiteStorage-functions","title":"Functions","text":""},{"location":"reference/api/core/storage/#jnkn.core.storage.SQLiteStorage.clear","title":"<code>clear()</code>","text":"<p>Clear all data.</p> Source code in <code>src/jnkn/core/storage/sqlite.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all data.\"\"\"\n    with self._connection() as conn:\n        conn.execute(\"DELETE FROM edges\")\n        conn.execute(\"DELETE FROM token_index\") # Explicit, though cascade handles it\n        conn.execute(\"DELETE FROM nodes\")\n        conn.execute(\"DELETE FROM scan_metadata\")\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.SQLiteStorage.close","title":"<code>close()</code>","text":"<p>Close connections if needed.</p> Source code in <code>src/jnkn/core/storage/sqlite.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close connections if needed.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.SQLiteStorage.get_stats","title":"<code>get_stats()</code>","text":"<p>Get storage statistics.</p> Source code in <code>src/jnkn/core/storage/sqlite.py</code> <pre><code>def get_stats(self) -&gt; Dict[str, Any]:\n    \"\"\"Get storage statistics.\"\"\"\n    with self._connection() as conn:\n        node_count = conn.execute(\"SELECT COUNT(*) as c FROM nodes\").fetchone()[\"c\"]\n        edge_count = conn.execute(\"SELECT COUNT(*) as c FROM edges\").fetchone()[\"c\"]\n        file_count = conn.execute(\"SELECT COUNT(*) as c FROM scan_metadata\").fetchone()[\"c\"]\n        token_count = conn.execute(\"SELECT COUNT(*) as c FROM token_index\").fetchone()[\"c\"]\n\n        type_rows = conn.execute(\n            \"SELECT type, COUNT(*) as c FROM nodes GROUP BY type\"\n        ).fetchall()\n\n        edge_type_rows = conn.execute(\n            \"SELECT type, COUNT(*) as c FROM edges GROUP BY type\"\n        ).fetchall()\n\n        return {\n            \"schema_version\": self._get_schema_version_internal(conn),\n            \"total_nodes\": node_count,\n            \"total_edges\": edge_count,\n            \"tracked_files\": file_count,\n            \"indexed_tokens\": token_count,\n            \"nodes_by_type\": {row[\"type\"]: row[\"c\"] for row in type_rows},\n            \"edges_by_type\": {row[\"type\"]: row[\"c\"] for row in edge_type_rows},\n            \"db_size_bytes\": self.db_path.stat().st_size if self.db_path.exists() else 0,\n        }\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.SQLiteStorage.load_all_edges","title":"<code>load_all_edges(min_confidence=0.0)</code>","text":"<p>Load all edges, optionally filtering by confidence. Uses the high_confidence_edges view if threshold &gt;= 0.7.</p> Source code in <code>src/jnkn/core/storage/sqlite.py</code> <pre><code>def load_all_edges(self, min_confidence: float = 0.0) -&gt; List[Edge]:\n    \"\"\"\n    Load all edges, optionally filtering by confidence.\n    Uses the high_confidence_edges view if threshold &gt;= 0.7.\n    \"\"\"\n    table = \"high_confidence_edges\" if min_confidence &gt;= 0.7 else \"edges\"\n    query = f\"SELECT * FROM {table}\"\n    params = []\n\n    if 0.0 &lt; min_confidence &lt; 0.7:\n        query += \" WHERE confidence &gt;= ?\"\n        params.append(min_confidence)\n\n    edges = []\n    with self._connection() as conn:\n        for row in conn.execute(query, params).fetchall():\n            try:\n                edges.append(self._row_to_edge(row))\n            except Exception:\n                pass\n    return edges\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.SQLiteStorage.load_graph","title":"<code>load_graph()</code>","text":"<p>Hydrate a full DependencyGraph from storage.</p> <p>Optimized to load token index from DB rather than rebuilding it.</p> Source code in <code>src/jnkn/core/storage/sqlite.py</code> <pre><code>def load_graph(self) -&gt; DependencyGraph:\n    \"\"\"\n    Hydrate a full DependencyGraph from storage.\n\n    Optimized to load token index from DB rather than rebuilding it.\n    \"\"\"\n    graph = DependencyGraph()\n\n    # Load nodes\n    all_nodes = self.load_all_nodes()\n    for node in all_nodes:\n        graph.add_node(node)\n\n    # Load edges\n    all_edges = self.load_all_edges()\n    for edge in all_edges:\n        graph.add_edge(edge)\n\n    # Note: DependencyGraph.add_node automatically updates the in-memory token index.\n    # Since we just rebuilt the graph object from scratch, its in-memory TokenIndex \n    # is already populated by `add_node`. \n    # We don't need to manually SELECT FROM token_index unless we were doing\n    # a \"lazy load\" graph that didn't hold everything in memory.\n    # However, `token_index` table is crucial for specific SQL-based lookups \n    # (e.g. stitching optimization) without loading the full graph.\n\n    return graph\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.SQLiteStorage.query_ancestors","title":"<code>query_ancestors(node_id, max_depth=-1)</code>","text":"<p>Query all ancestors using recursive CTE.</p> Source code in <code>src/jnkn/core/storage/sqlite.py</code> <pre><code>def query_ancestors(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n    \"\"\"Query all ancestors using recursive CTE.\"\"\"\n    with self._connection() as conn:\n        if max_depth &lt; 0:\n            rows = conn.execute(\"\"\"\n                WITH RECURSIVE ancestors AS (\n                    SELECT source_id as id, 1 as depth\n                    FROM edges WHERE target_id = ?\n                    UNION\n                    SELECT e.source_id, a.depth + 1\n                    FROM edges e JOIN ancestors a ON e.target_id = a.id\n                )\n                SELECT DISTINCT id FROM ancestors\n            \"\"\", (node_id,)).fetchall()\n        else:\n            rows = conn.execute(\"\"\"\n                WITH RECURSIVE ancestors AS (\n                    SELECT source_id as id, 1 as depth\n                    FROM edges WHERE target_id = ?\n                    UNION\n                    SELECT e.source_id, a.depth + 1\n                    FROM edges e JOIN ancestors a ON e.target_id = a.id\n                    WHERE a.depth &lt; ?\n                )\n                SELECT DISTINCT id FROM ancestors\n            \"\"\", (node_id, max_depth)).fetchall()\n        return [row[\"id\"] for row in rows]\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.SQLiteStorage.query_descendants","title":"<code>query_descendants(node_id, max_depth=-1)</code>","text":"<p>Query all descendants using recursive CTE.</p> Source code in <code>src/jnkn/core/storage/sqlite.py</code> <pre><code>def query_descendants(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n    \"\"\"Query all descendants using recursive CTE.\"\"\"\n    with self._connection() as conn:\n        if max_depth &lt; 0:\n            rows = conn.execute(\"\"\"\n                WITH RECURSIVE descendants AS (\n                    SELECT target_id as id, 1 as depth\n                    FROM edges WHERE source_id = ?\n                    UNION\n                    SELECT e.target_id, d.depth + 1\n                    FROM edges e JOIN descendants d ON e.source_id = d.id\n                )\n                SELECT DISTINCT id FROM descendants\n            \"\"\", (node_id,)).fetchall()\n        else:\n            rows = conn.execute(\"\"\"\n                WITH RECURSIVE descendants AS (\n                    SELECT target_id as id, 1 as depth\n                    FROM edges WHERE source_id = ?\n                    UNION\n                    SELECT e.target_id, d.depth + 1\n                    FROM edges e JOIN descendants d ON e.source_id = d.id\n                    WHERE d.depth &lt; ?\n                )\n                SELECT DISTINCT id FROM descendants\n            \"\"\", (node_id, max_depth)).fetchall()\n        return [row[\"id\"] for row in rows]\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.SQLiteStorage.save_node","title":"<code>save_node(node)</code>","text":"<p>Persist a single node.</p> Source code in <code>src/jnkn/core/storage/sqlite.py</code> <pre><code>def save_node(self, node: Node) -&gt; None:\n    \"\"\"Persist a single node.\"\"\"\n    self.save_nodes_batch([node])\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.SQLiteStorage.save_nodes_batch","title":"<code>save_nodes_batch(nodes)</code>","text":"<p>Persist multiple nodes and their tokens in a single transaction.</p> Source code in <code>src/jnkn/core/storage/sqlite.py</code> <pre><code>def save_nodes_batch(self, nodes: List[Node]) -&gt; int:\n    \"\"\"Persist multiple nodes and their tokens in a single transaction.\"\"\"\n    if not nodes:\n        return 0\n\n    with self._connection() as conn:\n        # 1. Upsert Nodes\n        conn.executemany(\"\"\"\n            INSERT OR REPLACE INTO nodes \n            (id, name, type, path, language, file_hash, tokens, metadata, created_at)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\", [\n            (n.id, n.name, n.type.value, n.path, n.language, n.file_hash,\n             json.dumps(n.tokens), json.dumps(n.metadata), n.created_at.isoformat())\n            for n in nodes\n        ])\n\n        # 2. Update Token Index\n        # Delete existing tokens for these nodes first to ensure clean state on update\n        node_ids = [n.id for n in nodes]\n        placeholders = \",\".join(\"?\" * len(node_ids))\n        conn.execute(f\"DELETE FROM token_index WHERE node_id IN ({placeholders})\", node_ids)\n\n        # Flatten tokens for batch insert\n        token_entries = []\n        for node in nodes:\n            # Use tokens from the object, defaulting to computed ones if missing\n            # (though model_post_init handles this usually)\n            tokens = node.tokens or []\n            for token in tokens:\n                token_entries.append((token, node.id))\n\n        if token_entries:\n            conn.executemany(\"\"\"\n                INSERT OR IGNORE INTO token_index (token, node_id)\n                VALUES (?, ?)\n            \"\"\", token_entries)\n\n    return len(nodes)\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter","title":"<code>StorageAdapter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract interface for persistence strategies.</p> <p>Implementations must provide: - Node and edge persistence - Graph hydration (loading) - Incremental scan support via file metadata - Batch operations for performance</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>class StorageAdapter(ABC):\n    \"\"\"\n    Abstract interface for persistence strategies.\n\n    Implementations must provide:\n    - Node and edge persistence\n    - Graph hydration (loading)\n    - Incremental scan support via file metadata\n    - Batch operations for performance\n    \"\"\"\n\n    @abstractmethod\n    def save_node(self, node: Node) -&gt; None:\n        \"\"\"Persist a single node.\"\"\"\n        pass\n\n    @abstractmethod\n    def save_nodes_batch(self, nodes: List[Node]) -&gt; int:\n        \"\"\"Persist multiple nodes in a single transaction.\"\"\"\n        pass\n\n    @abstractmethod\n    def save_edge(self, edge: Edge) -&gt; None:\n        \"\"\"Persist a single edge.\"\"\"\n        pass\n\n    @abstractmethod\n    def save_edges_batch(self, edges: List[Edge]) -&gt; int:\n        \"\"\"Persist multiple edges in a single transaction.\"\"\"\n        pass\n\n    @abstractmethod\n    def load_node(self, node_id: str) -&gt; Optional[Node]:\n        \"\"\"Load a node by ID.\"\"\"\n        pass\n\n    @abstractmethod\n    def load_all_nodes(self) -&gt; List[Node]:\n        \"\"\"Load all nodes from storage.\"\"\"\n        pass\n\n    @abstractmethod\n    def load_all_edges(self) -&gt; List[Edge]:\n        \"\"\"Load all edges from storage.\"\"\"\n        pass\n\n    @abstractmethod\n    def load_graph(self) -&gt; DependencyGraph:\n        \"\"\"Hydrate a full DependencyGraph from storage.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_node(self, node_id: str) -&gt; bool:\n        \"\"\"Delete a node and its associated edges.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_nodes_by_file(self, file_path: str) -&gt; int:\n        \"\"\"Delete all nodes originating from a file.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_edges_by_source(self, source_id: str) -&gt; int:\n        \"\"\"Delete all edges from a source node.\"\"\"\n        pass\n\n    @abstractmethod\n    def save_scan_metadata(self, metadata: ScanMetadata) -&gt; None:\n        \"\"\"Save file scan metadata for incremental scanning.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_scan_metadata(self, file_path: str) -&gt; Optional[ScanMetadata]:\n        \"\"\"Get scan metadata for a file.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_scan_metadata(self) -&gt; List[ScanMetadata]:\n        \"\"\"Get scan metadata for all files.\"\"\"\n        pass\n\n    @abstractmethod\n    def delete_scan_metadata(self, file_path: str) -&gt; bool:\n        \"\"\"Delete scan metadata for a file.\"\"\"\n        pass\n\n    @abstractmethod\n    def query_descendants(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n        \"\"\"Query all descendants of a node.\"\"\"\n        pass\n\n    @abstractmethod\n    def query_ancestors(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n        \"\"\"Query all ancestors of a node.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_schema_version(self) -&gt; int:\n        \"\"\"Get the current schema version.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get storage statistics.\"\"\"\n        pass\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"Clear all data from storage.\"\"\"\n        pass\n\n    @abstractmethod\n    def close(self) -&gt; None:\n        \"\"\"Close any open connections.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter-functions","title":"Functions","text":""},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Clear all data from storage.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"Clear all data from storage.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Close any open connections.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Close any open connections.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.delete_edges_by_source","title":"<code>delete_edges_by_source(source_id)</code>  <code>abstractmethod</code>","text":"<p>Delete all edges from a source node.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef delete_edges_by_source(self, source_id: str) -&gt; int:\n    \"\"\"Delete all edges from a source node.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.delete_node","title":"<code>delete_node(node_id)</code>  <code>abstractmethod</code>","text":"<p>Delete a node and its associated edges.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef delete_node(self, node_id: str) -&gt; bool:\n    \"\"\"Delete a node and its associated edges.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.delete_nodes_by_file","title":"<code>delete_nodes_by_file(file_path)</code>  <code>abstractmethod</code>","text":"<p>Delete all nodes originating from a file.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef delete_nodes_by_file(self, file_path: str) -&gt; int:\n    \"\"\"Delete all nodes originating from a file.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.delete_scan_metadata","title":"<code>delete_scan_metadata(file_path)</code>  <code>abstractmethod</code>","text":"<p>Delete scan metadata for a file.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef delete_scan_metadata(self, file_path: str) -&gt; bool:\n    \"\"\"Delete scan metadata for a file.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.get_all_scan_metadata","title":"<code>get_all_scan_metadata()</code>  <code>abstractmethod</code>","text":"<p>Get scan metadata for all files.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef get_all_scan_metadata(self) -&gt; List[ScanMetadata]:\n    \"\"\"Get scan metadata for all files.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.get_scan_metadata","title":"<code>get_scan_metadata(file_path)</code>  <code>abstractmethod</code>","text":"<p>Get scan metadata for a file.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef get_scan_metadata(self, file_path: str) -&gt; Optional[ScanMetadata]:\n    \"\"\"Get scan metadata for a file.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.get_schema_version","title":"<code>get_schema_version()</code>  <code>abstractmethod</code>","text":"<p>Get the current schema version.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef get_schema_version(self) -&gt; int:\n    \"\"\"Get the current schema version.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.get_stats","title":"<code>get_stats()</code>  <code>abstractmethod</code>","text":"<p>Get storage statistics.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef get_stats(self) -&gt; Dict[str, Any]:\n    \"\"\"Get storage statistics.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.load_all_edges","title":"<code>load_all_edges()</code>  <code>abstractmethod</code>","text":"<p>Load all edges from storage.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef load_all_edges(self) -&gt; List[Edge]:\n    \"\"\"Load all edges from storage.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.load_all_nodes","title":"<code>load_all_nodes()</code>  <code>abstractmethod</code>","text":"<p>Load all nodes from storage.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef load_all_nodes(self) -&gt; List[Node]:\n    \"\"\"Load all nodes from storage.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.load_graph","title":"<code>load_graph()</code>  <code>abstractmethod</code>","text":"<p>Hydrate a full DependencyGraph from storage.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef load_graph(self) -&gt; DependencyGraph:\n    \"\"\"Hydrate a full DependencyGraph from storage.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.load_node","title":"<code>load_node(node_id)</code>  <code>abstractmethod</code>","text":"<p>Load a node by ID.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef load_node(self, node_id: str) -&gt; Optional[Node]:\n    \"\"\"Load a node by ID.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.query_ancestors","title":"<code>query_ancestors(node_id, max_depth=-1)</code>  <code>abstractmethod</code>","text":"<p>Query all ancestors of a node.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef query_ancestors(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n    \"\"\"Query all ancestors of a node.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.query_descendants","title":"<code>query_descendants(node_id, max_depth=-1)</code>  <code>abstractmethod</code>","text":"<p>Query all descendants of a node.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef query_descendants(self, node_id: str, max_depth: int = -1) -&gt; List[str]:\n    \"\"\"Query all descendants of a node.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.save_edge","title":"<code>save_edge(edge)</code>  <code>abstractmethod</code>","text":"<p>Persist a single edge.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef save_edge(self, edge: Edge) -&gt; None:\n    \"\"\"Persist a single edge.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.save_edges_batch","title":"<code>save_edges_batch(edges)</code>  <code>abstractmethod</code>","text":"<p>Persist multiple edges in a single transaction.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef save_edges_batch(self, edges: List[Edge]) -&gt; int:\n    \"\"\"Persist multiple edges in a single transaction.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.save_node","title":"<code>save_node(node)</code>  <code>abstractmethod</code>","text":"<p>Persist a single node.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef save_node(self, node: Node) -&gt; None:\n    \"\"\"Persist a single node.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.save_nodes_batch","title":"<code>save_nodes_batch(nodes)</code>  <code>abstractmethod</code>","text":"<p>Persist multiple nodes in a single transaction.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef save_nodes_batch(self, nodes: List[Node]) -&gt; int:\n    \"\"\"Persist multiple nodes in a single transaction.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/storage/#jnkn.core.storage.StorageAdapter.save_scan_metadata","title":"<code>save_scan_metadata(metadata)</code>  <code>abstractmethod</code>","text":"<p>Save file scan metadata for incremental scanning.</p> Source code in <code>src/jnkn/core/storage/base.py</code> <pre><code>@abstractmethod\ndef save_scan_metadata(self, metadata: ScanMetadata) -&gt; None:\n    \"\"\"Save file scan metadata for incremental scanning.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/core/types/","title":"Core Types","text":""},{"location":"reference/api/core/types/#jnkn.core.types","title":"<code>jnkn.core.types</code>","text":"<p>Core type definitions for jnkn.</p> <p>This module defines the fundamental data structures used throughout the system: - NodeType: Categories of nodes in the dependency graph - RelationshipType: Types of edges between nodes - Node: Represents any entity (file, resource, env var, etc.) - Edge: Represents a directed relationship between nodes - MatchResult: Captures stitching match details with confidence - ScanMetadata: Tracks file state for incremental scanning</p>"},{"location":"reference/api/core/types/#jnkn.core.types-classes","title":"Classes","text":""},{"location":"reference/api/core/types/#jnkn.core.types.Edge","title":"<code>Edge</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Directed relationship between two Nodes.</p> <p>Represents a dependency: source_id depends on or references target_id. The direction convention is: - source_id: The node that HAS the dependency - target_id: The node that IS the dependency</p> If Python code reads an env var, the edge is: <p>source_id=\"file://app.py\" -&gt; target_id=\"env:DB_HOST\" type=READS</p> For infrastructure providing values <p>source_id=\"infra:db_host_output\" -&gt; target_id=\"env:DB_HOST\" type=PROVIDES</p> <p>Attributes:</p> Name Type Description <code>source_id</code> <code>str</code> <p>ID of the source node</p> <code>target_id</code> <code>str</code> <p>ID of the target node  </p> <code>type</code> <code>RelationshipType</code> <p>Relationship category from RelationshipType</p> <code>confidence</code> <code>float</code> <p>Match confidence score (0.0-1.0)</p> <code>match_strategy</code> <code>Optional[MatchStrategy]</code> <p>How this edge was discovered (for stitched edges)</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Additional context (matched_tokens, explanation, rule name)</p> <code>created_at</code> <code>datetime</code> <p>Timestamp of edge creation</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>class Edge(BaseModel):\n    \"\"\"\n    Directed relationship between two Nodes.\n\n    Represents a dependency: source_id depends on or references target_id.\n    The direction convention is:\n    - source_id: The node that HAS the dependency\n    - target_id: The node that IS the dependency\n\n    Example: If Python code reads an env var, the edge is:\n        source_id=\"file://app.py\" -&gt; target_id=\"env:DB_HOST\"\n        type=READS\n\n    For infrastructure providing values:\n        source_id=\"infra:db_host_output\" -&gt; target_id=\"env:DB_HOST\"\n        type=PROVIDES\n\n    Attributes:\n        source_id: ID of the source node\n        target_id: ID of the target node  \n        type: Relationship category from RelationshipType\n        confidence: Match confidence score (0.0-1.0)\n        match_strategy: How this edge was discovered (for stitched edges)\n        metadata: Additional context (matched_tokens, explanation, rule name)\n        created_at: Timestamp of edge creation\n    \"\"\"\n    source_id: str\n    target_id: str\n    type: RelationshipType\n    confidence: float = 1.0\n    match_strategy: Optional[MatchStrategy] = None\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n\n    def is_high_confidence(self, threshold: float = 0.8) -&gt; bool:\n        \"\"\"Check if this edge meets high confidence threshold.\"\"\"\n        return self.confidence &gt;= threshold\n\n    def is_stitched(self) -&gt; bool:\n        \"\"\"Check if this edge was created by stitching (vs direct parsing).\"\"\"\n        return self.match_strategy is not None\n\n    def get_matched_tokens(self) -&gt; List[str]:\n        \"\"\"Extract matched tokens from metadata if present.\"\"\"\n        return self.metadata.get(\"matched_tokens\", [])\n\n    def get_explanation(self) -&gt; str:\n        \"\"\"Extract explanation from metadata if present.\"\"\"\n        return self.metadata.get(\"explanation\", \"\")\n\n    def get_rule_name(self) -&gt; Optional[str]:\n        \"\"\"Extract the stitching rule name that created this edge.\"\"\"\n        return self.metadata.get(\"rule\")\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.Edge-functions","title":"Functions","text":""},{"location":"reference/api/core/types/#jnkn.core.types.Edge.get_explanation","title":"<code>get_explanation()</code>","text":"<p>Extract explanation from metadata if present.</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>def get_explanation(self) -&gt; str:\n    \"\"\"Extract explanation from metadata if present.\"\"\"\n    return self.metadata.get(\"explanation\", \"\")\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.Edge.get_matched_tokens","title":"<code>get_matched_tokens()</code>","text":"<p>Extract matched tokens from metadata if present.</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>def get_matched_tokens(self) -&gt; List[str]:\n    \"\"\"Extract matched tokens from metadata if present.\"\"\"\n    return self.metadata.get(\"matched_tokens\", [])\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.Edge.get_rule_name","title":"<code>get_rule_name()</code>","text":"<p>Extract the stitching rule name that created this edge.</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>def get_rule_name(self) -&gt; Optional[str]:\n    \"\"\"Extract the stitching rule name that created this edge.\"\"\"\n    return self.metadata.get(\"rule\")\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.Edge.is_high_confidence","title":"<code>is_high_confidence(threshold=0.8)</code>","text":"<p>Check if this edge meets high confidence threshold.</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>def is_high_confidence(self, threshold: float = 0.8) -&gt; bool:\n    \"\"\"Check if this edge meets high confidence threshold.\"\"\"\n    return self.confidence &gt;= threshold\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.Edge.is_stitched","title":"<code>is_stitched()</code>","text":"<p>Check if this edge was created by stitching (vs direct parsing).</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>def is_stitched(self) -&gt; bool:\n    \"\"\"Check if this edge was created by stitching (vs direct parsing).\"\"\"\n    return self.match_strategy is not None\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.MatchResult","title":"<code>MatchResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result of a stitching match attempt.</p> <p>Captures details about why two nodes were linked, enabling explainability and debugging of the matching process.</p> <p>Attributes:</p> Name Type Description <code>source_node</code> <code>str</code> <p>ID of the source node</p> <code>target_node</code> <code>str</code> <p>ID of the target node</p> <code>strategy</code> <code>MatchStrategy</code> <p>Which matching strategy succeeded</p> <code>confidence</code> <code>float</code> <p>Calculated confidence score</p> <code>matched_tokens</code> <code>List[str]</code> <p>Which tokens contributed to the match</p> <code>explanation</code> <code>str</code> <p>Human-readable description of why this matched</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>class MatchResult(BaseModel):\n    \"\"\"\n    Result of a stitching match attempt.\n\n    Captures details about why two nodes were linked, enabling\n    explainability and debugging of the matching process.\n\n    Attributes:\n        source_node: ID of the source node\n        target_node: ID of the target node\n        strategy: Which matching strategy succeeded\n        confidence: Calculated confidence score\n        matched_tokens: Which tokens contributed to the match\n        explanation: Human-readable description of why this matched\n    \"\"\"\n    source_node: str\n    target_node: str\n    strategy: MatchStrategy\n    confidence: float\n    matched_tokens: List[str] = Field(default_factory=list)\n    explanation: str = \"\"\n\n    def to_edge(self, relationship_type: RelationshipType, rule_name: str = \"\") -&gt; Edge:\n        \"\"\"\n        Convert this match result to an Edge.\n\n        Args:\n            relationship_type: The type of relationship this represents\n            rule_name: Name of the stitching rule that created this match\n\n        Returns:\n            An Edge instance with metadata populated from this result\n        \"\"\"\n        return Edge(\n            source_id=self.source_node,\n            target_id=self.target_node,\n            type=relationship_type,\n            confidence=self.confidence,\n            match_strategy=self.strategy,\n            metadata={\n                \"matched_tokens\": self.matched_tokens,\n                \"explanation\": self.explanation,\n                \"rule\": rule_name,\n            }\n        )\n\n    def is_better_than(self, other: \"MatchResult\") -&gt; bool:\n        \"\"\"\n        Compare two match results to determine which is stronger.\n\n        Used when multiple potential matches exist for the same source.\n        \"\"\"\n        if self.confidence != other.confidence:\n            return self.confidence &gt; other.confidence\n        # Tie-breaker: prefer more matched tokens\n        return len(self.matched_tokens) &gt; len(other.matched_tokens)\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.MatchResult-functions","title":"Functions","text":""},{"location":"reference/api/core/types/#jnkn.core.types.MatchResult.is_better_than","title":"<code>is_better_than(other)</code>","text":"<p>Compare two match results to determine which is stronger.</p> <p>Used when multiple potential matches exist for the same source.</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>def is_better_than(self, other: \"MatchResult\") -&gt; bool:\n    \"\"\"\n    Compare two match results to determine which is stronger.\n\n    Used when multiple potential matches exist for the same source.\n    \"\"\"\n    if self.confidence != other.confidence:\n        return self.confidence &gt; other.confidence\n    # Tie-breaker: prefer more matched tokens\n    return len(self.matched_tokens) &gt; len(other.matched_tokens)\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.MatchResult.to_edge","title":"<code>to_edge(relationship_type, rule_name='')</code>","text":"<p>Convert this match result to an Edge.</p> <p>Parameters:</p> Name Type Description Default <code>relationship_type</code> <code>RelationshipType</code> <p>The type of relationship this represents</p> required <code>rule_name</code> <code>str</code> <p>Name of the stitching rule that created this match</p> <code>''</code> <p>Returns:</p> Type Description <code>Edge</code> <p>An Edge instance with metadata populated from this result</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>def to_edge(self, relationship_type: RelationshipType, rule_name: str = \"\") -&gt; Edge:\n    \"\"\"\n    Convert this match result to an Edge.\n\n    Args:\n        relationship_type: The type of relationship this represents\n        rule_name: Name of the stitching rule that created this match\n\n    Returns:\n        An Edge instance with metadata populated from this result\n    \"\"\"\n    return Edge(\n        source_id=self.source_node,\n        target_id=self.target_node,\n        type=relationship_type,\n        confidence=self.confidence,\n        match_strategy=self.strategy,\n        metadata={\n            \"matched_tokens\": self.matched_tokens,\n            \"explanation\": self.explanation,\n            \"rule\": rule_name,\n        }\n    )\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.MatchStrategy","title":"<code>MatchStrategy</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Strategies used for fuzzy matching in stitching.</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>class MatchStrategy(StrEnum):\n    \"\"\"Strategies used for fuzzy matching in stitching.\"\"\"\n    EXACT = \"exact\"\n    NORMALIZED = \"normalized\"\n    TOKEN_OVERLAP = \"token_overlap\"\n    SUFFIX = \"suffix\"\n    PREFIX = \"prefix\"\n    CONTAINS = \"contains\"\n    SEMANTIC = \"semantic\"\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.Node","title":"<code>Node</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Universal Unit of Analysis.</p> <p>Represents any entity in the dependency graph: files, functions, infrastructure resources, database tables, environment variables, etc.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier (e.g., \"env:DB_HOST\", \"infra:aws_db_instance.main\")</p> <code>name</code> <code>str</code> <p>Human-readable name</p> <code>type</code> <code>NodeType</code> <p>Category from NodeType enum</p> <code>path</code> <code>Optional[str]</code> <p>File path where this node was discovered</p> <code>language</code> <code>Optional[str]</code> <p>Source language (python, terraform, kubernetes, etc.)</p> <code>file_hash</code> <code>Optional[str]</code> <p>Hash of source file for incremental scanning</p> <code>tokens</code> <code>List[str]</code> <p>Tokenized name for fuzzy matching</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Extensible key-value storage for parser-specific data</p> <code>created_at</code> <code>datetime</code> <p>Timestamp of node creation</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>class Node(BaseModel):\n    \"\"\"\n    Universal Unit of Analysis.\n\n    Represents any entity in the dependency graph: files, functions,\n    infrastructure resources, database tables, environment variables, etc.\n\n    Attributes:\n        id: Unique identifier (e.g., \"env:DB_HOST\", \"infra:aws_db_instance.main\")\n        name: Human-readable name\n        type: Category from NodeType enum\n        path: File path where this node was discovered\n        language: Source language (python, terraform, kubernetes, etc.)\n        file_hash: Hash of source file for incremental scanning\n        tokens: Tokenized name for fuzzy matching\n        metadata: Extensible key-value storage for parser-specific data\n        created_at: Timestamp of node creation\n    \"\"\"\n    id: str\n    name: str\n    type: NodeType\n    path: Optional[str] = None\n    language: Optional[str] = None\n    file_hash: Optional[str] = None\n    tokens: List[str] = Field(default_factory=list)\n    metadata: Dict[str, Any] = Field(default_factory=dict)\n    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n\n    def model_post_init(self, __context) -&gt; None:\n        \"\"\"Generate tokens from name if not provided.\"\"\"\n        if not self.tokens and self.name:\n            object.__setattr__(self, 'tokens', self._tokenize(self.name))\n\n    @staticmethod\n    def _tokenize(name: str) -&gt; List[str]:\n        \"\"\"\n        Split name into normalized tokens for matching.\n\n        Handles common naming conventions across languages:\n        - SCREAMING_SNAKE_CASE (env vars)\n        - snake_case (Python, Terraform)\n        - kebab-case (Kubernetes)\n        - dot.notation (Java packages, Terraform resources)\n        - path/separators (file paths)\n\n        Examples:\n            \"PAYMENT_DB_HOST\" -&gt; [\"payment\", \"db\", \"host\"]\n            \"aws_db_instance.main\" -&gt; [\"aws\", \"db\", \"instance\", \"main\"]\n            \"my-kubernetes-service\" -&gt; [\"my\", \"kubernetes\", \"service\"]\n        \"\"\"\n        normalized = name.lower()\n        for sep in [\"_\", \".\", \"-\", \"/\", \":\"]:\n            normalized = normalized.replace(sep, \" \")\n        return [t.strip() for t in normalized.split() if t.strip()]\n\n    def with_metadata(self, **kwargs) -&gt; \"Node\":\n        \"\"\"\n        Return a new Node with additional metadata merged in.\n\n        Useful for adding parser-specific data without mutation.\n\n        Example:\n            node = node.with_metadata(line_number=42, column=10)\n        \"\"\"\n        merged = {**self.metadata, **kwargs}\n        return self.model_copy(update={\"metadata\": merged})\n\n    def matches_tokens(self, other_tokens: List[str], min_overlap: int = 2) -&gt; bool:\n        \"\"\"\n        Check if this node's tokens overlap sufficiently with another set.\n\n        Args:\n            other_tokens: Tokens to compare against\n            min_overlap: Minimum number of shared tokens required\n\n        Returns:\n            True if overlap meets threshold\n        \"\"\"\n        overlap = set(self.tokens) &amp; set(other_tokens)\n        return len(overlap) &gt;= min_overlap\n\n    class Config:\n        frozen = True\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.Node-functions","title":"Functions","text":""},{"location":"reference/api/core/types/#jnkn.core.types.Node.matches_tokens","title":"<code>matches_tokens(other_tokens, min_overlap=2)</code>","text":"<p>Check if this node's tokens overlap sufficiently with another set.</p> <p>Parameters:</p> Name Type Description Default <code>other_tokens</code> <code>List[str]</code> <p>Tokens to compare against</p> required <code>min_overlap</code> <code>int</code> <p>Minimum number of shared tokens required</p> <code>2</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if overlap meets threshold</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>def matches_tokens(self, other_tokens: List[str], min_overlap: int = 2) -&gt; bool:\n    \"\"\"\n    Check if this node's tokens overlap sufficiently with another set.\n\n    Args:\n        other_tokens: Tokens to compare against\n        min_overlap: Minimum number of shared tokens required\n\n    Returns:\n        True if overlap meets threshold\n    \"\"\"\n    overlap = set(self.tokens) &amp; set(other_tokens)\n    return len(overlap) &gt;= min_overlap\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.Node.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Generate tokens from name if not provided.</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"Generate tokens from name if not provided.\"\"\"\n    if not self.tokens and self.name:\n        object.__setattr__(self, 'tokens', self._tokenize(self.name))\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.Node.with_metadata","title":"<code>with_metadata(**kwargs)</code>","text":"<p>Return a new Node with additional metadata merged in.</p> <p>Useful for adding parser-specific data without mutation.</p> Example <p>node = node.with_metadata(line_number=42, column=10)</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>def with_metadata(self, **kwargs) -&gt; \"Node\":\n    \"\"\"\n    Return a new Node with additional metadata merged in.\n\n    Useful for adding parser-specific data without mutation.\n\n    Example:\n        node = node.with_metadata(line_number=42, column=10)\n    \"\"\"\n    merged = {**self.metadata, **kwargs}\n    return self.model_copy(update={\"metadata\": merged})\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.NodeType","title":"<code>NodeType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Categories of nodes in the dependency graph.</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>class NodeType(StrEnum):\n    \"\"\"Categories of nodes in the dependency graph.\"\"\"\n    CODE_FILE = \"code_file\"\n    CODE_ENTITY = \"code_entity\"\n    INFRA_RESOURCE = \"infra_resource\"\n    INFRA_MODULE = \"infra_module\"\n    DATA_ASSET = \"data_asset\"\n    ENV_VAR = \"env_var\"\n    CONFIG_KEY = \"config_key\"\n    SECRET = \"secret\"\n    UNKNOWN = \"unknown\"\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.RelationshipType","title":"<code>RelationshipType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Types of relationships between nodes.</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>class RelationshipType(StrEnum):\n    \"\"\"Types of relationships between nodes.\"\"\"\n    CONTAINS = \"contains\"\n    IMPORTS = \"imports\"\n    EXTENDS = \"extends\"\n    CALLS = \"calls\"\n    READS = \"reads\"\n    WRITES = \"writes\"\n    PROVISIONS = \"provisions\"\n    CONFIGURES = \"configures\"\n    DEPENDS_ON = \"depends_on\"\n    PROVIDES = \"provides\"\n    CONSUMES = \"consumes\"\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.ScanMetadata","title":"<code>ScanMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for tracking file state in incremental scanning.</p> <p>Enables jnkn to skip unchanged files on subsequent scans, dramatically improving performance for large codebases.</p> <p>Attributes:</p> Name Type Description <code>file_path</code> <code>str</code> <p>Absolute or relative path to the file</p> <code>file_hash</code> <code>str</code> <p>Hash of file contents for change detection</p> <code>last_scanned</code> <code>datetime</code> <p>When this file was last processed</p> <code>node_count</code> <code>int</code> <p>Number of nodes extracted from this file</p> <code>edge_count</code> <code>int</code> <p>Number of edges extracted from this file</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>class ScanMetadata(BaseModel):\n    \"\"\"\n    Metadata for tracking file state in incremental scanning.\n\n    Enables jnkn to skip unchanged files on subsequent scans,\n    dramatically improving performance for large codebases.\n\n    Attributes:\n        file_path: Absolute or relative path to the file\n        file_hash: Hash of file contents for change detection\n        last_scanned: When this file was last processed\n        node_count: Number of nodes extracted from this file\n        edge_count: Number of edges extracted from this file\n    \"\"\"\n    file_path: str\n    file_hash: str\n    last_scanned: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))    \n    node_count: int = 0\n    edge_count: int = 0\n\n    @staticmethod\n    def compute_hash(file_path: str) -&gt; str:\n        \"\"\"\n        Compute hash of file contents for change detection.\n\n        Uses xxhash for speed if available, falls back to MD5.\n\n        Args:\n            file_path: Path to the file to hash\n\n        Returns:\n            Hex digest of file contents, or empty string on error\n        \"\"\"\n        try:\n            import xxhash\n            with open(file_path, \"rb\") as f:\n                return xxhash.xxh64(f.read()).hexdigest()\n        except ImportError:\n            # Fallback to MD5 if xxhash not installed\n            with open(file_path, \"rb\") as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            # File doesn't exist or can't be read\n            return \"\"\n\n    def is_stale(self, current_hash: str) -&gt; bool:\n        \"\"\"\n        Check if the file has changed since last scan.\n\n        Args:\n            current_hash: Hash of current file contents\n\n        Returns:\n            True if file has changed and needs re-scanning\n        \"\"\"\n        return self.file_hash != current_hash\n\n    @classmethod\n    def from_file(cls, file_path: str, node_count: int = 0, edge_count: int = 0) -&gt; \"ScanMetadata\":\n        \"\"\"\n        Create ScanMetadata from a file path.\n\n        Convenience method that computes the hash automatically.\n\n        Args:\n            file_path: Path to the file\n            node_count: Number of nodes extracted\n            edge_count: Number of edges extracted\n\n        Returns:\n            ScanMetadata instance with computed hash\n        \"\"\"\n        return cls(\n            file_path=file_path,\n            file_hash=cls.compute_hash(file_path),\n            node_count=node_count,\n            edge_count=edge_count,\n        )\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.ScanMetadata-functions","title":"Functions","text":""},{"location":"reference/api/core/types/#jnkn.core.types.ScanMetadata.compute_hash","title":"<code>compute_hash(file_path)</code>  <code>staticmethod</code>","text":"<p>Compute hash of file contents for change detection.</p> <p>Uses xxhash for speed if available, falls back to MD5.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file to hash</p> required <p>Returns:</p> Type Description <code>str</code> <p>Hex digest of file contents, or empty string on error</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>@staticmethod\ndef compute_hash(file_path: str) -&gt; str:\n    \"\"\"\n    Compute hash of file contents for change detection.\n\n    Uses xxhash for speed if available, falls back to MD5.\n\n    Args:\n        file_path: Path to the file to hash\n\n    Returns:\n        Hex digest of file contents, or empty string on error\n    \"\"\"\n    try:\n        import xxhash\n        with open(file_path, \"rb\") as f:\n            return xxhash.xxh64(f.read()).hexdigest()\n    except ImportError:\n        # Fallback to MD5 if xxhash not installed\n        with open(file_path, \"rb\") as f:\n            return hashlib.md5(f.read()).hexdigest()\n    except Exception:\n        # File doesn't exist or can't be read\n        return \"\"\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.ScanMetadata.from_file","title":"<code>from_file(file_path, node_count=0, edge_count=0)</code>  <code>classmethod</code>","text":"<p>Create ScanMetadata from a file path.</p> <p>Convenience method that computes the hash automatically.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file</p> required <code>node_count</code> <code>int</code> <p>Number of nodes extracted</p> <code>0</code> <code>edge_count</code> <code>int</code> <p>Number of edges extracted</p> <code>0</code> <p>Returns:</p> Type Description <code>ScanMetadata</code> <p>ScanMetadata instance with computed hash</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>@classmethod\ndef from_file(cls, file_path: str, node_count: int = 0, edge_count: int = 0) -&gt; \"ScanMetadata\":\n    \"\"\"\n    Create ScanMetadata from a file path.\n\n    Convenience method that computes the hash automatically.\n\n    Args:\n        file_path: Path to the file\n        node_count: Number of nodes extracted\n        edge_count: Number of edges extracted\n\n    Returns:\n        ScanMetadata instance with computed hash\n    \"\"\"\n    return cls(\n        file_path=file_path,\n        file_hash=cls.compute_hash(file_path),\n        node_count=node_count,\n        edge_count=edge_count,\n    )\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.ScanMetadata.is_stale","title":"<code>is_stale(current_hash)</code>","text":"<p>Check if the file has changed since last scan.</p> <p>Parameters:</p> Name Type Description Default <code>current_hash</code> <code>str</code> <p>Hash of current file contents</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file has changed and needs re-scanning</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>def is_stale(self, current_hash: str) -&gt; bool:\n    \"\"\"\n    Check if the file has changed since last scan.\n\n    Args:\n        current_hash: Hash of current file contents\n\n    Returns:\n        True if file has changed and needs re-scanning\n    \"\"\"\n    return self.file_hash != current_hash\n</code></pre>"},{"location":"reference/api/core/types/#jnkn.core.types.SchemaVersion","title":"<code>SchemaVersion</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Database schema version for migrations.</p> <p>Stored in the schema_version table to track which migrations have been applied to the SQLite database.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>int</code> <p>Integer version number (monotonically increasing)</p> <code>applied_at</code> <code>datetime</code> <p>When this migration was applied</p> <code>description</code> <code>str</code> <p>Human-readable description of what changed</p> Source code in <code>src/jnkn/core/types.py</code> <pre><code>class SchemaVersion(BaseModel):\n    \"\"\"\n    Database schema version for migrations.\n\n    Stored in the schema_version table to track which migrations\n    have been applied to the SQLite database.\n\n    Attributes:\n        version: Integer version number (monotonically increasing)\n        applied_at: When this migration was applied\n        description: Human-readable description of what changed\n    \"\"\"\n    version: int\n    applied_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))\n    description: str = \"\"\n</code></pre>"},{"location":"reference/api/parsing/base/","title":"Base Parser","text":""},{"location":"reference/api/parsing/base/#jnkn.parsing.base","title":"<code>jnkn.parsing.base</code>","text":"<p>Base Parser Infrastructure.</p>"},{"location":"reference/api/parsing/base/#jnkn.parsing.base-classes","title":"Classes","text":""},{"location":"reference/api/parsing/base/#jnkn.parsing.base.BaseParser","title":"<code>BaseParser</code>","text":"<p>               Bases: <code>IParser</code>, <code>ABC</code></p> <p>Abstract Base Class for all language parsers. Enforces strict typing of return values.</p> Source code in <code>src/jnkn/parsing/base.py</code> <pre><code>class BaseParser(IParser, ABC):\n    \"\"\"\n    Abstract Base Class for all language parsers.\n    Enforces strict typing of return values.\n    \"\"\"\n\n    def __init__(self, context: ParserContext):\n        self.context = context\n\n    @abstractmethod\n    def can_parse(self, file_path: Path) -&gt; bool:\n        \"\"\"\n        Determine if this parser supports the given file.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def parse(self, file_path: Path, content: bytes) -&gt; List[Union[Node, Edge]]:\n        \"\"\"\n        Parse the file content.\n\n        Must return a list containing strictly:\n        - jnkn.core.types.Node objects\n        - jnkn.core.types.Edge objects\n        \"\"\"\n        pass\n\n    def _relativize(self, path: Path) -&gt; str:\n        \"\"\"Helper to get path relative to project root.\"\"\"\n        try:\n            return str(path.relative_to(self.context.root_dir))\n        except ValueError:\n            return str(path)\n</code></pre>"},{"location":"reference/api/parsing/base/#jnkn.parsing.base.BaseParser-functions","title":"Functions","text":""},{"location":"reference/api/parsing/base/#jnkn.parsing.base.BaseParser.can_parse","title":"<code>can_parse(file_path)</code>  <code>abstractmethod</code>","text":"<p>Determine if this parser supports the given file.</p> Source code in <code>src/jnkn/parsing/base.py</code> <pre><code>@abstractmethod\ndef can_parse(self, file_path: Path) -&gt; bool:\n    \"\"\"\n    Determine if this parser supports the given file.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/parsing/base/#jnkn.parsing.base.BaseParser.parse","title":"<code>parse(file_path, content)</code>  <code>abstractmethod</code>","text":"<p>Parse the file content.</p> <p>Must return a list containing strictly: - jnkn.core.types.Node objects - jnkn.core.types.Edge objects</p> Source code in <code>src/jnkn/parsing/base.py</code> <pre><code>@abstractmethod\ndef parse(self, file_path: Path, content: bytes) -&gt; List[Union[Node, Edge]]:\n    \"\"\"\n    Parse the file content.\n\n    Must return a list containing strictly:\n    - jnkn.core.types.Node objects\n    - jnkn.core.types.Edge objects\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/api/parsing/base/#jnkn.parsing.base.CompositeParser","title":"<code>CompositeParser</code>","text":"<p>               Bases: <code>BaseParser</code></p> <p>Parser that delegates to multiple sub-parsers. Used to handle directories or multiple file types.</p> Source code in <code>src/jnkn/parsing/base.py</code> <pre><code>class CompositeParser(BaseParser):\n    \"\"\"\n    Parser that delegates to multiple sub-parsers.\n    Used to handle directories or multiple file types.\n    \"\"\"\n    def __init__(self, context: ParserContext, parsers: List[BaseParser]):\n        super().__init__(context)\n        self.parsers = parsers\n\n    def can_parse(self, file_path: Path) -&gt; bool:\n        return any(p.can_parse(file_path) for p in self.parsers)\n\n    def parse(self, file_path: Path, content: bytes) -&gt; List[Union[Node, Edge]]:\n        results = []\n        for parser in self.parsers:\n            if parser.can_parse(file_path):\n                results.extend(parser.parse(file_path, content))\n        return results\n</code></pre>"},{"location":"reference/api/parsing/base/#jnkn.parsing.base.ParserContext","title":"<code>ParserContext</code>","text":"<p>Context passed to parsers (e.g., root directory).</p> Source code in <code>src/jnkn/parsing/base.py</code> <pre><code>class ParserContext:\n    \"\"\"Context passed to parsers (e.g., root directory).\"\"\"\n    def __init__(self, root_dir: Path):\n        self.root_dir = root_dir\n</code></pre>"},{"location":"reference/api/parsing/kubernetes/","title":"Kubernetes Parser","text":""},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser","title":"<code>jnkn.parsing.kubernetes.parser</code>","text":"<p>Kubernetes Manifest Parser.</p> <p>This module provides a parser for Kubernetes YAML manifests. It handles the extraction of workloads, environment variables, configuration maps, secrets, and their interdependencies.</p> <p>It supports both single-document and multi-document (--- separated) YAML files.</p>"},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser-classes","title":"Classes","text":""},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser.K8sEnvVar","title":"<code>K8sEnvVar</code>  <code>dataclass</code>","text":"<p>Represents a detected environment variable in a Kubernetes container spec.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the environment variable.</p> <code>value</code> <code>Optional[str]</code> <p>Hardcoded string value, if present.</p> <code>config_map_name</code> <code>Optional[str]</code> <p>Name of the referenced ConfigMap.</p> <code>config_map_key</code> <code>Optional[str]</code> <p>Key within the ConfigMap.</p> <code>secret_name</code> <code>Optional[str]</code> <p>Name of the referenced Secret.</p> <code>secret_key</code> <code>Optional[str]</code> <p>Key within the Secret.</p> <code>field_ref</code> <code>Optional[str]</code> <p>Field reference (e.g. status.podIP).</p> Source code in <code>src/jnkn/parsing/kubernetes/parser.py</code> <pre><code>@dataclass\nclass K8sEnvVar:\n    \"\"\"\n    Represents a detected environment variable in a Kubernetes container spec.\n\n    Attributes:\n        name (str): The name of the environment variable.\n        value (Optional[str]): Hardcoded string value, if present.\n        config_map_name (Optional[str]): Name of the referenced ConfigMap.\n        config_map_key (Optional[str]): Key within the ConfigMap.\n        secret_name (Optional[str]): Name of the referenced Secret.\n        secret_key (Optional[str]): Key within the Secret.\n        field_ref (Optional[str]): Field reference (e.g. status.podIP).\n    \"\"\"\n    name: str\n    value: Optional[str] = None\n    config_map_name: Optional[str] = None\n    config_map_key: Optional[str] = None\n    secret_name: Optional[str] = None\n    secret_key: Optional[str] = None\n    field_ref: Optional[str] = None\n\n    @property\n    def is_direct_value(self) -&gt; bool:\n        \"\"\"bool: True if the variable uses a hardcoded value.\"\"\"\n        return self.value is not None\n\n    @property\n    def is_config_map_ref(self) -&gt; bool:\n        \"\"\"bool: True if the variable references a ConfigMap.\"\"\"\n        return self.config_map_name is not None\n\n    @property\n    def is_secret_ref(self) -&gt; bool:\n        \"\"\"bool: True if the variable references a Secret.\"\"\"\n        return self.secret_name is not None\n</code></pre>"},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser.K8sEnvVar-attributes","title":"Attributes","text":""},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser.K8sEnvVar.is_config_map_ref","title":"<code>is_config_map_ref</code>  <code>property</code>","text":"<p>bool: True if the variable references a ConfigMap.</p>"},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser.K8sEnvVar.is_direct_value","title":"<code>is_direct_value</code>  <code>property</code>","text":"<p>bool: True if the variable uses a hardcoded value.</p>"},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser.K8sEnvVar.is_secret_ref","title":"<code>is_secret_ref</code>  <code>property</code>","text":"<p>bool: True if the variable references a Secret.</p>"},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser.KubernetesParser","title":"<code>KubernetesParser</code>","text":"<p>               Bases: <code>LanguageParser</code></p> <p>Parser for Kubernetes YAML files.</p> <p>This parser uses heuristics to distinguish Kubernetes manifests from other YAML files (like CI configs). It extracts detailed information about workloads (Deployments, StatefulSets, etc.) and their configuration dependencies.</p> Source code in <code>src/jnkn/parsing/kubernetes/parser.py</code> <pre><code>class KubernetesParser(LanguageParser):\n    \"\"\"\n    Parser for Kubernetes YAML files.\n\n    This parser uses heuristics to distinguish Kubernetes manifests from other\n    YAML files (like CI configs). It extracts detailed information about\n    workloads (Deployments, StatefulSets, etc.) and their configuration dependencies.\n    \"\"\"\n\n    WORKLOAD_KINDS = {\n        \"Deployment\", \"StatefulSet\", \"Job\", \"CronJob\",\n        \"DaemonSet\", \"ReplicaSet\", \"Pod\",\n    }\n\n    def __init__(self, context: Optional[ParserContext] = None):\n        super().__init__(context)\n        if not YAML_AVAILABLE:\n            self._logger = logging.getLogger(__name__)\n            self._logger.warning(\"PyYAML not available, K8s parsing will be limited\")\n\n    @property\n    def name(self) -&gt; str:\n        return \"kubernetes\"\n\n    @property\n    def extensions(self) -&gt; Set[str]:\n        return {\".yaml\", \".yml\"}\n\n    @property\n    def description(self) -&gt; str:\n        return \"Kubernetes YAML manifest parser\"\n\n    def get_capabilities(self) -&gt; Set[ParserCapability]:\n        return {\n            ParserCapability.ENV_VARS,\n            ParserCapability.CONFIGS,\n            ParserCapability.SECRETS,\n            ParserCapability.DEPENDENCIES,\n        }\n\n    def can_parse(self, file_path: Path, content: Optional[bytes] = None) -&gt; bool:\n        \"\"\"\n        Heuristically check if a file is a Kubernetes manifest.\n\n        Since `.yaml` is a generic extension, this method checks for:\n        1. File path indicators (e.g. directories named 'k8s', 'charts').\n        2. Filename patterns (e.g. 'deployment.yaml', 'service.yaml').\n        3. File content markers (e.g. 'apiVersion:', 'kind:').\n\n        Args:\n            file_path (Path): Path to the file.\n            content (Optional[bytes]): File content for deep inspection.\n\n        Returns:\n            bool: True if the file appears to be a Kubernetes manifest.\n        \"\"\"\n        if file_path.suffix.lower() not in self.extensions:\n            return False\n\n        # 1. Directory heuristics\n        k8s_indicators = {\n            \"kubernetes\", \"k8s\", \"manifests\", \"deploy\",\n            \"deployments\", \"helm\", \"charts\", \"templates\",\n        }\n        for part in file_path.parts:\n            if part.lower() in k8s_indicators:\n                return True\n\n        # 2. Filename heuristics\n        name = file_path.stem.lower()\n        k8s_patterns = {\n            \"deployment\", \"service\", \"ingress\", \"configmap\",\n            \"secret\", \"statefulset\", \"daemonset\", \"job\",\n            \"cronjob\", \"namespace\", \"pod\", \"values\",\n        }\n        for pattern in k8s_patterns:\n            if pattern in name:\n                return True\n\n        # 3. Content heuristics\n        if content:\n            try:\n                start = content[:500].decode(\"utf-8\", errors=\"ignore\")\n                if \"apiVersion:\" in start and \"kind:\" in start:\n                    return True\n            except Exception:\n                pass\n\n        return False\n\n    def parse(\n        self,\n        file_path: Path,\n        content: bytes,\n        context: Optional[ParserContext] = None,\n    ) -&gt; Generator[Union[Node, Edge], None, None]:\n        \"\"\"\n        Parse a Kubernetes YAML file and extract graph elements.\n\n        Supports multi-document YAML files.\n\n        Args:\n            file_path (Path): Path to the file.\n            content (bytes): File content.\n            context (Optional[ParserContext]): Context override.\n\n        Yields:\n            Union[Node, Edge]: Nodes for resources (Deployments, ConfigMaps) and\n            Edges for relationships (env var usage, mounting).\n        \"\"\"\n        from ...core.types import ScanMetadata\n\n        if not YAML_AVAILABLE:\n            return\n\n        # 1. Yield the file node itself\n        try:\n            file_hash = ScanMetadata.compute_hash(str(file_path))\n        except Exception:\n            file_hash = \"\"\n\n        file_id = f\"file://{file_path}\"\n        yield Node(\n            id=file_id,\n            name=file_path.name,\n            type=NodeType.CODE_FILE,\n            path=str(file_path),\n            language=\"yaml\",\n            file_hash=file_hash,\n        )\n\n        # 2. Parse YAML content\n        try:\n            text = content.decode(self._context.encoding)\n        except UnicodeDecodeError:\n            try:\n                text = content.decode(\"latin-1\")\n            except Exception:\n                return\n\n        try:\n            documents = list(yaml.safe_load_all(text))\n        except yaml.YAMLError:\n            return\n\n        # 3. Process documents\n        for doc in documents:\n            if not doc or not isinstance(doc, dict):\n                continue\n            if \"apiVersion\" not in doc or \"kind\" not in doc:\n                continue\n\n            yield from self._process_document(file_path, file_id, doc)\n\n    def _process_document(\n        self,\n        file_path: Path,\n        file_id: str,\n        doc: Dict[str, Any],\n    ) -&gt; Generator[Union[Node, Edge], None, None]:\n        \"\"\"Internal helper to process a single K8s resource dict.\"\"\"\n        kind = doc.get(\"kind\", \"\")\n        metadata = doc.get(\"metadata\", {})\n        name = metadata.get(\"name\", \"\")\n        namespace = metadata.get(\"namespace\", \"default\")\n        api_version = doc.get(\"apiVersion\", \"\")\n\n        if not kind or not name:\n            return\n\n        # Generate K8s node ID\n        if namespace:\n            k8s_id = f\"k8s:{namespace}/{kind.lower()}/{name}\"\n        else:\n            k8s_id = f\"k8s:{kind.lower()}/{name}\"\n\n        # Determine node type based on kind\n        node_type = NodeType.INFRA_RESOURCE\n        if kind == \"Secret\":\n            node_type = NodeType.SECRET\n        elif kind == \"ConfigMap\":\n            node_type = NodeType.CONFIG_KEY\n\n        # Yield the resource node\n        yield Node(\n            id=k8s_id,\n            name=name,\n            type=node_type,\n            path=str(file_path),\n            metadata={\n                \"k8s_kind\": kind,\n                \"k8s_api_version\": api_version,\n                \"namespace\": namespace,\n            },\n        )\n\n        # Link file -&gt; resource\n        yield Edge(\n            source_id=file_id,\n            target_id=k8s_id,\n            type=RelationshipType.PROVISIONS,\n        )\n\n        # Extract workload specifics (env vars, volumes)\n        if kind in self.WORKLOAD_KINDS:\n            pod_spec = self._get_pod_spec(doc)\n            if pod_spec:\n                containers = pod_spec.get(\"containers\", [])\n                for container in containers:\n                    # 1. Env vars\n                    env_list = container.get(\"env\", [])\n                    for env_var in self._extract_env_vars(env_list):\n                        env_id = f\"env:{env_var.name}\"\n\n                        yield Node(\n                            id=env_id,\n                            name=env_var.name,\n                            type=NodeType.ENV_VAR,\n                            metadata={\"k8s_resource\": k8s_id},\n                        )\n                        yield Edge(\n                            source_id=k8s_id,\n                            target_id=env_id,\n                            type=RelationshipType.PROVIDES,\n                        )\n\n                        # Link to referenced ConfigMaps/Secrets\n                        if env_var.is_config_map_ref and env_var.config_map_name:\n                            cm_id = f\"k8s:{namespace}/configmap/{env_var.config_map_name}\"\n                            yield Edge(\n                                source_id=env_id,\n                                target_id=cm_id,\n                                type=RelationshipType.READS\n                            )\n\n                        if env_var.is_secret_ref and env_var.secret_name:\n                            secret_id = f\"k8s:{namespace}/secret/{env_var.secret_name}\"\n                            yield Edge(\n                                source_id=env_id,\n                                target_id=secret_id,\n                                type=RelationshipType.READS\n                            )\n\n                    # 2. envFrom references\n                    for env_from in container.get(\"envFrom\", []):\n                        if \"configMapRef\" in env_from:\n                            cm_name = env_from[\"configMapRef\"].get(\"name\")\n                            if cm_name:\n                                cm_id = f\"k8s:{namespace}/configmap/{cm_name}\"\n                                yield Edge(source_id=k8s_id, target_id=cm_id, type=RelationshipType.READS)\n                        if \"secretRef\" in env_from:\n                            secret_name = env_from[\"secretRef\"].get(\"name\")\n                            if secret_name:\n                                secret_id = f\"k8s:{namespace}/secret/{secret_name}\"\n                                yield Edge(source_id=k8s_id, target_id=secret_id, type=RelationshipType.READS)\n\n    def _get_pod_spec(self, doc: Dict[str, Any]) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Extract PodSpec from various workload kinds.\"\"\"\n        kind = doc.get(\"kind\", \"\")\n        spec = doc.get(\"spec\", {})\n\n        if kind == \"Pod\":\n            return spec\n        elif kind in (\"Deployment\", \"ReplicaSet\", \"DaemonSet\", \"StatefulSet\", \"Job\"):\n            return spec.get(\"template\", {}).get(\"spec\", {})\n        elif kind == \"CronJob\":\n            return spec.get(\"jobTemplate\", {}).get(\"spec\", {}).get(\"template\", {}).get(\"spec\", {})\n        return None\n\n    def _extract_env_vars(self, env_list: List[Dict[str, Any]]) -&gt; List[K8sEnvVar]:\n        \"\"\"Convert raw env list to structured K8sEnvVar objects.\"\"\"\n        result: List[K8sEnvVar] = []\n        for env in env_list:\n            name = env.get(\"name\")\n            if not name:\n                continue\n\n            var = K8sEnvVar(name=name)\n            if \"value\" in env:\n                var.value = str(env[\"value\"])\n            elif \"valueFrom\" in env:\n                vf = env[\"valueFrom\"]\n                if \"configMapKeyRef\" in vf:\n                    var.config_map_name = vf[\"configMapKeyRef\"].get(\"name\")\n                    var.config_map_key = vf[\"configMapKeyRef\"].get(\"key\")\n                elif \"secretKeyRef\" in vf:\n                    var.secret_name = vf[\"secretKeyRef\"].get(\"name\")\n                    var.secret_key = vf[\"secretKeyRef\"].get(\"key\")\n\n            result.append(var)\n        return result\n</code></pre>"},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser.KubernetesParser-functions","title":"Functions","text":""},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser.KubernetesParser.can_parse","title":"<code>can_parse(file_path, content=None)</code>","text":"<p>Heuristically check if a file is a Kubernetes manifest.</p> <p>Since <code>.yaml</code> is a generic extension, this method checks for: 1. File path indicators (e.g. directories named 'k8s', 'charts'). 2. Filename patterns (e.g. 'deployment.yaml', 'service.yaml'). 3. File content markers (e.g. 'apiVersion:', 'kind:').</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file.</p> required <code>content</code> <code>Optional[bytes]</code> <p>File content for deep inspection.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file appears to be a Kubernetes manifest.</p> Source code in <code>src/jnkn/parsing/kubernetes/parser.py</code> <pre><code>def can_parse(self, file_path: Path, content: Optional[bytes] = None) -&gt; bool:\n    \"\"\"\n    Heuristically check if a file is a Kubernetes manifest.\n\n    Since `.yaml` is a generic extension, this method checks for:\n    1. File path indicators (e.g. directories named 'k8s', 'charts').\n    2. Filename patterns (e.g. 'deployment.yaml', 'service.yaml').\n    3. File content markers (e.g. 'apiVersion:', 'kind:').\n\n    Args:\n        file_path (Path): Path to the file.\n        content (Optional[bytes]): File content for deep inspection.\n\n    Returns:\n        bool: True if the file appears to be a Kubernetes manifest.\n    \"\"\"\n    if file_path.suffix.lower() not in self.extensions:\n        return False\n\n    # 1. Directory heuristics\n    k8s_indicators = {\n        \"kubernetes\", \"k8s\", \"manifests\", \"deploy\",\n        \"deployments\", \"helm\", \"charts\", \"templates\",\n    }\n    for part in file_path.parts:\n        if part.lower() in k8s_indicators:\n            return True\n\n    # 2. Filename heuristics\n    name = file_path.stem.lower()\n    k8s_patterns = {\n        \"deployment\", \"service\", \"ingress\", \"configmap\",\n        \"secret\", \"statefulset\", \"daemonset\", \"job\",\n        \"cronjob\", \"namespace\", \"pod\", \"values\",\n    }\n    for pattern in k8s_patterns:\n        if pattern in name:\n            return True\n\n    # 3. Content heuristics\n    if content:\n        try:\n            start = content[:500].decode(\"utf-8\", errors=\"ignore\")\n            if \"apiVersion:\" in start and \"kind:\" in start:\n                return True\n        except Exception:\n            pass\n\n    return False\n</code></pre>"},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser.KubernetesParser.parse","title":"<code>parse(file_path, content, context=None)</code>","text":"<p>Parse a Kubernetes YAML file and extract graph elements.</p> <p>Supports multi-document YAML files.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the file.</p> required <code>content</code> <code>bytes</code> <p>File content.</p> required <code>context</code> <code>Optional[ParserContext]</code> <p>Context override.</p> <code>None</code> <p>Yields:</p> Type Description <code>Union[Node, Edge]</code> <p>Union[Node, Edge]: Nodes for resources (Deployments, ConfigMaps) and</p> <code>Union[Node, Edge]</code> <p>Edges for relationships (env var usage, mounting).</p> Source code in <code>src/jnkn/parsing/kubernetes/parser.py</code> <pre><code>def parse(\n    self,\n    file_path: Path,\n    content: bytes,\n    context: Optional[ParserContext] = None,\n) -&gt; Generator[Union[Node, Edge], None, None]:\n    \"\"\"\n    Parse a Kubernetes YAML file and extract graph elements.\n\n    Supports multi-document YAML files.\n\n    Args:\n        file_path (Path): Path to the file.\n        content (bytes): File content.\n        context (Optional[ParserContext]): Context override.\n\n    Yields:\n        Union[Node, Edge]: Nodes for resources (Deployments, ConfigMaps) and\n        Edges for relationships (env var usage, mounting).\n    \"\"\"\n    from ...core.types import ScanMetadata\n\n    if not YAML_AVAILABLE:\n        return\n\n    # 1. Yield the file node itself\n    try:\n        file_hash = ScanMetadata.compute_hash(str(file_path))\n    except Exception:\n        file_hash = \"\"\n\n    file_id = f\"file://{file_path}\"\n    yield Node(\n        id=file_id,\n        name=file_path.name,\n        type=NodeType.CODE_FILE,\n        path=str(file_path),\n        language=\"yaml\",\n        file_hash=file_hash,\n    )\n\n    # 2. Parse YAML content\n    try:\n        text = content.decode(self._context.encoding)\n    except UnicodeDecodeError:\n        try:\n            text = content.decode(\"latin-1\")\n        except Exception:\n            return\n\n    try:\n        documents = list(yaml.safe_load_all(text))\n    except yaml.YAMLError:\n        return\n\n    # 3. Process documents\n    for doc in documents:\n        if not doc or not isinstance(doc, dict):\n            continue\n        if \"apiVersion\" not in doc or \"kind\" not in doc:\n            continue\n\n        yield from self._process_document(file_path, file_id, doc)\n</code></pre>"},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser-functions","title":"Functions","text":""},{"location":"reference/api/parsing/kubernetes/#jnkn.parsing.kubernetes.parser.create_kubernetes_parser","title":"<code>create_kubernetes_parser(context=None)</code>","text":"<p>Factory function for KubernetesParser.</p> Source code in <code>src/jnkn/parsing/kubernetes/parser.py</code> <pre><code>def create_kubernetes_parser(context: Optional[ParserContext] = None) -&gt; KubernetesParser:\n    \"\"\"Factory function for KubernetesParser.\"\"\"\n    return KubernetesParser(context)\n</code></pre>"},{"location":"reference/api/parsing/python/","title":"Python Parser","text":""},{"location":"reference/api/parsing/python/#jnkn.parsing.python.parser","title":"<code>jnkn.parsing.python.parser</code>","text":"<p>Standardized Python Parser.</p>"},{"location":"reference/api/parsing/python/#jnkn.parsing.python.parser-classes","title":"Classes","text":""},{"location":"reference/api/parsing/python/#jnkn.parsing.python.parser.PythonParser","title":"<code>PythonParser</code>","text":"<p>               Bases: <code>BaseParser</code></p> Source code in <code>src/jnkn/parsing/python/parser.py</code> <pre><code>class PythonParser(BaseParser):\n    def can_parse(self, file_path: Path) -&gt; bool:\n        return file_path.suffix == \".py\"\n\n    def parse(self, file_path: Path, content: bytes) -&gt; List[Union[Node, Edge]]:\n        results: List[Union[Node, Edge]] = []\n\n        try:\n            tree = ast.parse(content)\n        except SyntaxError:\n            # If we can't parse syntax, we can't extract nodes\n            return []\n\n        rel_path = self._relativize(file_path)\n        file_id = f\"file://{rel_path}\"\n\n        # 1. Create the File Node\n        file_node = Node(\n            id=file_id,\n            name=file_path.name,\n            type=NodeType.CODE_FILE,\n            path=rel_path,\n            metadata={\"language\": \"python\"}\n        )\n        results.append(file_node)\n\n        # 2. Walk the AST to find env vars\n        # Simple walker looking for os.getenv / os.environ\n        for node in ast.walk(tree):\n            env_var_name = self._extract_env_var(node)\n            if env_var_name:\n                # We do NOT create the EnvVar node here (the Stitcher/Graph does that or we assume it exists).\n                # But typically parsers create a \"reference\" or \"demand\".\n                # In Jnkn architecture, we often create the EnvVar node lazily or create a dependency to it.\n\n                env_id = f\"env:{env_var_name}\"\n\n                # Create the EnvVar node (it's okay if duplicates exist, Graph deduplicates)\n                env_node = Node(\n                    id=env_id,\n                    name=env_var_name,\n                    type=NodeType.ENV_VAR,\n                    tokens=[t.lower() for t in env_var_name.split(\"_\")] # Simplified tokenization\n                )\n                results.append(env_node)\n\n                # Create the Edge (File READS EnvVar)\n                edge = Edge(\n                    source_id=file_id,\n                    target_id=env_id,\n                    type=RelationshipType.READS,\n                    metadata={\"line\": getattr(node, \"lineno\", 0)}\n                )\n                results.append(edge)\n\n        return results\n\n    def _extract_env_var(self, node: ast.AST) -&gt; str | None:\n        \"\"\"\n        Detect `os.getenv('VAR')` or `os.environ['VAR']`.\n        \"\"\"\n        # Case: os.getenv(\"VAR\")\n        if isinstance(node, ast.Call):\n            if isinstance(node.func, ast.Attribute):\n                if getattr(node.func.value, \"id\", \"\") == \"os\" and node.func.attr == \"getenv\":\n                    if node.args and isinstance(node.args[0], ast.Constant):\n                        return node.args[0].value\n\n        # Case: os.environ[\"VAR\"] or os.environ.get(\"VAR\")\n        # Simplified for brevity in this refactor example\n        if isinstance(node, ast.Subscript):\n            if isinstance(node.value, ast.Attribute):\n                if getattr(node.value.value, \"id\", \"\") == \"os\" and node.value.attr == \"environ\":\n                    if isinstance(node.slice, ast.Constant):\n                        return node.slice.value\n\n        return None\n</code></pre>"},{"location":"reference/api/parsing/terraform/","title":"Terraform Parser","text":""},{"location":"reference/api/parsing/terraform/#jnkn.parsing.terraform.parser","title":"<code>jnkn.parsing.terraform.parser</code>","text":"<p>Standardized Terraform Parser.</p>"},{"location":"reference/api/parsing/terraform/#jnkn.parsing.terraform.parser-classes","title":"Classes","text":""},{"location":"reference/api/stitching/engine/","title":"Stitching Engine","text":"<p>Cross-domain dependency stitching.</p> <p>This module implements the core \"glue\" logic that connects disparate domains: - Environment variables to infrastructure resources - Code references to data assets - Configuration keys to their providers</p> <p>Key capabilities include: - Linking environment variables to infrastructure resources (e.g., <code>PAYMENT_DB_HOST</code> -&gt; <code>aws_db_instance.payment</code>). - Linking infrastructure resources to each other (e.g., Security Groups to EC2 instances). - Configurable confidence scoring to minimize false positives.</p> <p>The stitching process transforms a collection of isolated nodes (from parsing) into a connected graph that represents the full system architecture.</p>"},{"location":"reference/api/stitching/engine/#jnkn.core.stitching-classes","title":"Classes","text":""},{"location":"reference/api/stitching/engine/#jnkn.core.stitching.Stitcher","title":"<code>Stitcher</code>","text":"<p>Orchestrator for cross-domain dependency stitching.</p> <p>The Stitcher manages the lifecycle of applying multiple <code>StitchingRule</code>s to a graph. It handles configuration, rule execution, and aggregation of results.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>MatchConfig</code> <p>Configuration for matching thresholds.</p> <code>rules</code> <code>List[StitchingRule]</code> <p>List of active rules to apply.</p>"},{"location":"reference/api/stitching/engine/#jnkn.core.stitching.Stitcher-functions","title":"Functions","text":""},{"location":"reference/api/stitching/engine/#jnkn.core.stitching.Stitcher.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the Stitcher.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[MatchConfig]</code> <p>Optional match configuration. Uses defaults if None.</p> <code>None</code>"},{"location":"reference/api/stitching/engine/#jnkn.core.stitching.Stitcher.stitch","title":"<code>stitch(graph)</code>","text":"<p>Apply all configured stitching rules to the graph.</p> <p>Discovered edges are automatically added to the graph if they do not already exist.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DependencyGraph</code> <p>The dependency graph to update.</p> required <p>Returns:</p> Type Description <code>List[Edge]</code> <p>List[Edge]: A list of all new edges created during this stitching session.</p>"},{"location":"reference/api/stitching/engine/#jnkn.core.stitching.MatchConfig","title":"<code>MatchConfig</code>","text":"<p>Configuration for the fuzzy matching engine.</p> <p>Controls the thresholds and weights used to determine if two artifacts are related.</p> <p>Attributes:</p> Name Type Description <code>min_confidence</code> <code>float</code> <p>The minimum score (0.0 - 1.0) required to create an edge. Defaults to 0.5.</p> <code>min_token_overlap</code> <code>int</code> <p>The minimum number of shared tokens required for a match. Defaults to 2.</p> <code>min_token_length</code> <code>int</code> <p>Tokens shorter than this length are ignored to reduce noise. Defaults to 2.</p> <code>strategy_weights</code> <code>Dict[MatchStrategy, float]</code> <p>A mapping of matching strategies to their base confidence scores.</p>"},{"location":"reference/api/stitching/engine/#jnkn.core.stitching.MatchConfig-functions","title":"Functions","text":""},{"location":"reference/api/stitching/engine/#jnkn.core.stitching.MatchConfig.__init__","title":"<code>__init__(min_confidence=0.5, min_token_overlap=2, min_token_length=2, strategy_weights=None)</code>","text":"<p>Initialize the match configuration.</p> <p>Parameters:</p> Name Type Description Default <code>min_confidence</code> <code>float</code> <p>Minimum score (0.0-1.0) to accept a match.</p> <code>0.5</code> <code>min_token_overlap</code> <code>int</code> <p>Minimum shared tokens required.</p> <code>2</code> <code>min_token_length</code> <code>int</code> <p>Minimum character length for valid tokens.</p> <code>2</code> <code>strategy_weights</code> <code>Optional[Dict[MatchStrategy, float]]</code> <p>Optional overrides for strategy scoring weights.</p> <code>None</code>"},{"location":"reference/api/stitching/matchers/","title":"Matchers","text":"<p>Token Matching Utilities for jnkn.</p> <p>This module provides utilities for token-based matching with: - Minimum token length filtering - Blocked token exclusion - Low-value token weighting - Configurable matching behavior</p> <p>The TokenMatcher class is the central component for all token-based operations in the stitching system.</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers-classes","title":"Classes","text":""},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenConfig","title":"<code>TokenConfig</code>  <code>dataclass</code>","text":"<p>Configuration for token matching behavior.</p> <p>Controls which tokens are considered significant for matching and how they contribute to confidence scores.</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenConfig-functions","title":"Functions","text":""},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenConfig.get_token_weight","title":"<code>get_token_weight(token)</code>","text":"<p>Get the weight multiplier for a token.</p> <p>Returns:</p> Type Description <code>float</code> <p>1.0 for normal tokens</p> <code>float</code> <p>0.0 for blocked tokens</p> <code>float</code> <p>0.5 for low-value tokens</p> <code>float</code> <p>0.3 for short tokens</p> <code>float</code> <p>Compounds if multiple conditions apply</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenConfig.is_blocked","title":"<code>is_blocked(token)</code>","text":"<p>Check if a token is blocked.</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenConfig.is_low_value","title":"<code>is_low_value(token)</code>","text":"<p>Check if a token is low-value.</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenConfig.is_short","title":"<code>is_short(token)</code>","text":"<p>Check if a token is too short.</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher","title":"<code>TokenMatcher</code>","text":"<p>Token-based matching with configurable filtering.</p> Usage <p>matcher = TokenMatcher()</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher--tokenize-names","title":"Tokenize names","text":"<p>tokens1 = matcher.tokenize(\"PAYMENT_DB_HOST\") tokens2 = matcher.tokenize(\"payment_database_host\")</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher--get-significant-tokens-only","title":"Get significant tokens only","text":"<p>sig1 = matcher.get_significant_tokens(tokens1) sig2 = matcher.get_significant_tokens(tokens2)</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher--calculate-overlap","title":"Calculate overlap","text":"<p>overlap, score = matcher.calculate_overlap(sig1, sig2)</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher-functions","title":"Functions","text":""},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the token matcher.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[TokenConfig]</code> <p>Optional TokenConfig. Uses defaults if not provided.</p> <code>None</code>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher.calculate_overlap","title":"<code>calculate_overlap(tokens1, tokens2)</code>","text":"<p>Calculate token overlap between two token lists.</p> <p>Parameters:</p> Name Type Description Default <code>tokens1</code> <code>List[str]</code> <p>First token list</p> required <code>tokens2</code> <code>List[str]</code> <p>Second token list</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], float]</code> <p>Tuple of (overlapping tokens, Jaccard similarity score)</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher.calculate_significant_overlap","title":"<code>calculate_significant_overlap(tokens1, tokens2)</code>","text":"<p>Calculate overlap using only significant tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens1</code> <code>List[str]</code> <p>First token list</p> required <code>tokens2</code> <code>List[str]</code> <p>Second token list</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], float]</code> <p>Tuple of (overlapping significant tokens, Jaccard similarity score)</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher.calculate_weighted_overlap","title":"<code>calculate_weighted_overlap(tokens1, tokens2)</code>","text":"<p>Calculate overlap with weighted scoring.</p> <p>Low-value and short tokens contribute less to the score.</p> <p>Parameters:</p> Name Type Description Default <code>tokens1</code> <code>List[str]</code> <p>First token list</p> required <code>tokens2</code> <code>List[str]</code> <p>Second token list</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], float]</code> <p>Tuple of (overlapping tokens, weighted score)</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher.get_match_quality","title":"<code>get_match_quality(source_tokens, target_tokens)</code>","text":"<p>Get detailed match quality information.</p> <p>Parameters:</p> Name Type Description Default <code>source_tokens</code> <code>List[str]</code> <p>Source token list</p> required <code>target_tokens</code> <code>List[str]</code> <p>Target token list</p> required <p>Returns:</p> Type Description <code>Dict[str, any]</code> <p>Dictionary with match details</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher.get_significant_tokens","title":"<code>get_significant_tokens(tokens)</code>","text":"<p>Filter tokens to only significant ones.</p> <p>Removes blocked tokens and those below minimum length.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[str]</code> <p>List of tokens</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of significant tokens</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher.get_weighted_tokens","title":"<code>get_weighted_tokens(tokens)</code>","text":"<p>Get tokens with their weight multipliers.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[str]</code> <p>List of tokens</p> required <p>Returns:</p> Type Description <code>List[Tuple[str, float]]</code> <p>List of (token, weight) tuples</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher.has_sufficient_overlap","title":"<code>has_sufficient_overlap(tokens1, tokens2)</code>","text":"<p>Check if two token lists have sufficient overlap for matching.</p> <p>Uses min_significant_tokens from config.</p> <p>Parameters:</p> Name Type Description Default <code>tokens1</code> <code>List[str]</code> <p>First token list</p> required <code>tokens2</code> <code>List[str]</code> <p>Second token list</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if overlap is sufficient</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher.normalize","title":"<code>normalize(name)</code>  <code>staticmethod</code>","text":"<p>Normalize a name by lowercasing and removing separators.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name to normalize</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized string (e.g., \"PAYMENT_DB_HOST\" -&gt; \"paymentdbhost\")</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.TokenMatcher.tokenize","title":"<code>tokenize(name)</code>  <code>staticmethod</code>","text":"<p>Split a name into tokens.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name to tokenize</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of lowercase tokens (e.g., \"PAYMENT_DB_HOST\" -&gt; [\"payment\", \"db\", \"host\"])</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers-functions","title":"Functions","text":""},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.create_default_matcher","title":"<code>create_default_matcher(config_path=None)</code>","text":"<p>Create a TokenMatcher with configuration from file or defaults.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Optional[Path]</code> <p>Optional path to config file.          Defaults to .jnkn/config.yaml</p> <code>None</code> <p>Returns:</p> Type Description <code>TokenMatcher</code> <p>Configured TokenMatcher</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.load_config_from_yaml","title":"<code>load_config_from_yaml(path)</code>","text":"<p>Load TokenConfig from a YAML configuration file.</p> Expected format <p>matching:   min_token_length: 3   min_significant_tokens: 2   blocked_tokens:     - id     - db     - host   low_value_tokens:     - aws     - main</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to YAML file</p> required <p>Returns:</p> Type Description <code>Optional[TokenConfig]</code> <p>TokenConfig if file exists and is valid, None otherwise</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.normalize","title":"<code>normalize(name)</code>","text":"<p>Normalize a name. See TokenMatcher.normalize.</p>"},{"location":"reference/api/stitching/matchers/#jnkn.stitching.matchers.tokenize","title":"<code>tokenize(name)</code>","text":"<p>Tokenize a name. See TokenMatcher.tokenize.</p>"},{"location":"reference/api/stitching/rules/","title":"Stitching Rules","text":"<p>Cross-domain dependency stitching.</p> <p>This module implements the core \"glue\" logic that connects disparate domains: - Environment variables to infrastructure resources - Code references to data assets - Configuration keys to their providers</p> <p>Key capabilities include: - Linking environment variables to infrastructure resources (e.g., <code>PAYMENT_DB_HOST</code> -&gt; <code>aws_db_instance.payment</code>). - Linking infrastructure resources to each other (e.g., Security Groups to EC2 instances). - Configurable confidence scoring to minimize false positives.</p> <p>The stitching process transforms a collection of isolated nodes (from parsing) into a connected graph that represents the full system architecture.</p>"},{"location":"reference/api/stitching/rules/#jnkn.core.stitching-classes","title":"Classes","text":""},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.StitchingRule","title":"<code>StitchingRule</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all stitching rules.</p> <p>A stitching rule defines logic to discover implicit relationships between nodes in the dependency graph. Subclasses must implement the <code>apply</code> method.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>MatchConfig</code> <p>The configuration used for scoring matches.</p>"},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.StitchingRule-functions","title":"Functions","text":""},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.StitchingRule.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the rule.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[MatchConfig]</code> <p>Optional match configuration. Uses defaults if None.</p> <code>None</code>"},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.StitchingRule.apply","title":"<code>apply(graph)</code>  <code>abstractmethod</code>","text":"<p>Apply this rule to discover new edges in the graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DependencyGraph</code> <p>The dependency graph to analyze.</p> required <p>Returns:</p> Type Description <code>List[Edge]</code> <p>List[Edge]: A list of newly discovered edges.</p>"},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.StitchingRule.get_name","title":"<code>get_name()</code>  <code>abstractmethod</code>","text":"<p>Return a descriptive, unique name for this rule.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The rule name (e.g., 'EnvVarToInfraRule').</p>"},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.EnvVarToInfraRule","title":"<code>EnvVarToInfraRule</code>","text":"<p>               Bases: <code>StitchingRule</code></p> <p>Stitching rule that links environment variables to infrastructure resources.</p> <p>This rule attempts to find the infrastructure resource that provides the value for a given environment variable. It uses strategies like normalized name matching and token overlap.</p> Directionality <p>Infra Resource (or Output) -&gt; PROVIDES -&gt; Environment Variable</p>"},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.EnvVarToInfraRule-functions","title":"Functions","text":""},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.EnvVarToInfraRule.apply","title":"<code>apply(graph)</code>","text":"<p>Execute the rule logic against the graph.</p> <p>Finds potential matches between <code>ENV_VAR</code> nodes and <code>INFRA_RESOURCE</code>/<code>CONFIG_KEY</code> nodes.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DependencyGraph</code> <p>The dependency graph.</p> required <p>Returns:</p> Type Description <code>List[Edge]</code> <p>List[Edge]: Discovered edges with confidence scores and metadata.</p>"},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.EnvVarToInfraRule.get_name","title":"<code>get_name()</code>","text":"<p>Get the rule name.</p>"},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.InfraToInfraRule","title":"<code>InfraToInfraRule</code>","text":"<p>               Bases: <code>StitchingRule</code></p> <p>Stitching rule that links infrastructure resources to other infrastructure resources.</p> <p>This rule identifies dependencies between resources based on naming conventions, such as a Security Group referencing a VPC by name token overlap.</p> Directionality <p>Determined hierarchically (e.g., VPC configures Subnet).</p>"},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.InfraToInfraRule-functions","title":"Functions","text":""},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.InfraToInfraRule.apply","title":"<code>apply(graph)</code>","text":"<p>Execute the rule logic against the graph.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>DependencyGraph</code> <p>The dependency graph.</p> required <p>Returns:</p> Type Description <code>List[Edge]</code> <p>List[Edge]: Discovered edges between infrastructure resources.</p>"},{"location":"reference/api/stitching/rules/#jnkn.core.stitching.InfraToInfraRule.get_name","title":"<code>get_name()</code>","text":"<p>Get the rule name.</p>"},{"location":"reference/api/stitching/suppressions/","title":"Suppressions","text":""},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions","title":"<code>jnkn.stitching.suppressions</code>","text":"<p>User Suppressions System for jnkn.</p> <p>This module provides a system for users to suppress false positive matches. Suppressions can be pattern-based (using glob patterns) and have optional expiration dates.</p> <p>Features: - Glob pattern matching for flexible suppression - YAML-based persistence - Expiration support - Integration with the Stitcher</p> <p>Storage Location: .jnkn/suppressions.yaml</p>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions-classes","title":"Classes","text":""},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.Suppression","title":"<code>Suppression</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single suppression rule.</p> <p>Matches source/target pairs using glob patterns.</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>class Suppression(BaseModel):\n    \"\"\"\n    A single suppression rule.\n\n    Matches source/target pairs using glob patterns.\n    \"\"\"\n    source_pattern: str = Field(..., description=\"Glob pattern for source node (e.g., 'env:*_ID')\")\n    target_pattern: str = Field(..., description=\"Glob pattern for target node (e.g., 'infra:*')\")\n    reason: str = Field(default=\"\", description=\"Why this suppression exists\")\n    created_by: str = Field(default=\"unknown\", description=\"Who created this suppression\")\n    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc), description=\"When created\")\n    expires_at: Optional[datetime] = Field(default=None, description=\"Optional expiration\")\n    id: Optional[str] = Field(default=None, description=\"Unique identifier\")\n    enabled: bool = Field(default=True, description=\"Whether suppression is active\")\n\n    def model_post_init(self, __context) -&gt; None:\n        \"\"\"Generate ID if not provided.\"\"\"\n        if self.id is None:\n            # Generate ID from pattern hash\n            import hashlib\n            content = f\"{self.source_pattern}|{self.target_pattern}|{self.created_at.isoformat()}\"\n            object.__setattr__(self, 'id', hashlib.md5(content.encode()).hexdigest()[:8])\n\n    def is_expired(self) -&gt; bool:\n        \"\"\"Check if this suppression has expired.\"\"\"\n        if self.expires_at is None:\n            return False\n        return datetime.now(timezone.utc) &gt; self.expires_at\n\n    def is_active(self) -&gt; bool:\n        \"\"\"Check if this suppression is currently active.\"\"\"\n        return self.enabled and not self.is_expired()\n\n    def matches(self, source_id: str, target_id: str) -&gt; bool:\n        \"\"\"\n        Check if this suppression matches a source/target pair.\n\n        Uses glob pattern matching (fnmatch).\n        \"\"\"\n        if not self.is_active():\n            return False\n\n        source_matches = fnmatch(source_id, self.source_pattern)\n        target_matches = fnmatch(target_id, self.target_pattern)\n\n        return source_matches and target_matches\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert to dictionary for YAML serialization.\"\"\"\n        result = {\n            \"source\": self.source_pattern,\n            \"target\": self.target_pattern,\n            \"reason\": self.reason,\n            \"created_by\": self.created_by,\n            \"created_at\": self.created_at.isoformat(),\n        }\n        if self.expires_at:\n            result[\"expires_at\"] = self.expires_at.isoformat()\n        if self.id:\n            result[\"id\"] = self.id\n        if not self.enabled:\n            result[\"enabled\"] = False\n        return result\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"Suppression\":\n        \"\"\"Create from dictionary (YAML deserialization).\"\"\"\n        created_at = data.get(\"created_at\")\n        if isinstance(created_at, str):\n            created_at = datetime.fromisoformat(created_at)\n\n        expires_at = data.get(\"expires_at\")\n        if isinstance(expires_at, str):\n            expires_at = datetime.fromisoformat(expires_at)\n\n        return cls(\n            source_pattern=data.get(\"source\", data.get(\"source_pattern\", \"*\")),\n            target_pattern=data.get(\"target\", data.get(\"target_pattern\", \"*\")),\n            reason=data.get(\"reason\", \"\"),\n            created_by=data.get(\"created_by\", \"unknown\"),\n            created_at=created_at or datetime.utcnow(),\n            expires_at=expires_at,\n            id=data.get(\"id\"),\n            enabled=data.get(\"enabled\", True),\n        )\n\n    class Config:\n        frozen = False\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.Suppression-functions","title":"Functions","text":""},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.Suppression.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create from dictionary (YAML deserialization).</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"Suppression\":\n    \"\"\"Create from dictionary (YAML deserialization).\"\"\"\n    created_at = data.get(\"created_at\")\n    if isinstance(created_at, str):\n        created_at = datetime.fromisoformat(created_at)\n\n    expires_at = data.get(\"expires_at\")\n    if isinstance(expires_at, str):\n        expires_at = datetime.fromisoformat(expires_at)\n\n    return cls(\n        source_pattern=data.get(\"source\", data.get(\"source_pattern\", \"*\")),\n        target_pattern=data.get(\"target\", data.get(\"target_pattern\", \"*\")),\n        reason=data.get(\"reason\", \"\"),\n        created_by=data.get(\"created_by\", \"unknown\"),\n        created_at=created_at or datetime.utcnow(),\n        expires_at=expires_at,\n        id=data.get(\"id\"),\n        enabled=data.get(\"enabled\", True),\n    )\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.Suppression.is_active","title":"<code>is_active()</code>","text":"<p>Check if this suppression is currently active.</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def is_active(self) -&gt; bool:\n    \"\"\"Check if this suppression is currently active.\"\"\"\n    return self.enabled and not self.is_expired()\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.Suppression.is_expired","title":"<code>is_expired()</code>","text":"<p>Check if this suppression has expired.</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def is_expired(self) -&gt; bool:\n    \"\"\"Check if this suppression has expired.\"\"\"\n    if self.expires_at is None:\n        return False\n    return datetime.now(timezone.utc) &gt; self.expires_at\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.Suppression.matches","title":"<code>matches(source_id, target_id)</code>","text":"<p>Check if this suppression matches a source/target pair.</p> <p>Uses glob pattern matching (fnmatch).</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def matches(self, source_id: str, target_id: str) -&gt; bool:\n    \"\"\"\n    Check if this suppression matches a source/target pair.\n\n    Uses glob pattern matching (fnmatch).\n    \"\"\"\n    if not self.is_active():\n        return False\n\n    source_matches = fnmatch(source_id, self.source_pattern)\n    target_matches = fnmatch(target_id, self.target_pattern)\n\n    return source_matches and target_matches\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.Suppression.model_post_init","title":"<code>model_post_init(__context)</code>","text":"<p>Generate ID if not provided.</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def model_post_init(self, __context) -&gt; None:\n    \"\"\"Generate ID if not provided.\"\"\"\n    if self.id is None:\n        # Generate ID from pattern hash\n        import hashlib\n        content = f\"{self.source_pattern}|{self.target_pattern}|{self.created_at.isoformat()}\"\n        object.__setattr__(self, 'id', hashlib.md5(content.encode()).hexdigest()[:8])\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.Suppression.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary for YAML serialization.</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert to dictionary for YAML serialization.\"\"\"\n    result = {\n        \"source\": self.source_pattern,\n        \"target\": self.target_pattern,\n        \"reason\": self.reason,\n        \"created_by\": self.created_by,\n        \"created_at\": self.created_at.isoformat(),\n    }\n    if self.expires_at:\n        result[\"expires_at\"] = self.expires_at.isoformat()\n    if self.id:\n        result[\"id\"] = self.id\n    if not self.enabled:\n        result[\"enabled\"] = False\n    return result\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionAwareStitcher","title":"<code>SuppressionAwareStitcher</code>","text":"<p>Mixin/wrapper for adding suppression awareness to stitching.</p> <p>This class wraps the Edge creation process to check suppressions.</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>class SuppressionAwareStitcher:\n    \"\"\"\n    Mixin/wrapper for adding suppression awareness to stitching.\n\n    This class wraps the Edge creation process to check suppressions.\n    \"\"\"\n\n    def __init__(self, store: Optional[SuppressionStore] = None):\n        \"\"\"\n        Initialize with a suppression store.\n\n        Args:\n            store: SuppressionStore instance (creates default if None)\n        \"\"\"\n        self.store = store or create_default_store()\n        self._suppressed_count = 0\n        self._suppressed_edges: List[Dict[str, Any]] = []\n\n    def should_create_edge(self, source_id: str, target_id: str) -&gt; bool:\n        \"\"\"\n        Check if an edge should be created (not suppressed).\n\n        Args:\n            source_id: Source node ID\n            target_id: Target node ID\n\n        Returns:\n            True if edge should be created\n        \"\"\"\n        match = self.store.is_suppressed(source_id, target_id)\n\n        if match.suppressed:\n            self._suppressed_count += 1\n            self._suppressed_edges.append({\n                \"source_id\": source_id,\n                \"target_id\": target_id,\n                \"reason\": match.reason,\n                \"pattern\": f\"{match.suppression.source_pattern} \u2192 {match.suppression.target_pattern}\" if match.suppression else \"\",\n            })\n            return False\n\n        return True\n\n    def reset_stats(self) -&gt; None:\n        \"\"\"Reset suppression statistics.\"\"\"\n        self._suppressed_count = 0\n        self._suppressed_edges = []\n\n    @property\n    def suppressed_count(self) -&gt; int:\n        \"\"\"Number of edges suppressed in current session.\"\"\"\n        return self._suppressed_count\n\n    @property\n    def suppressed_edges(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"List of suppressed edges with details.\"\"\"\n        return list(self._suppressed_edges)\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionAwareStitcher-attributes","title":"Attributes","text":""},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionAwareStitcher.suppressed_count","title":"<code>suppressed_count</code>  <code>property</code>","text":"<p>Number of edges suppressed in current session.</p>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionAwareStitcher.suppressed_edges","title":"<code>suppressed_edges</code>  <code>property</code>","text":"<p>List of suppressed edges with details.</p>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionAwareStitcher-functions","title":"Functions","text":""},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionAwareStitcher.__init__","title":"<code>__init__(store=None)</code>","text":"<p>Initialize with a suppression store.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>Optional[SuppressionStore]</code> <p>SuppressionStore instance (creates default if None)</p> <code>None</code> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def __init__(self, store: Optional[SuppressionStore] = None):\n    \"\"\"\n    Initialize with a suppression store.\n\n    Args:\n        store: SuppressionStore instance (creates default if None)\n    \"\"\"\n    self.store = store or create_default_store()\n    self._suppressed_count = 0\n    self._suppressed_edges: List[Dict[str, Any]] = []\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionAwareStitcher.reset_stats","title":"<code>reset_stats()</code>","text":"<p>Reset suppression statistics.</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def reset_stats(self) -&gt; None:\n    \"\"\"Reset suppression statistics.\"\"\"\n    self._suppressed_count = 0\n    self._suppressed_edges = []\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionAwareStitcher.should_create_edge","title":"<code>should_create_edge(source_id, target_id)</code>","text":"<p>Check if an edge should be created (not suppressed).</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>Source node ID</p> required <code>target_id</code> <code>str</code> <p>Target node ID</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if edge should be created</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def should_create_edge(self, source_id: str, target_id: str) -&gt; bool:\n    \"\"\"\n    Check if an edge should be created (not suppressed).\n\n    Args:\n        source_id: Source node ID\n        target_id: Target node ID\n\n    Returns:\n        True if edge should be created\n    \"\"\"\n    match = self.store.is_suppressed(source_id, target_id)\n\n    if match.suppressed:\n        self._suppressed_count += 1\n        self._suppressed_edges.append({\n            \"source_id\": source_id,\n            \"target_id\": target_id,\n            \"reason\": match.reason,\n            \"pattern\": f\"{match.suppression.source_pattern} \u2192 {match.suppression.target_pattern}\" if match.suppression else \"\",\n        })\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionMatch","title":"<code>SuppressionMatch</code>  <code>dataclass</code>","text":"<p>Result of checking if an edge is suppressed.</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>@dataclass\nclass SuppressionMatch:\n    \"\"\"Result of checking if an edge is suppressed.\"\"\"\n    suppressed: bool\n    suppression: Optional[Suppression] = None\n    reason: str = \"\"\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore","title":"<code>SuppressionStore</code>","text":"<p>Manages suppression rules with YAML persistence.</p> Usage <p>store = SuppressionStore() store.load()</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>class SuppressionStore:\n    \"\"\"\n    Manages suppression rules with YAML persistence.\n\n    Usage:\n        store = SuppressionStore()\n        store.load()\n\n        # Add suppression\n        store.add(Suppression(\n            source_pattern=\"env:HOST\",\n            target_pattern=\"infra:*\",\n            reason=\"HOST is too generic\"\n        ))\n\n        # Check if edge is suppressed\n        if store.is_suppressed(\"env:HOST\", \"infra:main\"):\n            print(\"Suppressed!\")\n\n        # Save changes\n        store.save()\n    \"\"\"\n\n    DEFAULT_PATH = Path(\".jnkn/suppressions.yaml\")\n\n    def __init__(self, path: Optional[Path] = None):\n        \"\"\"\n        Initialize the suppression store.\n\n        Args:\n            path: Path to YAML file. Defaults to .jnkn/suppressions.yaml\n        \"\"\"\n        self.path = path or self.DEFAULT_PATH\n        self._suppressions: List[Suppression] = []\n        self._loaded = False\n\n    def load(self) -&gt; int:\n        \"\"\"\n        Load suppressions from YAML file.\n\n        Returns:\n            Number of suppressions loaded\n        \"\"\"\n        if not self.path.exists():\n            logger.debug(f\"Suppressions file not found: {self.path}\")\n            self._loaded = True\n            return 0\n\n        try:\n            with open(self.path, \"r\") as f:\n                data = yaml.safe_load(f)\n\n            if not data:\n                self._suppressions = []\n                self._loaded = True\n                return 0\n\n            suppressions_data = data.get(\"suppressions\", [])\n            self._suppressions = [\n                Suppression.from_dict(s) for s in suppressions_data\n            ]\n\n            self._loaded = True\n            logger.info(f\"Loaded {len(self._suppressions)} suppressions from {self.path}\")\n            return len(self._suppressions)\n\n        except Exception as e:\n            logger.error(f\"Failed to load suppressions: {e}\")\n            self._suppressions = []\n            self._loaded = True\n            return 0\n\n    def save(self) -&gt; bool:\n        \"\"\"\n        Save suppressions to YAML file.\n\n        Returns:\n            True if successful\n        \"\"\"\n        try:\n            # Ensure directory exists\n            self.path.parent.mkdir(parents=True, exist_ok=True)\n\n            data = {\n                \"suppressions\": [s.to_dict() for s in self._suppressions]\n            }\n\n            with open(self.path, \"w\") as f:\n                yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n\n            logger.info(f\"Saved {len(self._suppressions)} suppressions to {self.path}\")\n            return True\n\n        except Exception as e:\n            logger.error(f\"Failed to save suppressions: {e}\")\n            return False\n\n    def add(\n        self,\n        source_pattern: str,\n        target_pattern: str,\n        reason: str = \"\",\n        created_by: str = \"user\",\n        expires_at: Optional[datetime] = None,\n    ) -&gt; Suppression:\n        \"\"\"\n        Add a new suppression.\n\n        Args:\n            source_pattern: Glob pattern for source\n            target_pattern: Glob pattern for target\n            reason: Why this suppression exists\n            created_by: Who created it\n            expires_at: Optional expiration datetime\n\n        Returns:\n            The created Suppression\n        \"\"\"\n        if not self._loaded:\n            self.load()\n\n        suppression = Suppression(\n            source_pattern=source_pattern,\n            target_pattern=target_pattern,\n            reason=reason,\n            created_by=created_by,\n            expires_at=expires_at,\n        )\n\n        self._suppressions.append(suppression)\n        logger.info(f\"Added suppression: {source_pattern} \u2192 {target_pattern}\")\n\n        return suppression\n\n    def remove(self, suppression_id: str) -&gt; bool:\n        \"\"\"\n        Remove a suppression by ID.\n\n        Args:\n            suppression_id: ID of suppression to remove\n\n        Returns:\n            True if removed, False if not found\n        \"\"\"\n        if not self._loaded:\n            self.load()\n\n        for i, s in enumerate(self._suppressions):\n            if s.id == suppression_id:\n                removed = self._suppressions.pop(i)\n                logger.info(f\"Removed suppression: {removed.source_pattern} \u2192 {removed.target_pattern}\")\n                return True\n\n        logger.warning(f\"Suppression not found: {suppression_id}\")\n        return False\n\n    def remove_by_index(self, index: int) -&gt; bool:\n        \"\"\"\n        Remove a suppression by list index (1-based for CLI friendliness).\n\n        Args:\n            index: 1-based index\n\n        Returns:\n            True if removed\n        \"\"\"\n        if not self._loaded:\n            self.load()\n\n        zero_index = index - 1\n        if 0 &lt;= zero_index &lt; len(self._suppressions):\n            removed = self._suppressions.pop(zero_index)\n            logger.info(f\"Removed suppression #{index}: {removed.source_pattern} \u2192 {removed.target_pattern}\")\n            return True\n\n        logger.warning(f\"Invalid suppression index: {index}\")\n        return False\n\n    def list(self, include_expired: bool = False) -&gt; List[Suppression]:\n        \"\"\"\n        List all suppressions.\n\n        Args:\n            include_expired: Whether to include expired suppressions\n\n        Returns:\n            List of suppressions\n        \"\"\"\n        if not self._loaded:\n            self.load()\n\n        if include_expired:\n            return list(self._suppressions)\n\n        return [s for s in self._suppressions if not s.is_expired()]\n\n    def is_suppressed(self, source_id: str, target_id: str) -&gt; SuppressionMatch:\n        \"\"\"\n        Check if an edge should be suppressed.\n\n        Args:\n            source_id: Source node ID\n            target_id: Target node ID\n\n        Returns:\n            SuppressionMatch with result and details\n        \"\"\"\n        if not self._loaded:\n            self.load()\n\n        for suppression in self._suppressions:\n            if suppression.matches(source_id, target_id):\n                logger.debug(\n                    f\"Suppressed: {source_id} \u2192 {target_id} \"\n                    f\"(pattern: {suppression.source_pattern} \u2192 {suppression.target_pattern})\"\n                )\n                return SuppressionMatch(\n                    suppressed=True,\n                    suppression=suppression,\n                    reason=suppression.reason or \"Matched suppression rule\",\n                )\n\n        return SuppressionMatch(suppressed=False)\n\n    def get_by_id(self, suppression_id: str) -&gt; Optional[Suppression]:\n        \"\"\"Get a suppression by ID.\"\"\"\n        if not self._loaded:\n            self.load()\n\n        for s in self._suppressions:\n            if s.id == suppression_id:\n                return s\n        return None\n\n    def clear_expired(self) -&gt; int:\n        \"\"\"\n        Remove all expired suppressions.\n\n        Returns:\n            Number of suppressions removed\n        \"\"\"\n        if not self._loaded:\n            self.load()\n\n        original_count = len(self._suppressions)\n        self._suppressions = [s for s in self._suppressions if not s.is_expired()]\n        removed = original_count - len(self._suppressions)\n\n        if removed &gt; 0:\n            logger.info(f\"Cleared {removed} expired suppressions\")\n\n        return removed\n\n    def find_matching(self, source_id: str, target_id: str) -&gt; List[Suppression]:\n        \"\"\"\n        Find all suppressions that match a source/target pair.\n\n        Useful for debugging why something is/isn't suppressed.\n        \"\"\"\n        if not self._loaded:\n            self.load()\n\n        return [s for s in self._suppressions if s.matches(source_id, target_id)]\n\n    @property\n    def count(self) -&gt; int:\n        \"\"\"Number of suppressions.\"\"\"\n        if not self._loaded:\n            self.load()\n        return len(self._suppressions)\n\n    @property\n    def active_count(self) -&gt; int:\n        \"\"\"Number of active (non-expired) suppressions.\"\"\"\n        if not self._loaded:\n            self.load()\n        return len([s for s in self._suppressions if s.is_active()])\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore--add-suppression","title":"Add suppression","text":"<p>store.add(Suppression(     source_pattern=\"env:HOST\",     target_pattern=\"infra:*\",     reason=\"HOST is too generic\" ))</p>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore--check-if-edge-is-suppressed","title":"Check if edge is suppressed","text":"<p>if store.is_suppressed(\"env:HOST\", \"infra:main\"):     print(\"Suppressed!\")</p>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore--save-changes","title":"Save changes","text":"<p>store.save()</p>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore-attributes","title":"Attributes","text":""},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.active_count","title":"<code>active_count</code>  <code>property</code>","text":"<p>Number of active (non-expired) suppressions.</p>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.count","title":"<code>count</code>  <code>property</code>","text":"<p>Number of suppressions.</p>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore-functions","title":"Functions","text":""},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.__init__","title":"<code>__init__(path=None)</code>","text":"<p>Initialize the suppression store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[Path]</code> <p>Path to YAML file. Defaults to .jnkn/suppressions.yaml</p> <code>None</code> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def __init__(self, path: Optional[Path] = None):\n    \"\"\"\n    Initialize the suppression store.\n\n    Args:\n        path: Path to YAML file. Defaults to .jnkn/suppressions.yaml\n    \"\"\"\n    self.path = path or self.DEFAULT_PATH\n    self._suppressions: List[Suppression] = []\n    self._loaded = False\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.add","title":"<code>add(source_pattern, target_pattern, reason='', created_by='user', expires_at=None)</code>","text":"<p>Add a new suppression.</p> <p>Parameters:</p> Name Type Description Default <code>source_pattern</code> <code>str</code> <p>Glob pattern for source</p> required <code>target_pattern</code> <code>str</code> <p>Glob pattern for target</p> required <code>reason</code> <code>str</code> <p>Why this suppression exists</p> <code>''</code> <code>created_by</code> <code>str</code> <p>Who created it</p> <code>'user'</code> <code>expires_at</code> <code>Optional[datetime]</code> <p>Optional expiration datetime</p> <code>None</code> <p>Returns:</p> Type Description <code>Suppression</code> <p>The created Suppression</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def add(\n    self,\n    source_pattern: str,\n    target_pattern: str,\n    reason: str = \"\",\n    created_by: str = \"user\",\n    expires_at: Optional[datetime] = None,\n) -&gt; Suppression:\n    \"\"\"\n    Add a new suppression.\n\n    Args:\n        source_pattern: Glob pattern for source\n        target_pattern: Glob pattern for target\n        reason: Why this suppression exists\n        created_by: Who created it\n        expires_at: Optional expiration datetime\n\n    Returns:\n        The created Suppression\n    \"\"\"\n    if not self._loaded:\n        self.load()\n\n    suppression = Suppression(\n        source_pattern=source_pattern,\n        target_pattern=target_pattern,\n        reason=reason,\n        created_by=created_by,\n        expires_at=expires_at,\n    )\n\n    self._suppressions.append(suppression)\n    logger.info(f\"Added suppression: {source_pattern} \u2192 {target_pattern}\")\n\n    return suppression\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.clear_expired","title":"<code>clear_expired()</code>","text":"<p>Remove all expired suppressions.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of suppressions removed</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def clear_expired(self) -&gt; int:\n    \"\"\"\n    Remove all expired suppressions.\n\n    Returns:\n        Number of suppressions removed\n    \"\"\"\n    if not self._loaded:\n        self.load()\n\n    original_count = len(self._suppressions)\n    self._suppressions = [s for s in self._suppressions if not s.is_expired()]\n    removed = original_count - len(self._suppressions)\n\n    if removed &gt; 0:\n        logger.info(f\"Cleared {removed} expired suppressions\")\n\n    return removed\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.find_matching","title":"<code>find_matching(source_id, target_id)</code>","text":"<p>Find all suppressions that match a source/target pair.</p> <p>Useful for debugging why something is/isn't suppressed.</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def find_matching(self, source_id: str, target_id: str) -&gt; List[Suppression]:\n    \"\"\"\n    Find all suppressions that match a source/target pair.\n\n    Useful for debugging why something is/isn't suppressed.\n    \"\"\"\n    if not self._loaded:\n        self.load()\n\n    return [s for s in self._suppressions if s.matches(source_id, target_id)]\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.get_by_id","title":"<code>get_by_id(suppression_id)</code>","text":"<p>Get a suppression by ID.</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def get_by_id(self, suppression_id: str) -&gt; Optional[Suppression]:\n    \"\"\"Get a suppression by ID.\"\"\"\n    if not self._loaded:\n        self.load()\n\n    for s in self._suppressions:\n        if s.id == suppression_id:\n            return s\n    return None\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.is_suppressed","title":"<code>is_suppressed(source_id, target_id)</code>","text":"<p>Check if an edge should be suppressed.</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>str</code> <p>Source node ID</p> required <code>target_id</code> <code>str</code> <p>Target node ID</p> required <p>Returns:</p> Type Description <code>SuppressionMatch</code> <p>SuppressionMatch with result and details</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def is_suppressed(self, source_id: str, target_id: str) -&gt; SuppressionMatch:\n    \"\"\"\n    Check if an edge should be suppressed.\n\n    Args:\n        source_id: Source node ID\n        target_id: Target node ID\n\n    Returns:\n        SuppressionMatch with result and details\n    \"\"\"\n    if not self._loaded:\n        self.load()\n\n    for suppression in self._suppressions:\n        if suppression.matches(source_id, target_id):\n            logger.debug(\n                f\"Suppressed: {source_id} \u2192 {target_id} \"\n                f\"(pattern: {suppression.source_pattern} \u2192 {suppression.target_pattern})\"\n            )\n            return SuppressionMatch(\n                suppressed=True,\n                suppression=suppression,\n                reason=suppression.reason or \"Matched suppression rule\",\n            )\n\n    return SuppressionMatch(suppressed=False)\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.list","title":"<code>list(include_expired=False)</code>","text":"<p>List all suppressions.</p> <p>Parameters:</p> Name Type Description Default <code>include_expired</code> <code>bool</code> <p>Whether to include expired suppressions</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Suppression]</code> <p>List of suppressions</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def list(self, include_expired: bool = False) -&gt; List[Suppression]:\n    \"\"\"\n    List all suppressions.\n\n    Args:\n        include_expired: Whether to include expired suppressions\n\n    Returns:\n        List of suppressions\n    \"\"\"\n    if not self._loaded:\n        self.load()\n\n    if include_expired:\n        return list(self._suppressions)\n\n    return [s for s in self._suppressions if not s.is_expired()]\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.load","title":"<code>load()</code>","text":"<p>Load suppressions from YAML file.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of suppressions loaded</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def load(self) -&gt; int:\n    \"\"\"\n    Load suppressions from YAML file.\n\n    Returns:\n        Number of suppressions loaded\n    \"\"\"\n    if not self.path.exists():\n        logger.debug(f\"Suppressions file not found: {self.path}\")\n        self._loaded = True\n        return 0\n\n    try:\n        with open(self.path, \"r\") as f:\n            data = yaml.safe_load(f)\n\n        if not data:\n            self._suppressions = []\n            self._loaded = True\n            return 0\n\n        suppressions_data = data.get(\"suppressions\", [])\n        self._suppressions = [\n            Suppression.from_dict(s) for s in suppressions_data\n        ]\n\n        self._loaded = True\n        logger.info(f\"Loaded {len(self._suppressions)} suppressions from {self.path}\")\n        return len(self._suppressions)\n\n    except Exception as e:\n        logger.error(f\"Failed to load suppressions: {e}\")\n        self._suppressions = []\n        self._loaded = True\n        return 0\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.remove","title":"<code>remove(suppression_id)</code>","text":"<p>Remove a suppression by ID.</p> <p>Parameters:</p> Name Type Description Default <code>suppression_id</code> <code>str</code> <p>ID of suppression to remove</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if removed, False if not found</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def remove(self, suppression_id: str) -&gt; bool:\n    \"\"\"\n    Remove a suppression by ID.\n\n    Args:\n        suppression_id: ID of suppression to remove\n\n    Returns:\n        True if removed, False if not found\n    \"\"\"\n    if not self._loaded:\n        self.load()\n\n    for i, s in enumerate(self._suppressions):\n        if s.id == suppression_id:\n            removed = self._suppressions.pop(i)\n            logger.info(f\"Removed suppression: {removed.source_pattern} \u2192 {removed.target_pattern}\")\n            return True\n\n    logger.warning(f\"Suppression not found: {suppression_id}\")\n    return False\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.remove_by_index","title":"<code>remove_by_index(index)</code>","text":"<p>Remove a suppression by list index (1-based for CLI friendliness).</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>1-based index</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if removed</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def remove_by_index(self, index: int) -&gt; bool:\n    \"\"\"\n    Remove a suppression by list index (1-based for CLI friendliness).\n\n    Args:\n        index: 1-based index\n\n    Returns:\n        True if removed\n    \"\"\"\n    if not self._loaded:\n        self.load()\n\n    zero_index = index - 1\n    if 0 &lt;= zero_index &lt; len(self._suppressions):\n        removed = self._suppressions.pop(zero_index)\n        logger.info(f\"Removed suppression #{index}: {removed.source_pattern} \u2192 {removed.target_pattern}\")\n        return True\n\n    logger.warning(f\"Invalid suppression index: {index}\")\n    return False\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.SuppressionStore.save","title":"<code>save()</code>","text":"<p>Save suppressions to YAML file.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def save(self) -&gt; bool:\n    \"\"\"\n    Save suppressions to YAML file.\n\n    Returns:\n        True if successful\n    \"\"\"\n    try:\n        # Ensure directory exists\n        self.path.parent.mkdir(parents=True, exist_ok=True)\n\n        data = {\n            \"suppressions\": [s.to_dict() for s in self._suppressions]\n        }\n\n        with open(self.path, \"w\") as f:\n            yaml.dump(data, f, default_flow_style=False, sort_keys=False)\n\n        logger.info(f\"Saved {len(self._suppressions)} suppressions to {self.path}\")\n        return True\n\n    except Exception as e:\n        logger.error(f\"Failed to save suppressions: {e}\")\n        return False\n</code></pre>"},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions-functions","title":"Functions","text":""},{"location":"reference/api/stitching/suppressions/#jnkn.stitching.suppressions.create_default_store","title":"<code>create_default_store(path=None)</code>","text":"<p>Create a SuppressionStore and load from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[Path]</code> <p>Optional path to YAML file</p> <code>None</code> <p>Returns:</p> Type Description <code>SuppressionStore</code> <p>Initialized SuppressionStore</p> Source code in <code>src/jnkn/stitching/suppressions.py</code> <pre><code>def create_default_store(path: Optional[Path] = None) -&gt; SuppressionStore:\n    \"\"\"\n    Create a SuppressionStore and load from disk.\n\n    Args:\n        path: Optional path to YAML file\n\n    Returns:\n        Initialized SuppressionStore\n    \"\"\"\n    store = SuppressionStore(path)\n    store.load()\n    return store\n</code></pre>"},{"location":"reference/cli/","title":"CLI Reference","text":""},{"location":"reference/cli/#global-options","title":"Global Options","text":"<pre><code>jnkn [OPTIONS] COMMAND [ARGS]\n</code></pre> Option Description <code>--db PATH</code> Database path (default: <code>.jnkn/jnkn.db</code>) <code>--config PATH</code> Config file (default: <code>.jnkn/config.yaml</code>) <code>--verbose</code> Enable verbose output <code>--quiet</code> Suppress non-error output <code>--version</code> Show version <code>--help</code> Show help"},{"location":"reference/cli/#commands","title":"Commands","text":""},{"location":"reference/cli/#jnkn-init","title":"<code>jnkn init</code>","text":"<p>Initialize Jnkn in the current directory.</p> <pre><code>jnkn init [OPTIONS]\n</code></pre> Option Description <code>--force</code> Overwrite existing configuration <p>Creates: - <code>.jnkn/config.yaml</code> - <code>.jnkn/suppressions.yaml</code> - <code>.jnknignore</code></p>"},{"location":"reference/cli/#jnkn-scan","title":"<code>jnkn scan</code>","text":"<p>Parse codebase and build dependency graph.</p> <pre><code>jnkn scan [OPTIONS]\n</code></pre> Option Default Description <code>--dir PATH</code> <code>.</code> Directory to scan <code>--full</code> <code>false</code> Force full rescan <code>--jobs N</code> <code>1</code> Parallel workers <code>--files FILE...</code> Scan specific files only <p>Examples:</p> <pre><code>jnkn scan\njnkn scan --dir src/ --dir terraform/\njnkn scan --full --jobs 4\n</code></pre>"},{"location":"reference/cli/#jnkn-blast-radius","title":"<code>jnkn blast-radius</code>","text":"<p>Calculate downstream impact.</p> <pre><code>jnkn blast ARTIFACT [ARTIFACTS...] [OPTIONS]\n</code></pre> Option Default Description <code>--max-depth N</code> <code>-1</code> Max traversal depth (-1 = unlimited) <code>--type TYPE</code> Filter by artifact type <code>--format FMT</code> <code>json</code> Output format: json, markdown, plain, sarif <p>Examples:</p> <pre><code>jnkn blast env:DATABASE_URL\njnkn blast env:X env:Y --max-depth 2\njnkn blast env:X --format markdown\n</code></pre>"},{"location":"reference/cli/#jnkn-explain","title":"<code>jnkn explain</code>","text":"<p>Explain why a match was made (or not made).</p> <pre><code>jnkn explain SOURCE TARGET [OPTIONS]\n</code></pre> Option Description <code>--why-not</code> Explain why match was rejected <code>--alternatives</code> Show other candidates considered <p>Examples:</p> <pre><code>jnkn explain env:DB_HOST infra:db_host\njnkn explain env:HOST infra:main --why-not\n</code></pre>"},{"location":"reference/cli/#jnkn-suppress","title":"<code>jnkn suppress</code>","text":"<p>Manage suppressions.</p> <pre><code>jnkn suppress COMMAND [OPTIONS]\n</code></pre>"},{"location":"reference/cli/#suppress-add","title":"<code>suppress add</code>","text":"<pre><code>jnkn suppress add SOURCE_PATTERN TARGET_PATTERN [OPTIONS]\n</code></pre> Option Description <code>--reason TEXT</code> Reason for suppression <code>--expires DATE</code> Expiration date (ISO format)"},{"location":"reference/cli/#suppress-list","title":"<code>suppress list</code>","text":"<pre><code>jnkn suppress list [OPTIONS]\n</code></pre> Option Description <code>--format FMT</code> Output format: table, json, yaml"},{"location":"reference/cli/#suppress-remove","title":"<code>suppress remove</code>","text":"<pre><code>jnkn suppress remove ID\njnkn suppress remove --source PATTERN --target PATTERN\n</code></pre>"},{"location":"reference/cli/#suppress-test","title":"<code>suppress test</code>","text":"<pre><code>jnkn suppress test SOURCE TARGET\n</code></pre>"},{"location":"reference/cli/#jnkn-stats","title":"<code>jnkn stats</code>","text":"<p>Show graph statistics.</p> <pre><code>jnkn stats [OPTIONS]\n</code></pre> Option Description <code>--nodes</code> Show all nodes <code>--edges</code> Show all edges <code>--format FMT</code> Output format: table, json"},{"location":"reference/cli/#jnkn-diff","title":"<code>jnkn diff</code>","text":"<p>Compare dependency graphs between git refs.</p> <pre><code>jnkn diff [REF1]..[REF2] [OPTIONS]\n</code></pre> Option Description <code>--show WHAT</code> Show: all, added, removed <code>--type TYPE</code> Filter by artifact type <code>--format FMT</code> Output format: json, markdown <p>Examples:</p> <pre><code>jnkn diff main..HEAD\njnkn diff main --show added\n</code></pre>"},{"location":"reference/cli/#jnkn-export","title":"<code>jnkn export</code>","text":"<p>Export the dependency graph.</p> <pre><code>jnkn export [OPTIONS]\n</code></pre> Option Default Description <code>--format FMT</code> <code>json</code> Format: json, graphml, dot, csv <code>--type TYPE</code> Filter by artifact type"},{"location":"reference/cli/#jnkn-clear","title":"<code>jnkn clear</code>","text":"<p>Clear all data.</p> <pre><code>jnkn clear [OPTIONS]\n</code></pre> Option Description <code>--force</code> Skip confirmation"},{"location":"reference/cli/#environment-variables","title":"Environment Variables","text":"Variable Description <code>JUNKAN_DB</code> Database path <code>JUNKAN_CONFIG</code> Config file path <code>JUNKAN_MIN_CONFIDENCE</code> Override confidence threshold <code>JUNKAN_LOG_LEVEL</code> Logging level: DEBUG, INFO, WARNING, ERROR"},{"location":"reference/cli/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General error 2 Configuration error 3 Parse error 10 Impact threshold exceeded (CI mode)"},{"location":"reference/cli/blast-radius/","title":"Blast Radius","text":"<p>Blast Radius Command - Calculate downstream impact.</p> <p>Fixed to ensure consistent semantic impact analysis regardless of storage backend. Prioritizes hydrating the Graph model to leverage bidirectional edge traversal.</p>"},{"location":"reference/cli/blast-radius/#jnkn.cli.commands.blast_radius-classes","title":"Classes","text":""},{"location":"reference/cli/blast-radius/#jnkn.cli.commands.blast_radius-functions","title":"Functions","text":""},{"location":"reference/cli/blast-radius/#jnkn.cli.commands.blast_radius.blast_radius","title":"<code>blast_radius(artifacts, db_path, max_depth, as_json)</code>","text":"<p>Calculate downstream impact for changed artifacts.</p>"},{"location":"reference/cli/diff/","title":"Diff","text":"<p>Diff Command - Semantic Diff Analysis for Data Pipelines.</p> <p>Analyzes what actually changed between two git refs, not just which files changed.</p> Usage"},{"location":"reference/cli/diff/#jnkn.cli.commands.diff--compare-against-main","title":"Compare against main","text":"<p>jnkn diff main HEAD</p>"},{"location":"reference/cli/diff/#jnkn.cli.commands.diff--compare-specific-branches","title":"Compare specific branches","text":"<p>jnkn diff feature-branch main</p>"},{"location":"reference/cli/diff/#jnkn.cli.commands.diff--output-as-json","title":"Output as JSON","text":"<p>jnkn diff main HEAD --format json</p>"},{"location":"reference/cli/diff/#jnkn.cli.commands.diff--only-show-breaking-changes","title":"Only show breaking changes","text":"<p>jnkn diff main HEAD --breaking-only</p>"},{"location":"reference/cli/diff/#jnkn.cli.commands.diff-functions","title":"Functions","text":""},{"location":"reference/cli/diff/#jnkn.cli.commands.diff.diff","title":"<code>diff(base_ref, head_ref, repo, output_format, output, breaking_only, columns_only, fail_on_breaking, quiet)</code>","text":"<p>Analyze semantic changes between git refs.</p> <p>Instead of just showing which files changed, this command analyzes WHAT changed in terms of columns, tables, and lineage.</p> <p>\b Examples:     # Basic usage     jnkn diff main HEAD</p> <pre><code># Compare branches\njnkn diff feature-x origin/main\n\n# CI/CD usage - fail if breaking changes\njnkn diff origin/main HEAD --fail-on-breaking\n\n# Generate markdown report\njnkn diff main HEAD --format markdown &gt; CHANGES.md\n</code></pre> <p>\b Exit Codes:     0 - Success (no breaking changes, or --fail-on-breaking not set)     1 - Breaking changes detected (with --fail-on-breaking)     2 - Error during analysis</p>"},{"location":"reference/cli/explain/","title":"Explain","text":"<p>Explain Command - Show why matches were made.</p> <p>Wraps the ExplanationGenerator for CLI access.</p>"},{"location":"reference/cli/explain/#jnkn.cli.commands.explain-functions","title":"Functions","text":""},{"location":"reference/cli/explain/#jnkn.cli.commands.explain.explain","title":"<code>explain(source_id, target_id, min_confidence, why_not, alternatives)</code>","text":"<p>Explain why a match was made (or not made).</p> <p>Shows the confidence calculation, signals considered, and alternative matches that were rejected.</p> <p>\b Examples:     jnkn explain env:PAYMENT_DB_HOST infra:payment_db_host     jnkn explain env:HOST infra:main --why-not     jnkn explain env:DB_URL infra:database --alternatives</p>"},{"location":"reference/cli/init/","title":"Init","text":"<p>Init Command - Onboarding Automation.</p> <p>This module handles the <code>jnkn init</code> command, which bootstraps a project with a configuration file tailored to the detected technology stack.</p>"},{"location":"reference/cli/init/#jnkn.cli.commands.initialize-classes","title":"Classes","text":""},{"location":"reference/cli/init/#jnkn.cli.commands.initialize-functions","title":"Functions","text":""},{"location":"reference/cli/init/#jnkn.cli.commands.initialize.create_gitignore","title":"<code>create_gitignore(jnkn_dir)</code>","text":"<p>Ensure the .jnkn/ directory is ignored by git.</p>"},{"location":"reference/cli/init/#jnkn.cli.commands.initialize.detect_stack","title":"<code>detect_stack(root_dir)</code>","text":"<p>Heuristically detect technologies used in the directory.</p>"},{"location":"reference/cli/init/#jnkn.cli.commands.initialize.init","title":"<code>init(force, demo)</code>","text":"<p>Initialize Jnkan in the current directory.</p> <p>If --demo is used, a sample project structure is created in ./jnkn-demo and initialized automatically.</p>"},{"location":"reference/cli/scan/","title":"Scan","text":"<p>Scan Command - Parse codebase and build dependency graph.</p>"},{"location":"reference/cli/scan/#jnkn.cli.commands.scan-classes","title":"Classes","text":""},{"location":"reference/cli/scan/#jnkn.cli.commands.scan-functions","title":"Functions","text":""},{"location":"reference/cli/scan/#jnkn.cli.commands.scan.scan","title":"<code>scan(directory, output, verbose, no_recursive)</code>","text":"<p>Scan directory and build dependency graph.</p> <p>Examples:</p> <p>jnkn scan jnkn scan ./src --output my_graph.db</p>"},{"location":"reference/cli/stats/","title":"Stats","text":"<p>Stats and Clear Commands - Graph statistics and cleanup.</p>"},{"location":"reference/cli/stats/#jnkn.cli.commands.stats-functions","title":"Functions","text":""},{"location":"reference/cli/stats/#jnkn.cli.commands.stats.clear","title":"<code>clear(graph_file, clear_all)</code>","text":"<p>Clear graph data. \b Examples:     jnkn clear     jnkn clear --all</p>"},{"location":"reference/cli/stats/#jnkn.cli.commands.stats.stats","title":"<code>stats(graph_file, as_json)</code>","text":"<p>Show graph statistics. Displays node counts, edge counts, and breakdowns by type.</p> <p>\b Examples:     jnkn stats     jnkn stats --json</p>"},{"location":"reference/cli/suppress/","title":"Suppress","text":"<p>Suppress Commands - Manage match suppressions.</p> <p>Allows you to suppress false positive matches that the stitcher creates.</p>"},{"location":"reference/cli/suppress/#jnkn.cli.commands.suppress-functions","title":"Functions","text":""},{"location":"reference/cli/suppress/#jnkn.cli.commands.suppress.suppress","title":"<code>suppress()</code>","text":"<p>Manage match suppressions.</p> <p>Suppressions prevent the stitcher from creating specific matches that you've identified as false positives.</p>"},{"location":"reference/cli/suppress/#jnkn.cli.commands.suppress.suppress_add","title":"<code>suppress_add(source_pattern, target_pattern, reason, created_by, expires_days, config_path)</code>","text":"<p>Add a new suppression rule.</p> Patterns use glob syntax <ul> <li>matches any characters ? matches single character</li> </ul> <p>\b Examples:     jnkn suppress add \"env:_ID\" \"infra:\" -r \"ID fields are generic\"     jnkn suppress add \"env:HOST\" \"infra:*\" -r \"HOST is generic\" -e 30</p>"},{"location":"reference/cli/suppress/#jnkn.cli.commands.suppress.suppress_list","title":"<code>suppress_list(config_path, include_expired, as_json)</code>","text":"<p>List all suppressions.</p> <p>\b Examples:     jnkn suppress list     jnkn suppress list --include-expired     jnkn suppress list --json</p>"},{"location":"reference/cli/suppress/#jnkn.cli.commands.suppress.suppress_remove","title":"<code>suppress_remove(identifier, config_path)</code>","text":"<p>Remove a suppression by ID or index.</p> <p>\b Examples:     jnkn suppress remove abc123     jnkn suppress remove 1</p>"},{"location":"reference/cli/suppress/#jnkn.cli.commands.suppress.suppress_test","title":"<code>suppress_test(source_id, target_id, config_path)</code>","text":"<p>Test if a source/target pair would be suppressed.</p> <p>\b Examples:     jnkn suppress test env:USER_ID infra:main</p>"},{"location":"reference/configuration/environment-vars/","title":"Environment Variables","text":"<p>List of supported environment variables.</p>"},{"location":"reference/configuration/jnkn-yaml/","title":"config.yaml Reference","text":"<p>Main configuration file at <code>.jnkn/config.yaml</code>.</p>"},{"location":"reference/configuration/jnkn-yaml/#full-schema","title":"Full Schema","text":"<pre><code># Jnkn Configuration\n\n# Parsing settings\nparsing:\n  # File extensions to parse (auto-detected if not specified)\n  extensions:\n    python: [\".py\", \".pyi\"]\n    terraform: [\".tf\", \".tfvars\"]\n    kubernetes: [\".yaml\", \".yml\"]\n    javascript: [\".js\", \".ts\", \".mjs\"]\n\n  # Python-specific settings\n  python:\n    # Additional extractors to load\n    extra_extractors:\n      - my_package.MyExtractor\n\n# Stitching settings\nstitching:\n  # Minimum confidence for creating edges (0.0 - 1.0)\n  min_confidence: 0.5\n\n  # Token configuration\n  min_token_length: 3\n\n  # Tokens that provide no signal (ignored in matching)\n  blocked_tokens:\n    - id\n    - key\n    - url\n    - host\n    - port\n\n  # Tokens that reduce confidence when matched alone\n  low_value_tokens:\n    - aws\n    - prod\n    - dev\n    - main\n    - test\n\n  # Confidence calculation weights\n  confidence:\n    signals:\n      exact_match: 1.0\n      normalized_match: 0.9\n      token_overlap_high: 0.85    # &gt; 80% overlap\n      token_overlap_medium: 0.7   # 50-80% overlap\n      suffix_match: 0.6\n      prefix_match: 0.6\n      contains: 0.5\n      single_token: 0.4\n\n    penalties:\n      short_token: 0.5      # Token &lt; 4 chars\n      common_token: 0.7     # Generic words\n      ambiguous: 0.8        # Multiple possible matches\n      low_value: 0.9        # Low-value tokens\n\n  # Per-rule overrides\n  rule_overrides:\n    EnvVarToInfraRule:\n      min_confidence: 0.6\n      blocked_tokens:\n        - host\n    K8sToSecretRule:\n      min_confidence: 0.4\n\n  # Disable specific rules\n  disabled_rules:\n    - SomeUnwantedRule\n\n  # Additional rules to load\n  extra_rules:\n    - my_package.MyRule\n\n# Analysis settings\nanalysis:\n  # Default max depth for blast radius (-1 = unlimited)\n  default_max_depth: -1\n\n  # Include transitive dependencies\n  include_transitive: true\n\n# Storage settings\nstorage:\n  # Database path\n  path: .jnkn/jnkn.db\n\n  # Enable WAL mode for better concurrency\n  wal_mode: true\n\n# Output settings\noutput:\n  # Default output format\n  default_format: json\n\n  # Include metadata in output\n  include_metadata: true\n\n  # Pretty print JSON\n  pretty: true\n\n# Logging\nlogging:\n  level: INFO  # DEBUG, INFO, WARNING, ERROR\n  format: \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n</code></pre>"},{"location":"reference/configuration/jnkn-yaml/#minimal-config","title":"Minimal Config","text":"<p>Most settings have sensible defaults. A minimal config:</p> <pre><code>stitching:\n  min_confidence: 0.5\n</code></pre>"},{"location":"reference/configuration/jnkn-yaml/#section-reference","title":"Section Reference","text":""},{"location":"reference/configuration/jnkn-yaml/#parsing","title":"<code>parsing</code>","text":"<p>Controls how files are discovered and parsed.</p> Key Type Default Description <code>extensions</code> dict auto File extensions per language <code>python.extra_extractors</code> list <code>[]</code> Additional Python extractors"},{"location":"reference/configuration/jnkn-yaml/#stitching","title":"<code>stitching</code>","text":"<p>Controls how cross-domain links are created.</p> Key Type Default Description <code>min_confidence</code> float <code>0.5</code> Minimum confidence threshold <code>min_token_length</code> int <code>3</code> Minimum token length <code>blocked_tokens</code> list <code>[]</code> Tokens to ignore <code>low_value_tokens</code> list <code>[]</code> Tokens that reduce confidence <code>confidence.signals</code> dict see above Signal weights <code>confidence.penalties</code> dict see above Penalty multipliers <code>rule_overrides</code> dict <code>{}</code> Per-rule settings <code>disabled_rules</code> list <code>[]</code> Rules to disable <code>extra_rules</code> list <code>[]</code> Additional rules to load"},{"location":"reference/configuration/jnkn-yaml/#analysis","title":"<code>analysis</code>","text":"<p>Controls blast radius and other analysis.</p> Key Type Default Description <code>default_max_depth</code> int <code>-1</code> Default traversal depth <code>include_transitive</code> bool <code>true</code> Include transitive deps"},{"location":"reference/configuration/jnkn-yaml/#storage","title":"<code>storage</code>","text":"<p>Controls the database.</p> Key Type Default Description <code>path</code> string <code>.jnkn/jnkn.db</code> Database file path <code>wal_mode</code> bool <code>true</code> Enable WAL mode"},{"location":"reference/configuration/jnkn-yaml/#output","title":"<code>output</code>","text":"<p>Controls output formatting.</p> Key Type Default Description <code>default_format</code> string <code>json</code> Default output format <code>include_metadata</code> bool <code>true</code> Include extra metadata <code>pretty</code> bool <code>true</code> Pretty print JSON"},{"location":"reference/configuration/jnkn-yaml/#logging","title":"<code>logging</code>","text":"<p>Controls logging.</p> Key Type Default Description <code>level</code> string <code>INFO</code> Log level <code>format</code> string see above Log format string"},{"location":"reference/configuration/jnknanignore/","title":".jnknignore Reference","text":"<p>Exclude files and directories from scanning.</p>"},{"location":"reference/configuration/jnknanignore/#location","title":"Location","text":"<p>Place <code>.jnknignore</code> in your project root (same level as <code>.jnkn/</code>).</p>"},{"location":"reference/configuration/jnknanignore/#syntax","title":"Syntax","text":"<p>Uses gitignore-style patterns.</p> <pre><code># Comments start with #\n\n# Ignore specific file\nsecret.py\n\n# Ignore by extension\n*.test.py\n*_test.go\n\n# Ignore directories\nnode_modules/\nvendor/\n__pycache__/\n.git/\n\n# Ignore paths\ntests/fixtures/\ndocs/\n\n# Negate (un-ignore)\n!important.test.py\n\n# Wildcards\n*.min.js\nbuild-*\n\n# Double star (any depth)\n**/generated/**\n</code></pre>"},{"location":"reference/configuration/jnknanignore/#pattern-reference","title":"Pattern Reference","text":"Pattern Matches Doesn't Match <code>*.py</code> <code>foo.py</code>, <code>dir/bar.py</code> <code>foo.pyc</code> <code>test_*.py</code> <code>test_foo.py</code> <code>foo_test.py</code> <code>tests/</code> <code>tests/foo.py</code> <code>src/tests.py</code> <code>**/test/**</code> <code>a/test/b.py</code>, <code>test/c.py</code> <code>testing/d.py</code> <code>!important.py</code> (negates previous rule)"},{"location":"reference/configuration/jnknanignore/#default-ignores","title":"Default Ignores","text":"<p>Even without <code>.jnknignore</code>, Jnkn skips:</p> <pre><code>.git/\n.hg/\n.svn/\n__pycache__/\n*.pyc\n*.pyo\nnode_modules/\n.terraform/\n</code></pre>"},{"location":"reference/configuration/jnknanignore/#examples","title":"Examples","text":""},{"location":"reference/configuration/jnknanignore/#python-project","title":"Python Project","text":"<pre><code># Virtual environments\nvenv/\n.venv/\nenv/\n\n# Caches\n__pycache__/\n*.pyc\n.pytest_cache/\n.mypy_cache/\n\n# Tests (optional - you may want to scan these)\ntests/\n*_test.py\ntest_*.py\n\n# Generated\n*.egg-info/\ndist/\nbuild/\n</code></pre>"},{"location":"reference/configuration/jnknanignore/#terraform-project","title":"Terraform Project","text":"<pre><code># State files\n*.tfstate\n*.tfstate.backup\n\n# Terraform internals\n.terraform/\n.terraform.lock.hcl\n\n# Environment-specific\nenvironments/dev/\nenvironments/staging/\n</code></pre>"},{"location":"reference/configuration/jnknanignore/#monorepo","title":"Monorepo","text":"<pre><code># Third-party\nvendor/\nnode_modules/\n\n# Generated code\n**/generated/\n**/*.pb.go\n\n# Documentation\ndocs/\n\n# Specific services to skip\nservices/deprecated-*/\n</code></pre>"},{"location":"reference/configuration/jnknanignore/#precedence","title":"Precedence","text":"<ol> <li>Built-in defaults (lowest)</li> <li><code>.jnknignore</code> in project root</li> <li><code>--exclude</code> CLI flag (highest)</li> </ol>"},{"location":"reference/configuration/jnknanignore/#debugging","title":"Debugging","text":"<p>See what's being ignored:</p> <pre><code>jnkn scan --verbose\n</code></pre> <p>Output shows:</p> <pre><code>Ignored: tests/test_foo.py (matched: tests/)\nIgnored: build/output.py (matched: build/)\nScanning: src/main.py\n</code></pre>"},{"location":"reference/configuration/suppressions-yaml/","title":"suppressions.yaml Reference","text":"<p>Manage false positive suppressions at <code>.jnkn/suppressions.yaml</code>.</p>"},{"location":"reference/configuration/suppressions-yaml/#schema","title":"Schema","text":"<pre><code>suppressions:\n  - id: 1\n    source_pattern: \"env:*_ID\"\n    target_pattern: \"infra:*\"\n    reason: \"ID fields are too generic for matching\"\n    enabled: true\n    expires: null\n    created_at: \"2024-01-15T10:30:00Z\"\n    created_by: \"user@example.com\"\n\n  - id: 2\n    source_pattern: \"env:HOST\"\n    target_pattern: \"infra:ghost_*\"\n    reason: \"ghost_writer is unrelated logging service\"\n    enabled: true\n    expires: \"2024-06-01\"\n    created_at: \"2024-01-15T10:35:00Z\"\n    created_by: null\n</code></pre>"},{"location":"reference/configuration/suppressions-yaml/#fields","title":"Fields","text":"Field Type Required Description <code>id</code> integer yes Unique identifier <code>source_pattern</code> string yes Glob pattern for source artifact <code>target_pattern</code> string yes Glob pattern for target artifact <code>reason</code> string no Why this suppression exists <code>enabled</code> boolean no Whether suppression is active (default: true) <code>expires</code> string no ISO 8601 date when suppression expires <code>created_at</code> string no ISO 8601 timestamp of creation <code>created_by</code> string no Who created this suppression"},{"location":"reference/configuration/suppressions-yaml/#glob-patterns","title":"Glob Patterns","text":"Pattern Matches <code>*</code> Any characters <code>?</code> Single character <code>[abc]</code> Character class <code>[!abc]</code> Negated character class"},{"location":"reference/configuration/suppressions-yaml/#examples","title":"Examples","text":"Pattern Matches Doesn't Match <code>env:*</code> <code>env:X</code>, <code>env:DATABASE_URL</code> <code>infra:X</code> <code>env:DB_*</code> <code>env:DB_HOST</code>, <code>env:DB_PORT</code> <code>env:DATABASE</code> <code>*_URL</code> <code>env:DATABASE_URL</code>, <code>infra:api_url</code> <code>env:URL_PREFIX</code> <code>env:???</code> <code>env:ABC</code>, <code>env:XYZ</code> <code>env:ABCD</code> <code>env:[A-Z]*</code> <code>env:ABC</code> <code>env:abc</code>"},{"location":"reference/configuration/suppressions-yaml/#order-of-evaluation","title":"Order of Evaluation","text":"<p>Suppressions are evaluated in order. First match wins.</p> <pre><code>suppressions:\n  # This matches first\n  - source_pattern: \"env:DATABASE_URL\"\n    target_pattern: \"infra:main_db\"\n    reason: \"Specific exception\"\n    enabled: true\n\n  # This is more general but evaluated second\n  - source_pattern: \"env:*\"\n    target_pattern: \"infra:*\"\n    reason: \"Broad suppression\"\n    enabled: true\n</code></pre>"},{"location":"reference/configuration/suppressions-yaml/#expiration","title":"Expiration","text":"<p>Suppressions can expire:</p> <pre><code>- source_pattern: \"env:TEMP_*\"\n  target_pattern: \"*\"\n  reason: \"Temporary during migration\"\n  expires: \"2024-06-01\"  # Ignored after this date\n</code></pre> <p>Expired suppressions remain in the file but are not applied.</p>"},{"location":"reference/configuration/suppressions-yaml/#disabling","title":"Disabling","text":"<p>Disable without removing:</p> <pre><code>- source_pattern: \"env:X\"\n  target_pattern: \"infra:Y\"\n  enabled: false  # Kept for reference but not applied\n</code></pre>"},{"location":"reference/configuration/suppressions-yaml/#managing-via-cli","title":"Managing via CLI","text":"<pre><code># Add\njnkn suppress add \"env:*_ID\" \"infra:*\" --reason \"Generic\"\n\n# List\njnkn suppress list\n\n# Remove\njnkn suppress remove 1\n\n# Enable/disable\njnkn suppress disable 2\njnkn suppress enable 2\n\n# Test\njnkn suppress test env:USER_ID infra:user_service\n</code></pre>"},{"location":"reference/configuration/suppressions-yaml/#best-practices","title":"Best Practices","text":"<ol> <li>Always add a reason \u2014 Future you will thank you</li> <li>Use specific patterns \u2014 Avoid <code>*</code> \u2192 <code>*</code></li> <li>Set expiration for temporary suppressions</li> <li>Commit to version control \u2014 Share with team</li> <li>Review periodically \u2014 Remove stale suppressions</li> </ol>"},{"location":"reference/output-formats/csv/","title":"CSV","text":"<p>CSV output format.</p>"},{"location":"reference/output-formats/json-schema/","title":"JSON Output Schema","text":"<p>Reference for Jnkn's JSON output formats.</p>"},{"location":"reference/output-formats/json-schema/#blast-radius","title":"Blast Radius","text":"<pre><code>jnkn blast env:DATABASE_URL --format json\n</code></pre> <pre><code>{\n  \"source_artifacts\": [\"env:DATABASE_URL\"],\n  \"total_impacted_count\": 5,\n  \"impacted_artifacts\": [\n    \"file://src/db/connection.py\",\n    \"file://src/api/users.py\",\n    \"infra:aws_db_instance.main\",\n    \"k8s:default/deployment/api\",\n    \"k8s:default/secret/db-creds\"\n  ],\n  \"breakdown\": {\n    \"code\": [\n      \"file://src/db/connection.py\",\n      \"file://src/api/users.py\"\n    ],\n    \"infra\": [\n      \"infra:aws_db_instance.main\"\n    ],\n    \"k8s\": [\n      \"k8s:default/deployment/api\",\n      \"k8s:default/secret/db-creds\"\n    ],\n    \"env\": [],\n    \"data\": []\n  },\n  \"max_depth_reached\": 2,\n  \"paths\": [\n    {\n      \"target\": \"file://src/db/connection.py\",\n      \"path\": [\"env:DATABASE_URL\", \"file://src/db/connection.py\"],\n      \"depth\": 1\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/output-formats/json-schema/#fields","title":"Fields","text":"Field Type Description <code>source_artifacts</code> string[] Input artifact IDs <code>total_impacted_count</code> integer Total downstream artifacts <code>impacted_artifacts</code> string[] All impacted artifact IDs <code>breakdown</code> object Artifacts grouped by type <code>max_depth_reached</code> integer Deepest traversal level <code>paths</code> object[] (Optional) Paths to each artifact"},{"location":"reference/output-formats/json-schema/#stats","title":"Stats","text":"<pre><code>jnkn stats --format json\n</code></pre> <pre><code>{\n  \"version\": \"0.1.0\",\n  \"schema_version\": 2,\n  \"database_path\": \".jnkn/jnkn.db\",\n  \"database_size_bytes\": 124500,\n  \"total_nodes\": 156,\n  \"total_edges\": 97,\n  \"tracked_files\": 47,\n  \"nodes_by_type\": {\n    \"code_file\": 42,\n    \"env_var\": 12,\n    \"infra_resource\": 8,\n    \"k8s_resource\": 6\n  },\n  \"edges_by_type\": {\n    \"reads\": 45,\n    \"imports\": 32,\n    \"provides\": 12,\n    \"configures\": 8\n  },\n  \"cross_domain_edges\": 8,\n  \"last_scan\": \"2024-01-15T10:30:00Z\"\n}\n</code></pre>"},{"location":"reference/output-formats/json-schema/#explain","title":"Explain","text":"<pre><code>jnkn explain env:DB_HOST infra:db_host --format json\n</code></pre> <pre><code>{\n  \"source\": {\n    \"id\": \"env:DB_HOST\",\n    \"type\": \"env_var\",\n    \"tokens\": [\"db\", \"host\"]\n  },\n  \"target\": {\n    \"id\": \"infra:db_host\",\n    \"type\": \"infra_resource\",\n    \"tokens\": [\"db\", \"host\"]\n  },\n  \"confidence\": {\n    \"score\": 0.85,\n    \"level\": \"HIGH\",\n    \"signals\": [\n      {\n        \"name\": \"normalized_match\",\n        \"weight\": 0.9,\n        \"description\": \"'dbhost' == 'dbhost'\"\n      }\n    ],\n    \"penalties\": [\n      {\n        \"name\": \"short_token\",\n        \"multiplier\": 0.95,\n        \"description\": \"Token 'db' is only 2 chars\"\n      }\n    ]\n  },\n  \"matched_tokens\": [\"db\", \"host\"],\n  \"would_create_edge\": true,\n  \"suppressed\": false\n}\n</code></pre>"},{"location":"reference/output-formats/json-schema/#diff","title":"Diff","text":"<pre><code>jnkn diff main..HEAD --format json\n</code></pre> <pre><code>{\n  \"base_ref\": \"main\",\n  \"target_ref\": \"HEAD\",\n  \"summary\": {\n    \"nodes_added\": 3,\n    \"nodes_removed\": 1,\n    \"edges_added\": 5,\n    \"edges_removed\": 2\n  },\n  \"added_nodes\": [\n    {\n      \"id\": \"env:NEW_VAR\",\n      \"type\": \"env_var\",\n      \"metadata\": {\n        \"file\": \"src/config.py\",\n        \"line\": 15\n      }\n    }\n  ],\n  \"removed_nodes\": [\n    {\n      \"id\": \"env:OLD_VAR\",\n      \"type\": \"env_var\"\n    }\n  ],\n  \"added_edges\": [\n    {\n      \"source\": \"file://src/new.py\",\n      \"target\": \"env:NEW_VAR\",\n      \"type\": \"reads\"\n    }\n  ],\n  \"removed_edges\": [\n    {\n      \"source\": \"file://src/old.py\",\n      \"target\": \"env:OLD_VAR\",\n      \"type\": \"reads\"\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/output-formats/json-schema/#export-full-graph","title":"Export (Full Graph)","text":"<pre><code>jnkn export --format json\n</code></pre> <pre><code>{\n  \"version\": \"0.1.0\",\n  \"exported_at\": \"2024-01-15T10:30:00Z\",\n  \"nodes\": [\n    {\n      \"id\": \"env:DATABASE_URL\",\n      \"name\": \"DATABASE_URL\",\n      \"type\": \"env_var\",\n      \"metadata\": {\n        \"source\": \"os.getenv\",\n        \"file\": \"src/config.py\",\n        \"line\": 10\n      }\n    },\n    {\n      \"id\": \"file://src/config.py\",\n      \"name\": \"config.py\",\n      \"type\": \"code_file\",\n      \"path\": \"src/config.py\",\n      \"language\": \"python\",\n      \"file_hash\": \"abc123...\"\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"file://src/config.py\",\n      \"target\": \"env:DATABASE_URL\",\n      \"type\": \"reads\",\n      \"metadata\": {\n        \"pattern\": \"os.getenv\",\n        \"confidence\": 1.0\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"reference/output-formats/json-schema/#node-types","title":"Node Types","text":"Type Description <code>code_file</code> Source code file <code>code_entity</code> Function/class definition <code>env_var</code> Environment variable <code>infra_resource</code> Terraform resource <code>k8s_resource</code> Kubernetes resource <code>data_asset</code> dbt model/source <code>unknown</code> Unresolved reference"},{"location":"reference/output-formats/json-schema/#edge-types","title":"Edge Types","text":"Type Description <code>reads</code> Source reads from target <code>imports</code> Source imports target <code>provides</code> Source provides target <code>configures</code> Source configures target <code>contains</code> Source contains target <code>references</code> Generic reference"},{"location":"reference/output-formats/sarif/","title":"SARIF","text":"<p>SARIF output format.</p>"},{"location":"reference/patterns/","title":"Supported Patterns","text":"<p>Jnkn detects environment variables and configuration patterns across multiple languages and frameworks.</p>"},{"location":"reference/patterns/#how-detection-works","title":"How Detection Works","text":"<ol> <li>Parse \u2014 Tree-sitter parses source code into an AST</li> <li>Extract \u2014 Pattern extractors find specific code patterns</li> <li>Normalize \u2014 Variable names are tokenized for matching</li> </ol> <pre><code>graph LR\n    F[Source File] --&gt; P[Parser]\n    P --&gt; A[AST]\n    A --&gt; E1[Extractor 1]\n    A --&gt; E2[Extractor 2]\n    A --&gt; E3[Extractor N]\n    E1 --&gt; N[Nodes]\n    E2 --&gt; N\n    E3 --&gt; N</code></pre>"},{"location":"reference/patterns/#supported-languages","title":"Supported Languages","text":"Language Patterns Status Python os.getenv, Pydantic, Click, django-environ \u2705 Full Terraform Resources, variables, outputs \u2705 Full Kubernetes ConfigMaps, Secrets, env vars \u2705 Full JavaScript process.env, dotenv \ud83d\udea7 Beta dbt ref(), source(), var() \u2705 Full"},{"location":"reference/patterns/#adding-new-patterns","title":"Adding New Patterns","text":"<p>If Jnkn doesn't detect your pattern:</p> <ol> <li>Check if it's a supported variation</li> <li>File a GitHub issue with an example</li> <li>Or write a custom extractor</li> </ol>"},{"location":"reference/patterns/#detection-confidence","title":"Detection Confidence","text":"<p>Each pattern has inherent confidence:</p> Source Base Confidence Notes Explicit (<code>os.getenv(\"VAR\")</code>) 1.0 Direct reference Framework (<code>Field(env=\"VAR\")</code>) 0.95 Framework-specific Inferred (Pydantic field name) 0.85 Name \u2192 env var mapping Heuristic (<code>DATABASE_URL = ...</code>) 0.7 Pattern-based guess"},{"location":"reference/patterns/#testing-detection","title":"Testing Detection","text":"<p>Check if a pattern is detected:</p> <pre><code>jnkn scan --verbose --files myfile.py\n</code></pre> <p>Or test in Python:</p> <pre><code>from jnkn.parsing.python import PythonParser\n\nparser = PythonParser()\ncontent = b'DATABASE_URL = os.getenv(\"DATABASE_URL\")'\n\nfor item in parser.parse(Path(\"test.py\"), content):\n    print(item)\n</code></pre>"},{"location":"reference/patterns/dbt-refs/","title":"dbt Patterns","text":"<p>Supported dbt references.</p>"},{"location":"reference/patterns/javascript-env-vars/","title":"JavaScript Patterns","text":"<p>Supported JavaScript patterns.</p>"},{"location":"reference/patterns/kubernetes-refs/","title":"Kubernetes Reference Patterns","text":"<p>All Kubernetes patterns Jnkn detects.</p>"},{"location":"reference/patterns/kubernetes-refs/#environment-variables","title":"Environment Variables","text":""},{"location":"reference/patterns/kubernetes-refs/#direct-values","title":"Direct Values","text":"<pre><code># \u2705 Detected as env:DATABASE_URL\nenv:\n  - name: DATABASE_URL\n    value: \"postgresql://localhost/db\"\n</code></pre>"},{"location":"reference/patterns/kubernetes-refs/#configmap-references","title":"ConfigMap References","text":"<pre><code># \u2705 Detected as env:DATABASE_HOST\n# \u2705 Creates edge to k8s:default/configmap/app-config\nenv:\n  - name: DATABASE_HOST\n    valueFrom:\n      configMapKeyRef:\n        name: app-config\n        key: db-host\n</code></pre>"},{"location":"reference/patterns/kubernetes-refs/#secret-references","title":"Secret References","text":"<pre><code># \u2705 Detected as env:DATABASE_PASSWORD\n# \u2705 Creates edge to k8s:default/secret/app-secrets\nenv:\n  - name: DATABASE_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        name: app-secrets\n        key: db-password\n</code></pre>"},{"location":"reference/patterns/kubernetes-refs/#field-references","title":"Field References","text":"<pre><code># \u2705 Detected as env:POD_NAME\nenv:\n  - name: POD_NAME\n    valueFrom:\n      fieldRef:\n        fieldPath: metadata.name\n</code></pre>"},{"location":"reference/patterns/kubernetes-refs/#envfrom","title":"envFrom","text":""},{"location":"reference/patterns/kubernetes-refs/#all-keys-from-configmap","title":"All Keys from ConfigMap","text":"<pre><code># \u2705 Creates edge to k8s:default/configmap/app-config\nenvFrom:\n  - configMapRef:\n      name: app-config\n</code></pre>"},{"location":"reference/patterns/kubernetes-refs/#all-keys-from-secret","title":"All Keys from Secret","text":"<pre><code># \u2705 Creates edge to k8s:default/secret/app-secrets\nenvFrom:\n  - secretRef:\n      name: app-secrets\n</code></pre>"},{"location":"reference/patterns/kubernetes-refs/#workload-types","title":"Workload Types","text":"<p>Jnkn parses these Kubernetes resources:</p> Kind Detected Deployment \u2705 StatefulSet \u2705 DaemonSet \u2705 Job \u2705 CronJob \u2705 Pod \u2705 ReplicaSet \u2705"},{"location":"reference/patterns/kubernetes-refs/#node-id-format","title":"Node ID Format","text":"<pre><code>k8s:{namespace}/{kind}/{name}\n</code></pre> <p>Examples: - <code>k8s:default/deployment/api-server</code> - <code>k8s:production/statefulset/database</code> - <code>k8s:default/configmap/app-config</code> - <code>k8s:default/secret/app-secrets</code></p>"},{"location":"reference/patterns/kubernetes-refs/#dependencies-detected","title":"Dependencies Detected","text":""},{"location":"reference/patterns/kubernetes-refs/#workload-configmap","title":"Workload \u2192 ConfigMap","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\n  namespace: default\nspec:\n  template:\n    spec:\n      containers:\n        - name: api\n          envFrom:\n            - configMapRef:\n                name: api-config\n# Edge: k8s:default/deployment/api-server \u2192 k8s:default/configmap/api-config\n</code></pre>"},{"location":"reference/patterns/kubernetes-refs/#workload-secret","title":"Workload \u2192 Secret","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  template:\n    spec:\n      containers:\n        - name: api\n          env:\n            - name: DB_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: db-creds\n                  key: password\n# Edge: k8s:default/deployment/api-server \u2192 k8s:default/secret/db-creds\n</code></pre>"},{"location":"reference/patterns/kubernetes-refs/#workload-serviceaccount","title":"Workload \u2192 ServiceAccount","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\nspec:\n  template:\n    spec:\n      serviceAccountName: api-sa\n# Edge: k8s:default/deployment/api-server \u2192 k8s:default/serviceaccount/api-sa\n</code></pre>"},{"location":"reference/patterns/kubernetes-refs/#cross-domain-matching","title":"Cross-Domain Matching","text":"<p>K8s env vars are matched to other domains:</p> K8s Env Var Python Confidence <code>DATABASE_URL</code> <code>os.getenv(\"DATABASE_URL\")</code> HIGH (0.95) <code>API_KEY</code> <code>env:API_KEY</code> HIGH (0.90) K8s ConfigMap Terraform Confidence <code>api-config</code> <code>infra:aws_ssm_parameter.api_config</code> MEDIUM (0.75)"},{"location":"reference/patterns/kubernetes-refs/#multi-document-yaml","title":"Multi-Document YAML","text":"<p>Jnkn handles multi-document YAML files:</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  DATABASE_HOST: \"localhost\"\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  template:\n    spec:\n      containers:\n        - name: app\n          envFrom:\n            - configMapRef:\n                name: app-config\n</code></pre> <p>Both resources are detected and linked.</p>"},{"location":"reference/patterns/kubernetes-refs/#not-detected","title":"Not Detected","text":"<pre><code># \u274c Helm templates (pre-rendered)\nenv:\n  - name: {{ .Values.envName }}\n    value: {{ .Values.envValue }}\n\n# \u274c Kustomize patches (pre-rendered)\n# Process with helm template or kustomize build first\n</code></pre>"},{"location":"reference/patterns/python-env-vars/","title":"Python Environment Variable Patterns","text":"<p>All Python patterns Jnkn detects.</p>"},{"location":"reference/patterns/python-env-vars/#standard-library","title":"Standard Library","text":""},{"location":"reference/patterns/python-env-vars/#osgetenv","title":"<code>os.getenv()</code>","text":"<pre><code># \u2705 Detected\nos.getenv(\"DATABASE_URL\")\nos.getenv(\"DATABASE_URL\", \"default\")\nos.getenv(\"DATABASE_URL\", default=\"default\")\n\n# \u274c Not detected (dynamic key)\nkey = \"DATABASE_URL\"\nos.getenv(key)\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#osenvironget","title":"<code>os.environ.get()</code>","text":"<pre><code># \u2705 Detected\nos.environ.get(\"DATABASE_URL\")\nos.environ.get(\"DATABASE_URL\", \"default\")\n\n# \u274c Not detected\nos.environ.get(key)\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#osenviron","title":"<code>os.environ[]</code>","text":"<pre><code># \u2705 Detected\nos.environ[\"DATABASE_URL\"]\n\n# \u274c Not detected\nos.environ[key]\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#from-imports","title":"From-imports","text":"<pre><code>from os import getenv, environ\n\n# \u2705 Detected\ngetenv(\"DATABASE_URL\")\nenviron.get(\"DATABASE_URL\")\nenviron[\"DATABASE_URL\"]\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#pydantic-settings","title":"Pydantic Settings","text":""},{"location":"reference/patterns/python-env-vars/#basesettings-fields","title":"BaseSettings Fields","text":"<pre><code>from pydantic_settings import BaseSettings\n\nclass Settings(BaseSettings):\n    # \u2705 Detected as env:DATABASE_URL\n    database_url: str\n\n    # \u2705 Detected as env:API_KEY\n    api_key: str = \"default\"\n\n    # \u2705 Detected as env:DEBUG\n    debug: bool = False\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#field-with-env","title":"Field with <code>env=</code>","text":"<pre><code>from pydantic import Field\nfrom pydantic_settings import BaseSettings\n\nclass Settings(BaseSettings):\n    # \u2705 Detected as env:DB_URL (explicit override)\n    database_url: str = Field(env=\"DB_URL\")\n\n    # \u2705 Detected as env:MY_API_KEY\n    api_key: str = Field(env=\"MY_API_KEY\")\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#env_prefix","title":"<code>env_prefix</code>","text":"<pre><code>class Settings(BaseSettings):\n    # \u2705 Detected as env:APP_DATABASE_URL\n    database_url: str\n\n    # \u2705 Detected as env:APP_DEBUG\n    debug: bool = False\n\n    class Config:\n        env_prefix = \"APP_\"\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#model_config","title":"<code>model_config</code>","text":"<pre><code>class Settings(BaseSettings):\n    database_url: str  # \u2705 env:MYAPP_DATABASE_URL\n\n    model_config = SettingsConfigDict(env_prefix=\"MYAPP_\")\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#click-typer","title":"Click / Typer","text":""},{"location":"reference/patterns/python-env-vars/#click-envvar","title":"Click <code>envvar=</code>","text":"<pre><code>import click\n\n@click.command()\n@click.option(\"--host\", envvar=\"API_HOST\")  # \u2705 Detected\n@click.option(\"--port\", envvar=\"API_PORT\", type=int)  # \u2705 Detected\n@click.option(\"--db\", envvar=[\"DB_URL\", \"DATABASE_URL\"])  # \u2705 Both detected\ndef main(host, port, db):\n    pass\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#typer-envvar","title":"Typer <code>envvar=</code>","text":"<pre><code>import typer\n\napp = typer.Typer()\n\n@app.command()\ndef main(\n    host: str = typer.Option(..., envvar=\"API_HOST\"),  # \u2705 Detected\n    port: int = typer.Option(8080, envvar=\"API_PORT\"),  # \u2705 Detected\n):\n    pass\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#django-environ","title":"django-environ","text":""},{"location":"reference/patterns/python-env-vars/#environenv","title":"<code>environ.Env()</code>","text":"<pre><code>import environ\n\nenv = environ.Env()\n\n# \u2705 All detected\nDEBUG = env(\"DEBUG\")\nDATABASE_URL = env.db(\"DATABASE_URL\")\nSECRET_KEY = env.str(\"SECRET_KEY\")\nALLOWED_HOSTS = env.list(\"ALLOWED_HOSTS\")\nCACHE_URL = env.cache(\"CACHE_URL\")\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#type-methods","title":"Type Methods","text":"Method Example Detected As <code>env()</code> <code>env(\"VAR\")</code> <code>env:VAR</code> <code>env.str()</code> <code>env.str(\"VAR\")</code> <code>env:VAR</code> <code>env.bool()</code> <code>env.bool(\"VAR\")</code> <code>env:VAR</code> <code>env.int()</code> <code>env.int(\"VAR\")</code> <code>env:VAR</code> <code>env.float()</code> <code>env.float(\"VAR\")</code> <code>env:VAR</code> <code>env.list()</code> <code>env.list(\"VAR\")</code> <code>env:VAR</code> <code>env.dict()</code> <code>env.dict(\"VAR\")</code> <code>env:VAR</code> <code>env.url()</code> <code>env.url(\"VAR\")</code> <code>env:VAR</code> <code>env.db()</code> <code>env.db(\"VAR\")</code> <code>env:VAR</code> <code>env.cache()</code> <code>env.cache(\"VAR\")</code> <code>env:VAR</code>"},{"location":"reference/patterns/python-env-vars/#python-dotenv","title":"python-dotenv","text":""},{"location":"reference/patterns/python-env-vars/#dotenv_values","title":"<code>dotenv_values()</code>","text":"<pre><code>from dotenv import dotenv_values\n\nconfig = dotenv_values(\".env\")\n\n# \u2705 Detected\nDATABASE_URL = config[\"DATABASE_URL\"]\nAPI_KEY = config.get(\"API_KEY\")\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#environs","title":"environs","text":"<pre><code>from environs import Env\n\nenv = Env()\nenv.read_env()\n\n# \u2705 All detected\nDEBUG = env.bool(\"DEBUG\")\nDATABASE_URL = env.str(\"DATABASE_URL\")\nPORT = env.int(\"PORT\", 8080)\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#heuristic-detection","title":"Heuristic Detection","text":"<p>Jnkn uses heuristics for patterns that look like env vars:</p> <pre><code># \u2705 Detected (heuristic, lower confidence)\nDATABASE_URL = some_config.get(\"DATABASE_URL\")\nAPI_KEY = settings[\"API_KEY\"]\n\n# These variable names suggest env vars:\n# *_URL, *_HOST, *_PORT, *_KEY, *_SECRET, *_TOKEN, *_PASSWORD\n</code></pre>"},{"location":"reference/patterns/python-env-vars/#not-detected","title":"Not Detected","text":"<pre><code># \u274c Dynamic keys\nkey = \"DATABASE_URL\"\nos.getenv(key)\n\n# \u274c Computed keys\nos.getenv(f\"PREFIX_{suffix}\")\n\n# \u274c Comments\n# os.getenv(\"DATABASE_URL\")\n\n# \u274c Strings\ndoc = 'Use os.getenv(\"DATABASE_URL\")'\n\n# \u274c Different module named os\nimport mypackage.os as os\nos.getenv(\"VAR\")  # Not stdlib\n</code></pre>"},{"location":"reference/patterns/terraform-resources/","title":"Terraform Resource Patterns","text":"<p>All Terraform patterns Jnkn detects.</p>"},{"location":"reference/patterns/terraform-resources/#resources","title":"Resources","text":"<pre><code># \u2705 Detected as infra:aws_db_instance.main\nresource \"aws_db_instance\" \"main\" {\n  identifier = \"my-database\"\n  engine     = \"postgres\"\n}\n\n# \u2705 Detected as infra:aws_s3_bucket.data\nresource \"aws_s3_bucket\" \"data\" {\n  bucket = \"my-data-bucket\"\n}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#node-id-format","title":"Node ID Format","text":"<pre><code>infra:{resource_type}.{resource_name}\n</code></pre> <p>Examples: - <code>infra:aws_db_instance.main</code> - <code>infra:aws_s3_bucket.logs</code> - <code>infra:google_compute_instance.web</code></p>"},{"location":"reference/patterns/terraform-resources/#variables","title":"Variables","text":"<pre><code># \u2705 Detected as infra:var.database_url\nvariable \"database_url\" {\n  type        = string\n  description = \"Database connection URL\"\n}\n\n# \u2705 Detected as infra:var.api_key\nvariable \"api_key\" {\n  type      = string\n  sensitive = true\n}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#node-id-format_1","title":"Node ID Format","text":"<pre><code>infra:var.{variable_name}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#outputs","title":"Outputs","text":"<pre><code># \u2705 Detected as infra:output.database_endpoint\noutput \"database_endpoint\" {\n  value = aws_db_instance.main.endpoint\n}\n\n# \u2705 Detected as infra:output.bucket_arn\noutput \"bucket_arn\" {\n  value = aws_s3_bucket.data.arn\n}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#node-id-format_2","title":"Node ID Format","text":"<pre><code>infra:output.{output_name}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#data-sources","title":"Data Sources","text":"<pre><code># \u2705 Detected as infra:data.aws_ami.ubuntu\ndata \"aws_ami\" \"ubuntu\" {\n  most_recent = true\n  owners      = [\"099720109477\"]\n}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#node-id-format_3","title":"Node ID Format","text":"<pre><code>infra:data.{data_type}.{data_name}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#locals","title":"Locals","text":"<pre><code># \u2705 Detected as infra:local.database_host\nlocals {\n  database_host = var.database_url\n  api_endpoint  = \"https://${var.domain}/api\"\n}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#node-id-format_4","title":"Node ID Format","text":"<pre><code>infra:local.{local_name}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#modules","title":"Modules","text":"<pre><code># \u2705 Detected as infra:module.vpc\nmodule \"vpc\" {\n  source = \"./modules/vpc\"\n\n  cidr_block = var.vpc_cidr\n}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#node-id-format_5","title":"Node ID Format","text":"<pre><code>infra:module.{module_name}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#dependencies-detected","title":"Dependencies Detected","text":"<p>Jnkn creates edges for:</p>"},{"location":"reference/patterns/terraform-resources/#variable-references","title":"Variable References","text":"<pre><code>resource \"aws_instance\" \"web\" {\n  ami = var.ami_id  # Edge: infra:aws_instance.web \u2192 infra:var.ami_id\n}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#resource-references","title":"Resource References","text":"<pre><code>resource \"aws_security_group_rule\" \"allow\" {\n  security_group_id = aws_security_group.main.id\n  # Edge: infra:aws_security_group_rule.allow \u2192 infra:aws_security_group.main\n}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#output-references","title":"Output References","text":"<pre><code>output \"instance_ip\" {\n  value = aws_instance.web.public_ip\n  # Edge: infra:output.instance_ip \u2192 infra:aws_instance.web\n}\n</code></pre>"},{"location":"reference/patterns/terraform-resources/#cross-domain-matching","title":"Cross-Domain Matching","text":"<p>Terraform resources are matched to env vars via token matching:</p> Terraform Env Var Match Confidence <code>aws_db_instance.main_db</code> <code>env:MAIN_DB_HOST</code> HIGH (0.85) <code>output.database_url</code> <code>env:DATABASE_URL</code> HIGH (0.92) <code>var.api_key</code> <code>env:API_KEY</code> HIGH (0.90)"},{"location":"reference/patterns/terraform-resources/#not-detected","title":"Not Detected","text":"<pre><code># \u274c Dynamic blocks (partial support)\ndynamic \"setting\" {\n  for_each = var.settings\n  content {\n    name  = setting.value.name\n    value = setting.value.value\n  }\n}\n\n# \u274c Complex expressions\nlocals {\n  name = var.enabled ? \"prod\" : \"dev\"\n}\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Step-by-step guides to learn Jnkn from basics to advanced usage.</p>"},{"location":"tutorials/#learning-path","title":"Learning Path","text":"<pre><code>graph LR\n    B[Beginner] --&gt; I[Intermediate] --&gt; A[Advanced]\n\n    B --&gt; B1[Scan Python]\n    B --&gt; B2[Blast Radius]\n    B --&gt; B3[Fix False Positives]\n\n    I --&gt; I1[Multi-Stack]\n    I --&gt; I2[CI/CD]\n\n    A --&gt; A1[Custom Parsers]\n    A --&gt; A2[Custom Rules]</code></pre>"},{"location":"tutorials/#beginner","title":"Beginner","text":"<p>Start here if you're new to Jnkn.</p> Tutorial Time What You'll Learn Scanning a Python Project 10 min Detect env vars in FastAPI/Django Understanding Blast Radius 15 min Read and interpret results Fixing False Positives 10 min Suppressions and tuning"},{"location":"tutorials/#intermediate","title":"Intermediate","text":"<p>For teams integrating Jnkn into workflows.</p> Tutorial Time What You'll Learn Multi-Stack Analysis 20 min Python + Terraform + dbt CI/CD Integration 25 min Full pipeline setup"},{"location":"tutorials/#advanced","title":"Advanced","text":"<p>Extend Jnkn for your specific needs.</p> Tutorial Time What You'll Learn Custom Parsers 30 min Add new language support Custom Stitching Rules 30 min Domain-specific matching"},{"location":"tutorials/advanced/custom-parsers/","title":"Custom Parsers","text":"<p>Extend Jnkn to detect patterns in new languages or frameworks.</p> <p>Time: 30 minutes</p>"},{"location":"tutorials/advanced/custom-parsers/#when-to-write-a-custom-parser","title":"When to Write a Custom Parser","text":"<ul> <li>Your framework has unique configuration patterns</li> <li>You use an unsupported language</li> <li>You have internal conventions Jnkn doesn't recognize</li> </ul>"},{"location":"tutorials/advanced/custom-parsers/#architecture","title":"Architecture","text":"<pre><code>graph TD\n    F[File] --&gt; E{Extractor&lt;br/&gt;Registry}\n    E --&gt; E1[StdlibExtractor]\n    E --&gt; E2[PydanticExtractor]\n    E --&gt; E3[YourExtractor]\n    E1 --&gt; N[Nodes + Edges]\n    E2 --&gt; N\n    E3 --&gt; N</code></pre> <p>Parsers are composed of Extractors \u2014 small, focused classes that detect specific patterns.</p>"},{"location":"tutorials/advanced/custom-parsers/#creating-an-extractor","title":"Creating an Extractor","text":""},{"location":"tutorials/advanced/custom-parsers/#step-1-define-the-extractor","title":"Step 1: Define the Extractor","text":"<p>Create <code>my_extractors/internal_config.py</code>:</p> <pre><code>\"\"\"Extractor for internal @config decorator pattern.\"\"\"\n\nfrom pathlib import Path\nfrom typing import Generator, Optional, Set, Union\nimport re\n\nfrom jnkn.parsing.python.extractors.base import BaseExtractor\nfrom jnkn.core.types import Node, Edge, NodeType, RelationshipType\n\n\nclass InternalConfigExtractor(BaseExtractor):\n    \"\"\"\n    Detects env vars from internal @config decorator.\n\n    Pattern:\n        @config(env=\"DATABASE_URL\", required=True)\n        def get_database_url() -&gt; str:\n            ...\n    \"\"\"\n\n    @property\n    def name(self) -&gt; str:\n        return \"internal_config\"\n\n    @property\n    def priority(self) -&gt; int:\n        return 75  # Between stdlib (100) and heuristic (10)\n\n    # Regex pattern for the decorator\n    PATTERN = re.compile(\n        r'@config\\s*\\(\\s*env\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']',\n        re.MULTILINE\n    )\n\n    def can_extract(self, text: str) -&gt; bool:\n        \"\"\"Quick check if this file uses @config.\"\"\"\n        return \"@config\" in text and \"env=\" in text\n\n    def extract(\n        self,\n        file_path: Path,\n        file_id: str,\n        tree,  # tree-sitter AST (optional)\n        text: str,\n        seen_vars: Set[str],\n    ) -&gt; Generator[Union[Node, Edge], None, None]:\n        \"\"\"Extract env vars from @config decorators.\"\"\"\n\n        for match in self.PATTERN.finditer(text):\n            var_name = match.group(1)\n\n            # Skip if already detected by another extractor\n            if var_name in seen_vars:\n                continue\n\n            # Calculate line number\n            line = text[:match.start()].count('\\n') + 1\n\n            env_id = f\"env:{var_name}\"\n\n            # Yield the env var node\n            yield Node(\n                id=env_id,\n                name=var_name,\n                type=NodeType.ENV_VAR,\n                metadata={\n                    \"source\": self.name,\n                    \"file\": str(file_path),\n                    \"line\": line,\n                },\n            )\n\n            # Yield edge: file reads this env var\n            yield Edge(\n                source_id=file_id,\n                target_id=env_id,\n                type=RelationshipType.READS,\n                metadata={\"pattern\": self.name},\n            )\n</code></pre>"},{"location":"tutorials/advanced/custom-parsers/#step-2-register-the-extractor","title":"Step 2: Register the Extractor","text":"<p>Create <code>my_extractors/__init__.py</code>:</p> <pre><code>from .internal_config import InternalConfigExtractor\n\nCUSTOM_EXTRACTORS = [InternalConfigExtractor]\n</code></pre>"},{"location":"tutorials/advanced/custom-parsers/#step-3-configure-jnkn","title":"Step 3: Configure Jnkn","text":"<p>In <code>.jnkn/config.yaml</code>:</p> <pre><code>parsing:\n  python:\n    extra_extractors:\n      - my_extractors.InternalConfigExtractor\n</code></pre> <p>Or programmatically:</p> <pre><code>from jnkn.parsing.python import PythonParser\nfrom my_extractors import InternalConfigExtractor\n\nparser = PythonParser()\nparser.register_extractor(InternalConfigExtractor())\n</code></pre>"},{"location":"tutorials/advanced/custom-parsers/#using-tree-sitter-recommended","title":"Using Tree-Sitter (Recommended)","text":"<p>For complex patterns, use tree-sitter instead of regex:</p> <pre><code>from jnkn.parsing.python.extractors.base import BaseExtractor\n\nclass TypedConfigExtractor(BaseExtractor):\n    \"\"\"Extract env vars using tree-sitter AST.\"\"\"\n\n    # Tree-sitter query\n    QUERY = \"\"\"\n    (decorated_definition\n      (decorator\n        (call\n          function: (identifier) @_func\n          arguments: (argument_list\n            (keyword_argument\n              name: (identifier) @_kwarg\n              value: (string) @env_var))))\n      (#eq? @_func \"config\")\n      (#eq? @_kwarg \"env\"))\n    \"\"\"\n\n    def extract(self, file_path, file_id, tree, text, seen_vars):\n        if tree is None:\n            return  # Fall back to regex if no tree-sitter\n\n        query = self._language.query(self.QUERY)\n        captures = query.captures(tree.root_node)\n\n        for node, capture_name in captures:\n            if capture_name != \"env_var\":\n                continue\n\n            var_name = node.text.decode(\"utf-8\").strip('\"\\'')\n\n            if var_name in seen_vars:\n                continue\n\n            yield Node(\n                id=f\"env:{var_name}\",\n                name=var_name,\n                type=NodeType.ENV_VAR,\n                metadata={\n                    \"source\": self.name,\n                    \"file\": str(file_path),\n                    \"line\": node.start_point[0] + 1,\n                },\n            )\n\n            yield Edge(\n                source_id=file_id,\n                target_id=f\"env:{var_name}\",\n                type=RelationshipType.READS,\n            )\n</code></pre>"},{"location":"tutorials/advanced/custom-parsers/#testing-your-extractor","title":"Testing Your Extractor","text":"<p>Create <code>tests/test_internal_config.py</code>:</p> <pre><code>import pytest\nfrom pathlib import Path\nfrom my_extractors import InternalConfigExtractor\nfrom jnkn.core.types import NodeType\n\nclass TestInternalConfigExtractor:\n    @pytest.fixture\n    def extractor(self):\n        return InternalConfigExtractor()\n\n    def test_detects_config_decorator(self, extractor):\n        code = '''\n@config(env=\"DATABASE_URL\", required=True)\ndef get_db_url():\n    pass\n\n@config(env=\"API_KEY\")\ndef get_api_key():\n    pass\n'''\n\n        nodes = list(extractor.extract(\n            file_path=Path(\"test.py\"),\n            file_id=\"file://test.py\",\n            tree=None,\n            text=code,\n            seen_vars=set(),\n        ))\n\n        env_nodes = [n for n in nodes if hasattr(n, 'type') and n.type == NodeType.ENV_VAR]\n        env_names = {n.name for n in env_nodes}\n\n        assert env_names == {\"DATABASE_URL\", \"API_KEY\"}\n\n    def test_skips_already_seen(self, extractor):\n        code = '@config(env=\"DATABASE_URL\")\\ndef f(): pass'\n\n        nodes = list(extractor.extract(\n            file_path=Path(\"test.py\"),\n            file_id=\"file://test.py\",\n            tree=None,\n            text=code,\n            seen_vars={\"DATABASE_URL\"},  # Already seen\n        ))\n\n        env_nodes = [n for n in nodes if hasattr(n, 'type') and n.type == NodeType.ENV_VAR]\n        assert len(env_nodes) == 0\n</code></pre> <p>Run tests:</p> <pre><code>pytest tests/test_internal_config.py -v\n</code></pre>"},{"location":"tutorials/advanced/custom-parsers/#complete-example-go-parser","title":"Complete Example: Go Parser","text":"<p>For a new language, create a full parser:</p> <pre><code># jnkn_go/parser.py\nfrom jnkn.parsing.base import LanguageParser\n\nclass GoParser(LanguageParser):\n    @property\n    def name(self) -&gt; str:\n        return \"go\"\n\n    @property\n    def extensions(self) -&gt; list[str]:\n        return [\".go\"]\n\n    def parse(self, file_path, content):\n        # Detect os.Getenv(\"VAR\")\n        pattern = r'os\\.Getenv\\s*\\(\\s*\"([^\"]+)\"\\s*\\)'\n        ...\n</code></pre>"},{"location":"tutorials/advanced/custom-parsers/#next-steps","title":"Next Steps","text":"<ul> <li> Custom stitching rules</li> <li> Python patterns reference</li> </ul>"},{"location":"tutorials/advanced/custom-stitching-rules/","title":"Custom Stitching Rules","text":"<p>Create domain-specific rules for linking artifacts across your stack.</p> <p>Time: 30 minutes</p>"},{"location":"tutorials/advanced/custom-stitching-rules/#when-to-create-custom-rules","title":"When to Create Custom Rules","text":"<ul> <li>Your organization has naming conventions Jnkn doesn't know</li> <li>You need to link artifact types in specific ways</li> <li>Default token matching produces too many false positives</li> </ul>"},{"location":"tutorials/advanced/custom-stitching-rules/#how-stitching-works","title":"How Stitching Works","text":"<pre><code>graph LR\n    N1[Node A] --&gt; R{Rule Engine}\n    N2[Node B] --&gt; R\n    R --&gt; M{Matcher}\n    M --&gt; C[Confidence&lt;br/&gt;Calculator]\n    C --&gt; E[Edge]</code></pre> <p>Rules filter which node pairs to consider. Matchers compute similarity. Confidence scores determine if a link is created.</p>"},{"location":"tutorials/advanced/custom-stitching-rules/#creating-a-custom-rule","title":"Creating a Custom Rule","text":""},{"location":"tutorials/advanced/custom-stitching-rules/#example-link-python-services-to-their-terraform-modules","title":"Example: Link Python Services to Their Terraform Modules","text":"<p>Convention: Python service <code>payment_service</code> should link to Terraform module <code>modules/payment</code>.</p> <pre><code># my_rules/service_to_module.py\nfrom jnkn.stitching.rules import StitchingRule\nfrom jnkn.core.types import Node, NodeType\n\nclass ServiceToModuleRule(StitchingRule):\n    \"\"\"\n    Link Python service files to Terraform modules.\n\n    Convention: src/services/{name}_service.py \u2192 terraform/modules/{name}/\n    \"\"\"\n\n    @property\n    def name(self) -&gt; str:\n        return \"service_to_module\"\n\n    @property\n    def source_types(self) -&gt; set[NodeType]:\n        return {NodeType.CODE_FILE}\n\n    @property\n    def target_types(self) -&gt; set[NodeType]:\n        return {NodeType.INFRA_RESOURCE}\n\n    def should_evaluate(self, source: Node, target: Node) -&gt; bool:\n        \"\"\"Quick filter before expensive matching.\"\"\"\n        # Source must be a Python service file\n        if not source.path:\n            return False\n        if \"_service.py\" not in source.path:\n            return False\n\n        # Target must be a Terraform module resource\n        if not target.id.startswith(\"infra:module.\"):\n            return False\n\n        return True\n\n    def extract_service_name(self, path: str) -&gt; str | None:\n        \"\"\"Extract service name from path like 'src/services/payment_service.py'.\"\"\"\n        import re\n        match = re.search(r'/(\\w+)_service\\.py$', path)\n        return match.group(1) if match else None\n\n    def extract_module_name(self, node_id: str) -&gt; str | None:\n        \"\"\"Extract module name from ID like 'infra:module.payment.aws_rds'.\"\"\"\n        import re\n        match = re.search(r'^infra:module\\.(\\w+)\\.', node_id)\n        return match.group(1) if match else None\n\n    def compute_confidence(self, source: Node, target: Node) -&gt; float:\n        \"\"\"Compute match confidence.\"\"\"\n        service_name = self.extract_service_name(source.path)\n        module_name = self.extract_module_name(target.id)\n\n        if not service_name or not module_name:\n            return 0.0\n\n        # Exact match\n        if service_name == module_name:\n            return 0.95\n\n        # Partial match (e.g., \"payment\" in \"payment_gateway\")\n        if service_name in module_name or module_name in service_name:\n            return 0.75\n\n        return 0.0\n</code></pre>"},{"location":"tutorials/advanced/custom-stitching-rules/#register-the-rule","title":"Register the Rule","text":"<p>In <code>.jnkn/config.yaml</code>:</p> <pre><code>stitching:\n  extra_rules:\n    - my_rules.ServiceToModuleRule\n</code></pre> <p>Or programmatically:</p> <pre><code>from jnkn.stitching import StitchingEngine\nfrom my_rules import ServiceToModuleRule\n\nengine = StitchingEngine()\nengine.register_rule(ServiceToModuleRule())\n</code></pre>"},{"location":"tutorials/advanced/custom-stitching-rules/#rule-interface","title":"Rule Interface","text":"<pre><code>class StitchingRule(ABC):\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Unique identifier for this rule.\"\"\"\n\n    @property\n    @abstractmethod\n    def source_types(self) -&gt; set[NodeType]:\n        \"\"\"Node types this rule matches as sources.\"\"\"\n\n    @property\n    @abstractmethod\n    def target_types(self) -&gt; set[NodeType]:\n        \"\"\"Node types this rule matches as targets.\"\"\"\n\n    @abstractmethod\n    def should_evaluate(self, source: Node, target: Node) -&gt; bool:\n        \"\"\"Quick filter before expensive computation.\"\"\"\n\n    @abstractmethod\n    def compute_confidence(self, source: Node, target: Node) -&gt; float:\n        \"\"\"Return confidence score 0.0-1.0.\"\"\"\n\n    def get_metadata(self, source: Node, target: Node) -&gt; dict:\n        \"\"\"Optional: Add metadata to the created edge.\"\"\"\n        return {\"rule\": self.name}\n</code></pre>"},{"location":"tutorials/advanced/custom-stitching-rules/#advanced-context-aware-rules","title":"Advanced: Context-Aware Rules","text":"<p>Rules can access the full graph for context:</p> <pre><code>class TransitiveDependencyRule(StitchingRule):\n    \"\"\"Link nodes that share a common dependency.\"\"\"\n\n    def __init__(self, graph):\n        self.graph = graph\n\n    def compute_confidence(self, source: Node, target: Node) -&gt; float:\n        # Find common dependencies\n        source_deps = set(self.graph.get_dependencies(source.id))\n        target_deps = set(self.graph.get_dependencies(target.id))\n\n        common = source_deps &amp; target_deps\n\n        if not common:\n            return 0.0\n\n        # More shared dependencies = higher confidence\n        overlap = len(common) / min(len(source_deps), len(target_deps))\n        return min(0.9, overlap)\n</code></pre>"},{"location":"tutorials/advanced/custom-stitching-rules/#testing-rules","title":"Testing Rules","text":"<pre><code>import pytest\nfrom my_rules import ServiceToModuleRule\nfrom jnkn.core.types import Node, NodeType\n\nclass TestServiceToModuleRule:\n    @pytest.fixture\n    def rule(self):\n        return ServiceToModuleRule()\n\n    def test_exact_match(self, rule):\n        source = Node(\n            id=\"file://src/services/payment_service.py\",\n            name=\"payment_service.py\",\n            type=NodeType.CODE_FILE,\n            path=\"src/services/payment_service.py\",\n        )\n        target = Node(\n            id=\"infra:module.payment.aws_rds\",\n            name=\"aws_rds\",\n            type=NodeType.INFRA_RESOURCE,\n        )\n\n        assert rule.should_evaluate(source, target) is True\n        assert rule.compute_confidence(source, target) == 0.95\n\n    def test_no_match(self, rule):\n        source = Node(\n            id=\"file://src/utils.py\",\n            name=\"utils.py\",\n            type=NodeType.CODE_FILE,\n            path=\"src/utils.py\",\n        )\n        target = Node(\n            id=\"infra:module.payment.aws_rds\",\n            name=\"aws_rds\",\n            type=NodeType.INFRA_RESOURCE,\n        )\n\n        assert rule.should_evaluate(source, target) is False\n</code></pre>"},{"location":"tutorials/advanced/custom-stitching-rules/#built-in-rules","title":"Built-in Rules","text":"<p>Jnkn ships with these rules:</p> Rule Source \u2192 Target Description <code>EnvVarToInfraRule</code> env_var \u2192 infra Token matching <code>CodeToEnvRule</code> code_file \u2192 env_var Direct detection <code>K8sToSecretRule</code> k8s_workload \u2192 k8s_secret ConfigMap/Secret refs <code>DbtRefRule</code> dbt_model \u2192 dbt_model <code>ref()</code> calls"},{"location":"tutorials/advanced/custom-stitching-rules/#disabling-built-in-rules","title":"Disabling Built-in Rules","text":"<pre><code>stitching:\n  disabled_rules:\n    - EnvVarToInfraRule  # Too many false positives for us\n</code></pre>"},{"location":"tutorials/advanced/custom-stitching-rules/#next-steps","title":"Next Steps","text":"<ul> <li> Confidence model explanation</li> <li> Stitching engine architecture</li> </ul>"},{"location":"tutorials/advanced/enterprise-deployment/","title":"Enterprise Deployment","text":"<p>Air-gapped installation and compliance.</p>"},{"location":"tutorials/advanced/extending-jnkn/","title":"Extending Jnkn","text":"<p>Using the plugin architecture.</p>"},{"location":"tutorials/beginner/fixing-false-positives/","title":"Fixing False Positives","text":"<p>Learn to tune Jnkn when it makes incorrect matches.</p> <p>Time: 10 minutes</p>"},{"location":"tutorials/beginner/fixing-false-positives/#when-to-suppress","title":"When to Suppress","text":"<p>False positives happen when Jnkn links unrelated artifacts due to similar names:</p> <pre><code>env:HOST  \u2190\u2192  infra:ghost_writer    # Wrong! Just share \"host\" substring\nenv:ID    \u2190\u2192  infra:user_id         # Wrong! \"ID\" is too generic\n</code></pre>"},{"location":"tutorials/beginner/fixing-false-positives/#strategy-1-raise-confidence-threshold","title":"Strategy 1: Raise Confidence Threshold","text":"<p>Edit <code>.jnkn/config.yaml</code>:</p> <pre><code>stitching:\n  min_confidence: 0.6  # Default is 0.5\n</code></pre> <p>Higher threshold = fewer matches = fewer false positives (but may miss real links).</p>"},{"location":"tutorials/beginner/fixing-false-positives/#strategy-2-add-suppressions","title":"Strategy 2: Add Suppressions","text":"<p>Suppress specific patterns that you know are wrong.</p>"},{"location":"tutorials/beginner/fixing-false-positives/#suppress-a-single-match","title":"Suppress a Single Match","text":"<pre><code>jnkn suppress add \"env:HOST\" \"infra:ghost_writer\" \\\n  --reason \"Unrelated - ghost_writer is a logging service\"\n</code></pre>"},{"location":"tutorials/beginner/fixing-false-positives/#suppress-a-pattern","title":"Suppress a Pattern","text":"<p>Use globs to suppress categories:</p> <pre><code># All *_ID env vars are too generic\njnkn suppress add \"env:*_ID\" \"infra:*\" \\\n  --reason \"ID fields are too generic for matching\"\n\n# Ignore all matches to test infrastructure  \njnkn suppress add \"*\" \"infra:test_*\" \\\n  --reason \"Test infrastructure, not production\"\n</code></pre>"},{"location":"tutorials/beginner/fixing-false-positives/#list-suppressions","title":"List Suppressions","text":"<pre><code>jnkn suppress list\n</code></pre> <pre><code>ID  Source        Target         Reason                    Expires\n1   env:HOST      infra:ghost_*  Unrelated services        never\n2   env:*_ID      infra:*        ID fields too generic     never\n3   *             infra:test_*   Test infrastructure       2024-03-01\n</code></pre>"},{"location":"tutorials/beginner/fixing-false-positives/#remove-a-suppression","title":"Remove a Suppression","text":"<pre><code>jnkn suppress remove 1\n</code></pre>"},{"location":"tutorials/beginner/fixing-false-positives/#test-a-suppression","title":"Test a Suppression","text":"<p>Check if a match would be suppressed:</p> <pre><code>jnkn suppress test env:USER_ID infra:user_service\n</code></pre> <pre><code>\u2713 Would be SUPPRESSED by rule 2: \"env:*_ID\" \u2192 \"infra:*\"\n</code></pre>"},{"location":"tutorials/beginner/fixing-false-positives/#strategy-3-configure-token-filtering","title":"Strategy 3: Configure Token Filtering","text":"<p>Some tokens are too common to be meaningful. Edit <code>.jnkn/config.yaml</code>:</p> <pre><code>stitching:\n  blocked_tokens:\n    - id\n    - key\n    - url\n    - host\n    - port\n    - name\n\n  min_token_length: 3  # Ignore tokens shorter than this\n</code></pre>"},{"location":"tutorials/beginner/fixing-false-positives/#suppressions-file","title":"Suppressions File","text":"<p>Suppressions are stored in <code>.jnkn/suppressions.yaml</code>:</p> <pre><code>suppressions:\n  - source_pattern: \"env:*_ID\"\n    target_pattern: \"infra:*\"\n    reason: \"ID fields are too generic\"\n    enabled: true\n    expires: null\n    created_at: \"2024-01-15T10:30:00Z\"\n\n  - source_pattern: \"env:HOST\"\n    target_pattern: \"infra:ghost_*\"\n    reason: \"Unrelated services\"\n    enabled: true\n    expires: \"2024-06-01\"\n    created_at: \"2024-01-15T10:35:00Z\"\n</code></pre> <p>Commit suppressions to git</p> <p>Suppressions should be version-controlled so the whole team benefits.</p>"},{"location":"tutorials/beginner/fixing-false-positives/#verifying-your-fixes","title":"Verifying Your Fixes","text":"<p>After adding suppressions, re-scan:</p> <pre><code>jnkn scan --full\n</code></pre> <p>Check that false positives are gone:</p> <pre><code>jnkn blast env:HOST\n</code></pre>"},{"location":"tutorials/beginner/fixing-false-positives/#when-to-not-suppress","title":"When to NOT Suppress","text":"<p>Don't suppress just because a match is unexpected. Investigate first:</p> <pre><code>jnkn explain env:PAYMENT_DB_HOST infra:payment_database\n</code></pre> <p>If the confidence is high and the tokens genuinely overlap, the match might be correct \u2014 you may have discovered a real dependency you didn't know about.</p>"},{"location":"tutorials/beginner/fixing-false-positives/#next-steps","title":"Next Steps","text":"<ul> <li> Configure confidence in detail</li> <li> Understand how matching works</li> </ul>"},{"location":"tutorials/beginner/scanning-python-project/","title":"Scanning a Python Project","text":"<p>Learn to detect environment variables in a Python application.</p> <p>Time: 10 minutes</p>"},{"location":"tutorials/beginner/scanning-python-project/#prerequisites","title":"Prerequisites","text":"<ul> <li>Jnkn installed (<code>pip install jnkn[full]</code>)</li> <li>A Python project (or use our example)</li> </ul>"},{"location":"tutorials/beginner/scanning-python-project/#setup","title":"Setup","text":"<p>If you don't have a project, create a sample:</p> <pre><code>mkdir jnkn-tutorial &amp;&amp; cd jnkn-tutorial\n</code></pre> <p>Create <code>app/config.py</code>:</p> <pre><code>import os\nfrom pydantic_settings import BaseSettings\n\nclass Settings(BaseSettings):\n    database_url: str\n    api_key: str\n    debug: bool = False\n\n    class Config:\n        env_prefix = \"APP_\"\n\nsettings = Settings()\n\n# Also using direct env access\nREDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost:6379\")\nLOG_LEVEL = os.environ.get(\"LOG_LEVEL\", \"INFO\")\n</code></pre> <p>Create <code>app/main.py</code>:</p> <pre><code>from config import settings, REDIS_URL\nimport os\n\ndef get_db_connection():\n    return connect(os.getenv(\"DATABASE_URL\"))  # Different from settings!\n\ndef get_cache():\n    return Redis.from_url(REDIS_URL)\n</code></pre>"},{"location":"tutorials/beginner/scanning-python-project/#scan","title":"Scan","text":"<pre><code>jnkn init\njnkn scan\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udd0d Scanning /path/to/jnkn-tutorial\n\ud83d\udcc1 Found 2 Python files\n\u2705 Parsed 8 nodes, 6 edges\n\nDetected Environment Variables:\n  env:APP_DATABASE_URL     (pydantic_settings)\n  env:APP_API_KEY          (pydantic_settings)  \n  env:APP_DEBUG            (pydantic_settings)\n  env:REDIS_URL            (os.getenv)\n  env:LOG_LEVEL            (os.environ.get)\n  env:DATABASE_URL         (os.getenv)\n</code></pre>"},{"location":"tutorials/beginner/scanning-python-project/#understanding-the-output","title":"Understanding the Output","text":"<p>Jnkn detected 6 environment variables from multiple patterns:</p> Variable Source Pattern <code>APP_DATABASE_URL</code> Pydantic Settings <code>env_prefix + field_name</code> <code>APP_API_KEY</code> Pydantic Settings <code>env_prefix + field_name</code> <code>APP_DEBUG</code> Pydantic Settings <code>env_prefix + field_name</code> <code>REDIS_URL</code> Direct access <code>os.getenv()</code> <code>LOG_LEVEL</code> Direct access <code>os.environ.get()</code> <code>DATABASE_URL</code> Direct access <code>os.getenv()</code> <p>Potential Bug Detected</p> <p>Notice <code>DATABASE_URL</code> and <code>APP_DATABASE_URL</code> \u2014 these might be the same intent but different names. Jnkn helps you spot these inconsistencies.</p>"},{"location":"tutorials/beginner/scanning-python-project/#view-the-graph","title":"View the Graph","text":"<pre><code>jnkn stats\n</code></pre> <pre><code>\ud83d\udcca Graph Statistics\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nNodes by Type:\n  code_file:    2\n  env_var:      6\n\nEdges by Type:\n  reads:        6\n</code></pre>"},{"location":"tutorials/beginner/scanning-python-project/#explore-dependencies","title":"Explore Dependencies","text":"<p>Which files read <code>REDIS_URL</code>?</p> <pre><code>jnkn blast env:REDIS_URL\n</code></pre> <pre><code>{\n  \"source\": \"env:REDIS_URL\",\n  \"impacted\": [\"file://app/config.py\"],\n  \"total_impacted\": 1\n}\n</code></pre>"},{"location":"tutorials/beginner/scanning-python-project/#supported-python-patterns","title":"Supported Python Patterns","text":"<p>Jnkn detects these patterns automatically:</p> Pattern Example <code>os.getenv()</code> <code>os.getenv(\"VAR\")</code> <code>os.environ.get()</code> <code>os.environ.get(\"VAR\")</code> <code>os.environ[]</code> <code>os.environ[\"VAR\"]</code> Pydantic Settings <code>class Config(BaseSettings)</code> Pydantic Field <code>Field(env=\"VAR\")</code> Click/Typer <code>@click.option(envvar=\"VAR\")</code> django-environ <code>env.str(\"VAR\")</code> <p>See Python Patterns Reference for the full list.</p>"},{"location":"tutorials/beginner/scanning-python-project/#next-steps","title":"Next Steps","text":"<ul> <li> Understanding Blast Radius</li> <li> Add Terraform to your scan</li> </ul>"},{"location":"tutorials/beginner/scanning-terraform/","title":"Scanning Terraform","text":"<p>Learn how to map infrastructure resources.</p>"},{"location":"tutorials/beginner/understanding-blast-radius/","title":"Understanding Blast Radius","text":"<p>Learn to read Jnkn's impact analysis output.</p> <p>Time: 15 minutes</p>"},{"location":"tutorials/beginner/understanding-blast-radius/#what-is-blast-radius","title":"What is Blast Radius?","text":"<p>Blast radius answers: \"If I change X, what else might break?\"</p> <pre><code>graph TD\n    ENV[env:DATABASE_URL] --&gt; F1[connection.py]\n    ENV --&gt; F2[users.py]\n    ENV --&gt; TF[aws_db_instance.main]\n    F1 --&gt; F3[api.py]\n\n    style ENV fill:#ff6b6b,stroke:#c92a2a\n    style F1 fill:#ffd43b,stroke:#fab005\n    style F2 fill:#ffd43b,stroke:#fab005\n    style TF fill:#ffd43b,stroke:#fab005\n    style F3 fill:#ffe066,stroke:#fab005</code></pre> <p>Changing <code>DATABASE_URL</code> directly impacts 3 artifacts, and transitively impacts 1 more.</p>"},{"location":"tutorials/beginner/understanding-blast-radius/#running-blast-radius","title":"Running Blast Radius","text":"<pre><code>jnkn blast env:DATABASE_URL\n</code></pre>"},{"location":"tutorials/beginner/understanding-blast-radius/#json-output-default","title":"JSON Output (Default)","text":"<pre><code>{\n  \"source_artifacts\": [\"env:DATABASE_URL\"],\n  \"total_impacted_count\": 4,\n  \"impacted_artifacts\": [\n    \"file://src/db/connection.py\",\n    \"file://src/api/users.py\",\n    \"infra:aws_db_instance.main\",\n    \"file://src/api/api.py\"\n  ],\n  \"breakdown\": {\n    \"code\": [\"file://src/db/connection.py\", \"file://src/api/users.py\", \"file://src/api/api.py\"],\n    \"infra\": [\"infra:aws_db_instance.main\"],\n    \"env\": [],\n    \"data\": []\n  },\n  \"max_depth_reached\": 2\n}\n</code></pre>"},{"location":"tutorials/beginner/understanding-blast-radius/#understanding-the-fields","title":"Understanding the Fields","text":"Field Meaning <code>source_artifacts</code> What you're analyzing <code>total_impacted_count</code> Total downstream artifacts <code>impacted_artifacts</code> List of affected artifacts <code>breakdown</code> Grouped by type <code>max_depth_reached</code> How far the impact travels"},{"location":"tutorials/beginner/understanding-blast-radius/#reading-artifact-ids","title":"Reading Artifact IDs","text":"<p>Jnkn uses prefixed IDs to identify artifact types:</p> Prefix Type Example <code>env:</code> Environment variable <code>env:DATABASE_URL</code> <code>file://</code> Code file <code>file://src/app.py</code> <code>infra:</code> Infrastructure resource <code>infra:aws_rds.main</code> <code>data:</code> Data asset (dbt model) <code>data:fct_orders</code> <code>k8s:</code> Kubernetes resource <code>k8s:default/deployment/api</code>"},{"location":"tutorials/beginner/understanding-blast-radius/#limiting-depth","title":"Limiting Depth","text":"<p>For large graphs, limit traversal depth:</p> <pre><code>jnkn blast env:DATABASE_URL --max-depth 1\n</code></pre> <p>This shows only direct dependencies, not transitive ones.</p>"},{"location":"tutorials/beginner/understanding-blast-radius/#multiple-sources","title":"Multiple Sources","text":"<p>Analyze multiple artifacts at once:</p> <pre><code>jnkn blast env:DATABASE_URL env:REDIS_URL\n</code></pre> <p>The result shows the union of all impacted artifacts.</p>"},{"location":"tutorials/beginner/understanding-blast-radius/#output-formats","title":"Output Formats","text":"JSONMarkdownPlain <pre><code>jnkn blast env:X --format json\n</code></pre> <pre><code>jnkn blast env:X --format markdown\n</code></pre> <pre><code>## Blast Radius: env:X\n\n**4 artifacts impacted**\n\n| Type | Artifact |\n|------|----------|\n| code | file://src/db/connection.py |\n| code | file://src/api/users.py |\n| infra | infra:aws_db_instance.main |\n</code></pre> <pre><code>jnkn blast env:X --format plain\n</code></pre> <pre><code>file://src/db/connection.py\nfile://src/api/users.py\ninfra:aws_db_instance.main\n</code></pre>"},{"location":"tutorials/beginner/understanding-blast-radius/#interpreting-results","title":"Interpreting Results","text":""},{"location":"tutorials/beginner/understanding-blast-radius/#low-impact-1-3-artifacts","title":"Low Impact (1-3 artifacts)","text":"<p>Safe to proceed. Normal for leaf nodes or well-encapsulated components.</p>"},{"location":"tutorials/beginner/understanding-blast-radius/#medium-impact-4-10-artifacts","title":"Medium Impact (4-10 artifacts)","text":"<p>Review carefully. Consider notifying affected teams.</p>"},{"location":"tutorials/beginner/understanding-blast-radius/#high-impact-10-artifacts","title":"High Impact (10+ artifacts)","text":"<p>Proceed with caution. This change affects many components:</p> <ul> <li>Schedule deployment during low-traffic periods</li> <li>Have rollback plan ready</li> <li>Consider feature flags</li> </ul>"},{"location":"tutorials/beginner/understanding-blast-radius/#why-is-something-in-the-blast-radius","title":"Why Is Something in the Blast Radius?","text":"<p>Use <code>explain</code> to understand why two artifacts are linked:</p> <pre><code>jnkn explain env:DATABASE_URL infra:aws_db_instance.main\n</code></pre> <pre><code>Confidence: 0.87 (HIGH)\n\nSignals:\n  [+0.85] token_overlap: 3/3 tokens match [database, url, main]\n  [+0.10] suffix_match: both end in common pattern\n\nPenalties:\n  [-0.08] short_token: 'url' is only 3 chars\n</code></pre>"},{"location":"tutorials/beginner/understanding-blast-radius/#next-steps","title":"Next Steps","text":"<ul> <li> Fix false positives</li> <li> Configure confidence</li> </ul>"},{"location":"tutorials/intermediate/ci-cd-integration/","title":"CI/CD Integration","text":"<p>Set up a complete CI pipeline with impact analysis, blocking, and notifications.</p> <p>Time: 25 minutes</p>"},{"location":"tutorials/intermediate/ci-cd-integration/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    PR[Pull Request] --&gt; Scan[jnkn scan]\n    Scan --&gt; Diff[jnkn diff]\n    Diff --&gt; Check{Impact &gt; threshold?}\n    Check --&gt;|No| Pass[\u2705 Pass]\n    Check --&gt;|Yes| Block[\u274c Block + Comment]</code></pre>"},{"location":"tutorials/intermediate/ci-cd-integration/#github-actions-full-setup","title":"GitHub Actions: Full Setup","text":"<p>Create <code>.github/workflows/jnkn.yml</code>:</p> <pre><code>name: Jnkn Impact Analysis\n\non:\n  pull_request:\n    paths:\n      - '**.py'\n      - '**.tf'\n      - '**.yaml'\n      - '**.yml'\n\npermissions:\n  contents: read\n  pull-requests: write\n\nenv:\n  JUNKAN_MIN_CONFIDENCE: 0.5\n  IMPACT_THRESHOLD: 10\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout PR\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Checkout base branch\n        run: git fetch origin ${{ github.base_ref }}\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n          cache: 'pip'\n\n      - name: Install Jnkn\n        run: pip install jnkn[full]\n\n      - name: Scan base branch\n        run: |\n          git checkout origin/${{ github.base_ref }}\n          jnkn scan --db .jnkn/base.db\n\n      - name: Scan PR branch\n        run: |\n          git checkout ${{ github.sha }}\n          jnkn scan --db .jnkn/pr.db\n\n      - name: Analyze impact\n        id: impact\n        run: |\n          # Get changed files\n          CHANGED=$(git diff --name-only origin/${{ github.base_ref }}...HEAD)\n\n          # Calculate total impact\n          TOTAL_IMPACT=0\n          REPORT=\"\"\n\n          for file in $CHANGED; do\n            if [[ \"$file\" =~ \\.(py|tf|yaml|yml)$ ]]; then\n              IMPACT=$(jnkn blast \"file://$file\" --db .jnkn/pr.db --format json 2&gt;/dev/null | jq -r '.total_impacted_count // 0')\n              TOTAL_IMPACT=$((TOTAL_IMPACT + IMPACT))\n\n              if [ \"$IMPACT\" -gt 0 ]; then\n                REPORT=\"$REPORT\\n### \\`$file\\` \u2014 $IMPACT artifacts\\n\"\n                REPORT=\"$REPORT$(jnkn blast \"file://$file\" --db .jnkn/pr.db --format markdown 2&gt;/dev/null || echo '')\\n\"\n              fi\n            fi\n          done\n\n          echo \"total=$TOTAL_IMPACT\" &gt;&gt; $GITHUB_OUTPUT\n          echo \"report&lt;&lt;EOF\" &gt;&gt; $GITHUB_OUTPUT\n          echo -e \"$REPORT\" &gt;&gt; $GITHUB_OUTPUT\n          echo \"EOF\" &gt;&gt; $GITHUB_OUTPUT\n\n      - name: Comment on PR\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const impact = ${{ steps.impact.outputs.total }};\n            const threshold = ${{ env.IMPACT_THRESHOLD }};\n            const report = `${{ steps.impact.outputs.report }}`;\n\n            let status = impact &gt; threshold ? '\ud83d\udd34' : '\ud83d\udfe2';\n            let message = `## ${status} Jnkn Impact Analysis\\n\\n`;\n            message += `**Total Impact:** ${impact} artifacts\\n\\n`;\n\n            if (impact &gt; threshold) {\n              message += `\u26a0\ufe0f **High impact change!** Threshold is ${threshold}.\\n\\n`;\n            }\n\n            if (report.trim()) {\n              message += `### Details\\n${report}`;\n            } else {\n              message += `No cross-domain dependencies detected.`;\n            }\n\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: message\n            });\n\n      - name: Enforce threshold\n        if: steps.impact.outputs.total &gt; env.IMPACT_THRESHOLD\n        run: |\n          echo \"::error::Impact (${{ steps.impact.outputs.total }}) exceeds threshold (${{ env.IMPACT_THRESHOLD }})\"\n          exit 1\n</code></pre>"},{"location":"tutorials/intermediate/ci-cd-integration/#what-this-does","title":"What This Does","text":"<ol> <li>Scans both branches \u2014 Compares base and PR states</li> <li>Calculates impact \u2014 For each changed file</li> <li>Posts a comment \u2014 With detailed breakdown</li> <li>Blocks if too risky \u2014 Fails the check if impact exceeds threshold</li> </ol>"},{"location":"tutorials/intermediate/ci-cd-integration/#example-pr-comment","title":"Example PR Comment","text":"<pre><code>## \ud83d\udd34 Jnkn Impact Analysis\n\n**Total Impact:** 15 artifacts\n\n\u26a0\ufe0f **High impact change!** Threshold is 10.\n\n### Details\n\n### `terraform/rds.tf` \u2014 8 artifacts\n\n| Type | Artifact | Confidence |\n|------|----------|------------|\n| env_var | env:DATABASE_URL | 0.92 |\n| code | src/db/connection.py | 0.88 |\n| code | src/api/users.py | 0.85 |\n| k8s | deployment/api | 0.82 |\n...\n\n### `src/config.py` \u2014 7 artifacts\n...\n</code></pre>"},{"location":"tutorials/intermediate/ci-cd-integration/#customization-options","title":"Customization Options","text":""},{"location":"tutorials/intermediate/ci-cd-integration/#adjust-threshold-by-path","title":"Adjust Threshold by Path","text":"<pre><code>- name: Set threshold\n  run: |\n    if [[ \"${{ github.event.pull_request.base.ref }}\" == \"main\" ]]; then\n      echo \"IMPACT_THRESHOLD=5\" &gt;&gt; $GITHUB_ENV  # Stricter for main\n    else\n      echo \"IMPACT_THRESHOLD=20\" &gt;&gt; $GITHUB_ENV\n    fi\n</code></pre>"},{"location":"tutorials/intermediate/ci-cd-integration/#skip-analysis-for-certain-prs","title":"Skip Analysis for Certain PRs","text":"<pre><code>- name: Check skip label\n  if: contains(github.event.pull_request.labels.*.name, 'skip-jnkn')\n  run: echo \"Skipping Jnkn analysis\"\n</code></pre>"},{"location":"tutorials/intermediate/ci-cd-integration/#notify-slack-on-high-impact","title":"Notify Slack on High Impact","text":"<pre><code>- name: Notify Slack\n  if: steps.impact.outputs.total &gt; 15\n  uses: slackapi/slack-github-action@v1\n  with:\n    payload: |\n      {\n        \"text\": \"\ud83d\udea8 High-impact PR: ${{ github.event.pull_request.html_url }}\\nImpact: ${{ steps.impact.outputs.total }} artifacts\"\n      }\n  env:\n    SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}\n</code></pre>"},{"location":"tutorials/intermediate/ci-cd-integration/#caching-for-speed","title":"Caching for Speed","text":"<pre><code>- name: Cache Jnkn database\n  uses: actions/cache@v4\n  with:\n    path: .jnkn/\n    key: jnkn-${{ hashFiles('**/*.py', '**/*.tf') }}\n    restore-keys: jnkn-\n</code></pre>"},{"location":"tutorials/intermediate/ci-cd-integration/#next-steps","title":"Next Steps","text":"<ul> <li> GitLab CI setup</li> <li> Configure thresholds</li> </ul>"},{"location":"tutorials/intermediate/custom-confidence-tuning/","title":"Confidence Tuning","text":"<p>Adjusting matching thresholds for your organization.</p>"},{"location":"tutorials/intermediate/multi-stack-analysis/","title":"Multi-Stack Analysis","text":"<p>Analyze dependencies across Python, Terraform, and Kubernetes together.</p> <p>Time: 20 minutes</p>"},{"location":"tutorials/intermediate/multi-stack-analysis/#the-setup","title":"The Setup","text":"<p>A typical production stack:</p> <pre><code>my-service/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 app/\n\u2502       \u251c\u2500\u2500 config.py      # Python settings\n\u2502       \u2514\u2500\u2500 main.py        # Application code\n\u251c\u2500\u2500 terraform/\n\u2502   \u251c\u2500\u2500 main.tf           # Infrastructure\n\u2502   \u2514\u2500\u2500 variables.tf\n\u2514\u2500\u2500 k8s/\n    \u2514\u2500\u2500 deployment.yaml   # Kubernetes manifests\n</code></pre>"},{"location":"tutorials/intermediate/multi-stack-analysis/#python-configuration","title":"Python Configuration","text":"<p><code>src/app/config.py</code>:</p> <pre><code>import os\n\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nREDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\nAPI_SECRET = os.environ[\"API_SECRET\"]\n</code></pre>"},{"location":"tutorials/intermediate/multi-stack-analysis/#terraform-infrastructure","title":"Terraform Infrastructure","text":"<p><code>terraform/main.tf</code>:</p> <pre><code>resource \"aws_db_instance\" \"main\" {\n  identifier = \"my-service-db\"\n  engine     = \"postgres\"\n}\n\nresource \"aws_elasticache_cluster\" \"redis\" {\n  cluster_id = \"my-service-redis\"\n}\n\noutput \"database_url\" {\n  value = \"postgresql://${aws_db_instance.main.endpoint}/mydb\"\n}\n\noutput \"redis_host\" {\n  value = aws_elasticache_cluster.redis.cache_nodes[0].address\n}\n</code></pre>"},{"location":"tutorials/intermediate/multi-stack-analysis/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p><code>k8s/deployment.yaml</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-service\nspec:\n  template:\n    spec:\n      containers:\n        - name: app\n          env:\n            - name: DATABASE_URL\n              valueFrom:\n                secretKeyRef:\n                  name: my-service-secrets\n                  key: database-url\n            - name: REDIS_HOST\n              valueFrom:\n                configMapKeyRef:\n                  name: my-service-config\n                  key: redis-host\n            - name: API_SECRET\n              valueFrom:\n                secretKeyRef:\n                  name: my-service-secrets\n                  key: api-secret\n</code></pre>"},{"location":"tutorials/intermediate/multi-stack-analysis/#scan-everything","title":"Scan Everything","text":"<pre><code>jnkn scan\n</code></pre> <p>Output:</p> <pre><code>\ud83d\udd0d Scanning /path/to/my-service\n\ud83d\udcc1 Found 5 files (2 Python, 2 Terraform, 1 Kubernetes)\n\u2705 Parsed 15 nodes, 12 edges\n\ud83e\uddf5 Stitching cross-domain dependencies...\n\u2705 Created 6 cross-domain links\n</code></pre>"},{"location":"tutorials/intermediate/multi-stack-analysis/#the-unified-graph","title":"The Unified Graph","text":"<p>Jnkn builds a single graph across all stacks:</p> <pre><code>graph TD\n    subgraph Python\n        PY1[config.py]\n        ENV1[env:DATABASE_URL]\n        ENV2[env:REDIS_HOST]\n        ENV3[env:API_SECRET]\n    end\n\n    subgraph Terraform\n        TF1[aws_db_instance.main]\n        TF2[aws_elasticache.redis]\n        OUT1[output:database_url]\n        OUT2[output:redis_host]\n    end\n\n    subgraph Kubernetes\n        K8S[deployment/my-service]\n        SEC[secret/my-service-secrets]\n        CM[configmap/my-service-config]\n    end\n\n    PY1 --&gt; ENV1\n    PY1 --&gt; ENV2\n    PY1 --&gt; ENV3\n\n    TF1 -.-&gt;|stitched| ENV1\n    TF2 -.-&gt;|stitched| ENV2\n\n    K8S --&gt; SEC\n    K8S --&gt; CM\n    SEC -.-&gt;|stitched| ENV1\n    SEC -.-&gt;|stitched| ENV3\n    CM -.-&gt;|stitched| ENV2\n\n    style ENV1 fill:#ff6b6b\n    style ENV2 fill:#ff6b6b\n    style ENV3 fill:#ff6b6b</code></pre>"},{"location":"tutorials/intermediate/multi-stack-analysis/#cross-domain-queries","title":"Cross-Domain Queries","text":""},{"location":"tutorials/intermediate/multi-stack-analysis/#what-breaks-if-we-change-the-database","title":"What breaks if we change the database?","text":"<pre><code>jnkn blast infra:aws_db_instance.main\n</code></pre> <pre><code>{\n  \"source\": \"infra:aws_db_instance.main\",\n  \"impacted\": [\n    \"env:DATABASE_URL\",\n    \"file://src/app/config.py\",\n    \"k8s:default/secret/my-service-secrets\"\n  ]\n}\n</code></pre>"},{"location":"tutorials/intermediate/multi-stack-analysis/#what-provides-redis_host","title":"What provides REDIS_HOST?","text":"<pre><code>jnkn explain env:REDIS_HOST infra:aws_elasticache.redis\n</code></pre> <pre><code>Confidence: 0.82 (HIGH)\n  [+0.85] token_overlap: [redis, host] match\n  [+0.10] terraform_output: output.redis_host references this resource\n</code></pre>"},{"location":"tutorials/intermediate/multi-stack-analysis/#what-happens-if-we-delete-the-configmap","title":"What happens if we delete the ConfigMap?","text":"<pre><code>jnkn blast k8s:default/configmap/my-service-config\n</code></pre> <pre><code>{\n  \"source\": \"k8s:default/configmap/my-service-config\",\n  \"impacted\": [\n    \"k8s:default/deployment/my-service\",\n    \"env:REDIS_HOST\",\n    \"file://src/app/config.py\"\n  ]\n}\n</code></pre>"},{"location":"tutorials/intermediate/multi-stack-analysis/#real-world-scenario","title":"Real-World Scenario","text":"<p>PR: \"Rename <code>redis_host</code> output to <code>cache_endpoint</code>\"</p> <pre><code># What's the impact?\njnkn blast infra:output.redis_host\n</code></pre> <pre><code>\u26a0\ufe0f  4 artifacts affected:\n\n  \u2022 env:REDIS_HOST (config.py will fail to get value)\n  \u2022 k8s:default/configmap/my-service-config (references old name)\n  \u2022 k8s:default/deployment/my-service (pod won't start)\n  \u2022 file://src/app/config.py (REDIS_HOST will be None)\n</code></pre> <p>Action: Update ConfigMap and Python code before merging the Terraform change.</p>"},{"location":"tutorials/intermediate/multi-stack-analysis/#next-steps","title":"Next Steps","text":"<ul> <li> Set up CI to catch these automatically</li> <li> Add dbt to your analysis</li> </ul>"},{"location":"tutorials/intermediate/team-workflows/","title":"Team Workflows","text":"<p>Sharing config and suppressions across teams.</p>"}]}