"""
Check Command - CI/CD Gate for Pre-Merge Impact Analysis.

This module implements the logic to verify changes against the dependency graph.
It acts as the primary gatekeeper in CI/CD pipelines.
"""

import logging
import re
import subprocess
import sys
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, List, Optional, Set, Tuple

import click
from pydantic import BaseModel, Field
from rich.console import Console

from ...analysis.blast_radius import BlastRadiusAnalyzer
from ...core.enhanced_stitching import EnhancedStitcher
from ...core.manifest import ProjectManifest
from ...core.mappings import MappingMatcher
from ...core.resolver import DependencyResolver
from ...core.storage.sqlite import SQLiteStorage
from ...core.types import NodeType
from ...parsing.engine import ScanConfig, create_default_engine
from ..renderers import JsonRenderer
from ..utils import SKIP_DIRS, load_graph

logger = logging.getLogger(__name__)
console = Console(stderr=True)  # Use stderr for logs to not pollute JSON output pipes


# --- API Models (External Contract) ---
class CheckResultStatus(str, Enum):
    PASS = "PASS"
    BLOCKED = "BLOCKED"
    WARN = "WARN"


class ApiViolation(BaseModel):
    rule: str
    severity: str
    message: str


class CheckResponse(BaseModel):
    """
    Standardized response for check command.
    """

    result: CheckResultStatus
    exit_code: int
    changed_files_count: int
    critical_count: int
    high_count: int
    violations: List[ApiViolation] = Field(default_factory=list)


# --- Internal Domain Models ---
class CheckResult(Enum):
    PASS = 0
    BLOCKED = 1
    WARN = 2


@dataclass
class ChangedFile:
    path: str
    change_type: str  # 'A', 'M', 'D', etc.
    old_path: str | None = None


@dataclass
class CheckReport:
    """Internal report generated by the analysis engine."""

    result: CheckResult = CheckResult.PASS
    changed_files: List[ChangedFile] = field(default_factory=list)
    critical_count: int = 0
    high_count: int = 0
    violations: List[Any] = field(default_factory=list)


class CheckEngine:
    """
    Performs risk assessment on changed files using the dependency graph.
    """

    def __init__(
        self, 
        graph_path: str = ".jnkn/jnkn.db",
        ignore_dirs: Optional[Set[str]] = None,
    ):
        self.graph_path = graph_path
        self.graph = load_graph(graph_path)
        self.ignore_dirs = ignore_dirs or set()
        self.manifest: Optional[ProjectManifest] = None
        self._mapping_matcher: Optional[MappingMatcher] = None

    def ensure_graph_exists(self, force_scan: bool = False):
        """
        Auto-scan capability: If graph is missing, build it on the fly.
        This enables the 'init -> check' workflow without an explicit 'scan' step.
        """
        if not force_scan and self.graph is not None and self.graph.node_count > 0:
            # Still load manifest for ignore checking
            root_dir = Path.cwd()
            self.manifest = ProjectManifest.load(root_dir / "jnkn.toml")
            if self.manifest.mappings:
                self._mapping_matcher = MappingMatcher(self.manifest.mappings)
            return

        console.print("[dim]Graph not found or empty. Running auto-scan...[/dim]")

        # Setup paths
        db_path = Path(self.graph_path)
        root_dir = Path.cwd()

        # Load manifest
        self.manifest = ProjectManifest.load(root_dir / "jnkn.toml")
        if self.manifest.mappings:
            self._mapping_matcher = MappingMatcher(self.manifest.mappings)

        # Initialize components
        engine = create_default_engine()
        storage = SQLiteStorage(db_path)
        storage.clear()

        # Build skip_dirs set - combine defaults with user-specified
        skip_dirs = SKIP_DIRS.copy()
        skip_dirs.update(self.ignore_dirs)

        # 1. Resolve Dependencies (Phase 1)
        scan_targets = [(root_dir, self.manifest.name)]

        try:
            resolver = DependencyResolver(root_dir)
            resolution = resolver.resolve()
            for dep in resolution.dependencies:
                scan_targets.append((dep.path, dep.name))
        except Exception as e:
            console.print(f"[yellow]Warning: Dependency resolution failed: {e}[/yellow]")

        # 2. Scan all targets
        for target_path, repo_name in scan_targets:
            if not target_path.exists():
                continue

            config = ScanConfig(
                root_dir=target_path,
                skip_dirs=skip_dirs,
                incremental=False,
                source_repo_name=repo_name,
            )
            result = engine.scan_and_store(storage, config)

            if result.is_err():
                console.print(
                    f"[red]Scan failed for {repo_name}:[/red] {result.unwrap_err().message}"
                )

        # 3. Stitch - use EnhancedStitcher if we have mappings
        graph = storage.load_graph()
        
        if self.manifest.mappings:
            stitcher = EnhancedStitcher(
                mappings=self.manifest.mappings,
                min_confidence=0.3,
            )
            stitch_result = stitcher.stitch(graph)
            storage.save_edges_batch(stitch_result.edges)
        else:
            from ...core.stitching import Stitcher
            stitcher = Stitcher()
            new_edges = stitcher.stitch(graph)
            storage.save_edges_batch(new_edges)

        # 4. Reload
        self.graph = storage.load_graph()
        storage.close()
        console.print(f"[dim]Auto-scan complete. Found {self.graph.node_count} nodes.[/dim]")

    def analyze(self, changes: List[ChangedFile]) -> CheckReport:
        # Ensure we have data to analyze
        self.ensure_graph_exists()

        report = CheckReport(changed_files=changes)

        if not self.graph:
            self._analyze_static(changes, report)
            return report

        analyzer = BlastRadiusAnalyzer(self.graph)

        # 1. Check for global issues (like missing providers)
        # Get env vars without providers directly from graph
        env_nodes = self.graph.get_nodes_by_type(NodeType.ENV_VAR)
        
        for node in env_nodes:
            # Skip malformed env var names (parser false positives)
            if not self._is_valid_env_var_name(node.name):
                continue
            
            # Check if this env var is ignored via mappings
            if self._mapping_matcher and self._mapping_matcher.is_ignored(node.id):
                continue
            
            # Check if this node has any incoming 'provides' edges
            in_edges = self.graph.get_in_edges(node.id)
            has_provider = any(e.type.value == "provides" for e in in_edges)
            
            if not has_provider:
                # Skip if in ignored directory
                node_path = getattr(node, 'path', '') or ''
                if self._should_ignore_path(node_path):
                    continue
                    
                report.violations.append(
                    ApiViolation(
                        rule="MISSING_PROVIDER",
                        severity="critical",
                        message=f"{node.name} has no infrastructure provider",
                    )
                )
                report.critical_count += 1

        # 2. If specific files changed, analyze their blast radius
        for file in changes:
            # Skip test files and ignored directories
            if self._should_ignore_path(file.path):
                continue

            self._analyze_file_changes(file, analyzer, report)

        return report

    def _is_valid_env_var_name(self, name: str) -> bool:
        """
        Check if an env var name is valid (filter out parser false positives).
        
        Valid env var names:
        - Are at least 3 characters
        - Contain only alphanumeric chars and underscores
        - Don't start with a number
        - Are uppercase (convention)
        """
        if not name:
            return False
        
        # Too short - likely false positive
        if len(name) < 3:
            return False
        
        # Contains invalid characters (backslash, $, {, }, etc.)
        if any(c in name for c in r'\${}[]()'):
            return False
        
        # Template syntax
        if name.startswith('$') or name.startswith('{'):
            return False
        
        # Too generic single words (common false positives)
        generic_names = {'VAR', 'name', 'value', 'key', 'data', 'config', 'options'}
        if name in generic_names:
            return False
        
        # Should be uppercase with underscores (standard convention)
        # Allow mixed case but flag pure lowercase as suspicious
        if name.islower() and '_' not in name:
            return False
        
        return True

    def _should_ignore_path(self, path: str) -> bool:
        """Check if a path should be ignored based on ignore_dirs."""
        if not path:
            return False
            
        path_lower = path.lower()
        
        # Split path into parts for flexible matching
        path_parts = set(path_lower.replace("\\", "/").split("/"))
        
        # Check against ignore_dirs - match if ANY part of the path matches
        for ignore_dir in self.ignore_dirs:
            ignore_lower = ignore_dir.lower().strip("/")
            # Match as a path component (not substring)
            if ignore_lower in path_parts:
                return True
        
        # Also ignore common test patterns
        test_dir_names = {"tests", "test", "fixtures", "corpus", "__tests__", "e2e", "e2e_live"}
        if path_parts & test_dir_names:  # Set intersection
            return True
            
        return False

    def _analyze_file_changes(
        self, file: ChangedFile, analyzer: BlastRadiusAnalyzer, report: CheckReport
    ):
        """Analyze blast radius for a specific file change."""
        # Find nodes in this file
        file_path = file.path
        
        # Check if it's an infrastructure file
        if file_path.endswith((".tf", ".tfvars")):
            # Terraform change - check what depends on outputs from this file
            affected = analyzer.analyze_file_impact(file_path)
            if affected:
                for node_id in affected[:5]:  # Limit to top 5
                    report.violations.append(
                        ApiViolation(
                            rule="INFRA_IMPACT",
                            severity="high",
                            message=f"Change to {file_path} affects {node_id}",
                        )
                    )
                    report.high_count += 1

    def _analyze_static(self, changes: List[ChangedFile], report: CheckReport):
        """Fallback analysis when no graph is available."""
        for file in changes:
            if self._should_ignore_path(file.path):
                continue
            self._analyze_single_file_heuristic(file, report)

    def _analyze_single_file_heuristic(self, file: ChangedFile, report: CheckReport):
        """Heuristic checks based purely on filenames."""
        path = file.path.lower()

        if path.endswith((".tf", ".tfvars", ".hcl")):
            report.violations.append(
                ApiViolation(
                    rule="INFRA_CHANGE",
                    severity="critical",
                    message=f"Infrastructure change detected: {file.path}",
                )
            )
            report.critical_count += 1

        elif "migrations" in path or "schema" in path or path.endswith(".sql"):
            report.violations.append(
                ApiViolation(
                    rule="SCHEMA_CHANGE",
                    severity="high",
                    message=f"Data schema change detected: {file.path}",
                )
            )
            report.high_count += 1


def get_changed_files_from_git(base_ref: str, head_ref: str) -> List[ChangedFile]:
    """Get changed files between two git refs using git diff."""
    try:
        result = subprocess.run(
            ["git", "diff", "--name-status", base_ref, head_ref],
            capture_output=True,
            text=True,
            check=True,
        )
    except subprocess.CalledProcessError:
        return []

    changes = []
    for line in result.stdout.splitlines():
        if not line.strip():
            continue
        parts = line.split("\t")
        status = parts[0][0]
        if status == "R":
            if len(parts) >= 3:
                changes.append(ChangedFile(path=parts[2], change_type="RENAME", old_path=parts[1]))
        else:
            changes.append(ChangedFile(path=parts[1], change_type=status))
    return changes


def get_changed_files_from_diff_file(diff_path: str) -> List[ChangedFile]:
    """Parse a standard Unified Diff file."""
    changes = []
    path = Path(diff_path)
    content = path.read_text()
    diff_pattern = re.compile(r"^diff --git a/(.*?) b/(.*?)$", re.MULTILINE)
    for match in diff_pattern.finditer(content):
        changes.append(ChangedFile(path=match.group(2), change_type="MODIFIED"))
    return changes


class _null_context:
    def __enter__(self):
        pass

    def __exit__(self, *args):
        pass


@click.command()
@click.option(
    "--diff", "diff_file", type=click.Path(exists=True), help="Path to a unified diff file"
)
@click.option("--git-diff", "git_diff", nargs=2, help="Git refs to compare (BASE HEAD)")
@click.option(
    "--ignore-critical",
    is_flag=True,
    help="Do NOT exit with error even if critical violations found",
)
@click.option(
    "--ignore-dirs",
    "ignore_dirs_str",
    default="",
    help="Comma-separated list of directories to ignore (e.g., 'tests,fixtures,examples')",
)
@click.option("--json", "as_json", is_flag=True, help="Output as JSON (Standard Envelope)")
@click.option("--format", "output_format", type=click.Choice(["text", "markdown"]), default="text")
@click.option("--quiet", "-q", is_flag=True, help="Suppress text output")
def check(
    diff_file: str | None,
    git_diff: Tuple[str, str] | None,
    ignore_critical: bool,
    ignore_dirs_str: str,
    as_json: bool,
    output_format: str,
    quiet: bool,
):
    """
    Run pre-merge impact analysis.

    Compares changes against the dependency graph to detect breaking changes.
    Will automatically scan the codebase if no graph exists.

    \b
    Examples:
        jnkn check
        jnkn check --ignore-dirs "tests,fixtures,examples"
        jnkn check --git-diff main HEAD
        jnkn check --json
    """
    renderer = JsonRenderer("check")
    context_manager = renderer.capture() if as_json else _null_context()
    error_to_report = None
    api_response = None
    fail_on_critical = not ignore_critical  # Default to Safe (Fail on critical)

    # Parse ignore_dirs
    ignore_dirs: Set[str] = set()
    if ignore_dirs_str:
        ignore_dirs = {d.strip() for d in ignore_dirs_str.split(",") if d.strip()}

    with context_manager:
        try:
            changed_files = []
            if diff_file:
                changed_files = get_changed_files_from_diff_file(diff_file)
            elif git_diff:
                base, head = git_diff
                changed_files = get_changed_files_from_git(base, head)
            else:
                try:
                    changed_files = get_changed_files_from_git("HEAD~1", "HEAD")
                except Exception:
                    pass

            engine = CheckEngine(ignore_dirs=ignore_dirs)
            
            # Force scan if we have no changes (e.g. fresh repo) to ensure graph is built
            if not changed_files and not engine.graph:
                engine.ensure_graph_exists(force_scan=True)

            report = engine.analyze(changed_files)

            if fail_on_critical and report.critical_count > 0:
                report.result = CheckResult.BLOCKED
            elif report.high_count > 5:
                report.result = CheckResult.WARN

            api_violations = [
                ApiViolation(rule=v.rule, severity=v.severity, message=v.message)
                for v in report.violations
            ]

            api_response = CheckResponse(
                result=CheckResultStatus[report.result.name],
                exit_code=report.result.value,
                changed_files_count=len(report.changed_files),
                critical_count=report.critical_count,
                high_count=report.high_count,
                violations=api_violations,
            )

        except Exception as e:
            error_to_report = e

    if as_json:
        if error_to_report:
            renderer.render_error(error_to_report)
            sys.exit(1)
        elif api_response:
            renderer.render_success(api_response)
            sys.exit(api_response.exit_code)
    else:
        if error_to_report:
            if not quiet:
                console.print(f"‚ùå [red]Error:[/red] {error_to_report}")
            sys.exit(1)

        if api_response:
            color = "green"
            if api_response.result == CheckResultStatus.BLOCKED:
                color = "red"
            elif api_response.result == CheckResultStatus.WARN:
                color = "yellow"

            if not quiet:
                if api_response.violations:
                    console.print(
                        f"\n[bold]Analysis Complete:[/bold] {len(api_response.violations)} violations found."
                    )
                    for v in api_response.violations:
                        icon = (
                            "üî¥"
                            if v.severity == "critical"
                            else "üü†"
                            if v.severity == "high"
                            else "‚ö™"
                        )
                        console.print(f"  {icon} [{v.severity.upper()}] {v.message}")
                else:
                    console.print("\n‚úÖ Analysis Complete: 0 violations found.")

                console.print(f"\nResult: [bold {color}]{api_response.result.value}[/bold {color}]")

            sys.exit(api_response.exit_code)